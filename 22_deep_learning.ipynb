{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "22-deep-learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IndraniMandal/CSC310-S20/blob/master/22_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In order to run this notebook you have to have the GPU accelerator enabled (see notebook on requesting additional computing resources)"
      ],
      "metadata": {
        "id": "eLHGgyy8bCJl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQHHZ_9wfo73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0f6258-41a5-47d1-d9de-f88c10ebc978"
      },
      "source": [
        "###### Set Up #####\n",
        "# verify our folder with the data and module assets is installed\n",
        "# if it is installed make sure it is the latest\n",
        "!test -e ds-assets && cd ds-assets && git pull && cd ..\n",
        "# if it is not installed clone it \n",
        "!test ! -e ds-assets && git clone https://github.com/IndraniMandal/ds-assets.git\n",
        "# point to the folder with the assets\n",
        "home = \"ds-assets/assets/\" \n",
        "import sys\n",
        "sys.path.append(home)      # add home folder to module search path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ds-assets'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 168 (delta 0), reused 2 (delta 0), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (168/168), 7.40 MiB | 13.96 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26dbkkRxfo75"
      },
      "source": [
        "# Natural Language Processing: Deep Learning\n",
        "\n",
        "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is part of a broader family of machine learning methods based on the layers used in artificial neural networks.  Here is how deep learning fits into the broader AI picture,\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/AI-ML-DL.png\" height='500' width='500'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2icT0i7fo76"
      },
      "source": [
        "# Deep Neural Networks\n",
        "\n",
        "A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. \n",
        "\n",
        "DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network\n",
        "\n",
        "DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network doed accurately recognize a particular pattern, an algorithm (backpropagation) will adjust the weights appropriately.\n",
        "\n",
        "<img src='https://i.stack.imgur.com/OH3gI.png' height='250' width='750'>\n",
        "\n",
        "The difference between ANNs and DNNs is the number of hidden layers in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49-UR_Tfo77"
      },
      "source": [
        "# NLP and Deep Learning\n",
        "\n",
        "DNNs are particularly well suited for NLP.  But before we look at this application we need to talk about `word embeddings`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzlvy22Gfo78"
      },
      "source": [
        "## From Vector Model to Word Embeddings\n",
        "\n",
        "### The Vector Model\n",
        "\n",
        "In the [document vector model](https://en.wikipedia.org/wiki/Vector_space_model) of a collection of documents each word that appears in the collection is defined as a dimension in the corresponding vector model and each document appears as a feature vector in this model.  Consider the following figure,\n",
        "\n",
        "<!-- ![](https://ahmedbesbes.com/images/article_5/tfidf.jpg) -->\n",
        "\n",
        "<!-- ![](https://raw.githubusercontent.com/lutzhamel/fake-news/master/term-doc.jpg) -->\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/lutzhamel/fake-news/master/term-doc.jpg\" height=\"350\" width=\"450\">\n",
        "\n",
        "Here each column represents the feature vector of one of the documents in the collection and the rows are the features or dimensions of the vectors. Notice that there is one feature for each word that appears in the collection of documents. The column vectors can be used for training a text classifier, that is, the transpose of the term-doc matrix shown here can be used directly as a training set for a classifier. \n",
        " \n",
        "The fields in the term-doc matrix are the counts of how many times a word appears in a document.  However, there are many ways to encode the occurences of words in the collection within this matrix. In the binary `CountVectorizer`  the fields are just 0 and 1 indicating whether a particular word appears in a document or not. Perhaps the most famous encoding is [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf), short for term frequency–inverse document frequency.  Sklearn supports a [TD-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "\n",
        "### Disadvantages of the Vector Model\n",
        "\n",
        "The vector representation of documents has two important consequences for document classification problems: \n",
        "\n",
        "* The order and contexts of words are lost. To see the importance of the word context consider these [two sentences](https://jair.org/index.php/jair/article/view/11030): “it was not good, it was actually quite bad” and “it was not bad, it was actually quite good”.  The vector representation of these sentences is exactly the same but they obviously have very different meanings or classifications.  The vector representation of  documents is often  called the *bag of words* representation referring to the fact that it loses all order and context information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ_4H8gbfo79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d9c6d9-0822-48d0-fa58-26a4116aca4e"
      },
      "source": [
        "# show the vector models of our two sentences\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "doc_names = [\"sen1\", \"sen2\"]\n",
        "docs = [\"it was not good, it was actually quite bad\",\n",
        "        \"it was not bad, it was actually quite good\"]\n",
        "\n",
        "# process documents\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", binary = True)\n",
        "docarray = vectorizer.fit_transform(docs).toarray()\n",
        "coords = vectorizer.get_feature_names_out()\n",
        "docterm = pd.DataFrame(data=docarray,index=doc_names,columns=coords)\n",
        "print(\"\\nDocterm:\")\n",
        "print(docterm)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Docterm:\n",
            "      actually  bad  good  it  not  quite  was\n",
            "sen1         1    1     1   1    1      1    1\n",
            "sen2         1    1     1   1    1      1    1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHe1VX9jfo7-"
      },
      "source": [
        "*  Semantic similarities between words cannot be represented. To see the importance of semantic similarity consider one document that discusses dogs and another document that discusses puppies. From a vector model perspective the feature set for these two documents will not intersect in terms of the notion of dog because the vector model simply considers dogs and puppies to be two different features and the similarity of these documents will not be apparent to a machine learning algorithm.\n",
        "```\n",
        "Docterm:\n",
        "           ...  dogs  puppies  ...\n",
        "sen1    ...     1        0  ...\n",
        "sen2    ...     0        1  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DXDbby6fo7-"
      },
      "source": [
        "### Word Embeddings\n",
        "\n",
        "Here words (not documents) are represented as [*embedding vectors*](https://en.wikipedia.org/wiki/Word_embedding) with the idea that two words that are semantically similar to each other have similar vectors. Consider the following figure,\n",
        "\n",
        "\n",
        " <img src=\"https://www.researchgate.net/profile/Tom_Kenter/publication/325451970/figure/fig1/AS:632023664304129@1527697592901/Visualization-of-3-dimensional-word-embeddings.png\" height=\"200\" width=\"250\"/>\n",
        "\n",
        "This figure represents a 3D embedding space and we can see that concepts that are similar to each other are close together in this embedding space.  Therefore the similarity of our two documents talking about dogs and puppies is expressed as a \"vector simililarity\" which is most often computed as the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) rather than comparing features directly,\n",
        "\n",
        "<img src=\"http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png\" height=\"200\" width=\"800\">\n",
        "\n",
        "In other words, we are comparing the semantic notion of dogs and puppies rather than the precise syntax of the words.\n",
        "\n",
        "Here is another example of five embedding vectors in a 2D embedding space,\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/lutzhamel/word2vec-simplified/master/word-vectors.png\">\n",
        "\n",
        "The five vectors represent the words,\n",
        "\n",
        "* Red – Queen\n",
        "* Blue – King\n",
        "* Green – Man\n",
        "* Black – Woman\n",
        "* Yellow – Oil\n",
        "\n",
        "Applying vector similarity here it becomes obvious that the vectors representing 'Man' and 'Woman' are most similar to each other, that is, they are semantically most closely related.  It is also easy to see that the vector representing 'Oil' is most dissimilar to all the other vectors.\n",
        "\n",
        "One of the more popular word embeddings is [word2vec](https://en.wikipedia.org/wiki/Word2vec) created by Google which embeds words in a 300D vector space.  Sentences are now represented as a `len x 300` matrix (or tensor in DNN terminology) where `len` is the number of words in the sentence and `300` is the embedding dimension. That is, each word in a sentence is embedded in that 300D space.  The dimensions themselves are \"statistical dimensions\" and not any specific word calculated by the word embedding algorithm.\n",
        "\n",
        "A GitHub repository that explores word2vec a little bit further can be found [here](https://github.com/lutzhamel/word2vec-simplified)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qigBMbKQfo7_"
      },
      "source": [
        "## Processing Documents for DNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjlJkTm8fo7_"
      },
      "source": [
        "In deep neural networks documents are no longer compressed into a vector representation of just word occurences.  Instead, deep neural networks process actual sequences of words (coded as a integers) as they appear in the documents thereby *maintaining the order and contexts* of words. Consider the following code snippet using the [Keras](https://keras.io) tokenizer applied to our two sentences from above,\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# train tokenizer - since both sentences contain\n",
        "# the same words we can train the tokenizer on \n",
        "# just one of the sentences\n",
        "tokenizer.fit_on_texts([\"it was not good, it was actually quite bad\"])\n",
        "# print sequences\n",
        "print(tokenizer.texts_to_sequences([\"it was not good, it was actually quite bad\"])[0])\n",
        "print(tokenizer.texts_to_sequences([\"it was not bad, it was actually quite good\"])[0])\n",
        "# print word index\n",
        "print(tokenizer.word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y74clI-deGBU",
        "outputId": "faaa663b-13c1-473b-f69d-fdbef873da0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 1, 2, 5, 6, 7]\n",
            "[1, 2, 3, 7, 1, 2, 5, 6, 4]\n",
            "{'it': 1, 'was': 2, 'not': 3, 'good': 4, 'actually': 5, 'quite': 6, 'bad': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These sequences can be directly fed into a deep neural network for training and classification. Notice that word order and context are nicely preserved in this representation.  This is very different from the Naive Bayes training from our previous NLP applications."
      ],
      "metadata": {
        "id": "Q-hML7euemZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Deep Neural Network for Fake-News\n",
        "\n",
        "The deep neural network we are using for our experiment can be seen here as a Python implementation using the [Keras](https://keras.io) deep learning library,"
      ],
      "metadata": {
        "id": "bu2-t6lCTUn5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M_V4Ud4_jxTO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "TEXT_DATA = 'https://raw.githubusercontent.com/IndraniMandal/fake-news/master/data/fake_or_real_news.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get our text data and preprocess it,"
      ],
      "metadata": {
        "id": "IlPlFoBNU2Kg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9fjCaUesjxTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41854939-0488-4b01-d904-e62800020424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                              title  \\\n",
            "0   8476                       You Can Smell Hillary’s Fear   \n",
            "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
            "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
            "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
            "4    875   The Battle of New York: Why This Primary Matters   \n",
            "\n",
            "                                                text label  \n",
            "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
            "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
            "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
            "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
            "4  It's primary day in New York and front-runners...  REAL  \n"
          ]
        }
      ],
      "source": [
        "# read in our data and preprocess it\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "print(df.head())\n",
        "\n",
        "# only select stories with lengths gt 0 -- there are some texts with len = 0\n",
        "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
        "df = df[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TOZR1rhZjxTR",
        "outputId": "c6960d1e-4229-4dc1-b8ed-b49ca83703ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6335 texts.\n"
          ]
        }
      ],
      "source": [
        "# prepare text samples and their labels\n",
        "texts = df['text']\n",
        "labels = df['label']\n",
        "print('Found {} texts.'.format(texts.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mJZQaPZMjxTR",
        "outputId": "af965774-1693-437e-c07b-b07b15a91c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot the distribution of article lengths in terms of word counts\n",
        "text_lengths = texts.apply(lambda x: len(x.split(\" \")))\n",
        "plt.hist(text_lengths)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph above we see that the majority of documents have 5000 words or less.  Therefore we pick the constant `MAX_SEQUENCE_LENGTH` to be 5000."
      ],
      "metadata": {
        "id": "M7bXPRKaVd0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BI-hQADqjxTP"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 5000\n",
        "MAX_NUM_WORDS = 25000\n",
        "EMBEDDING_DIM = 300\n",
        "TEST_SPLIT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ybJ-TLPCjxTQ"
      },
      "outputs": [],
      "source": [
        "# a function that allows us to evaluate our models\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_model(predict_fun, X_train, y_train, X_test, y_test):\n",
        "    '''\n",
        "    evaluate the model, both training and testing errors are reported\n",
        "    '''\n",
        "    # training error\n",
        "    y_predict_train = predict_fun(X_train)\n",
        "    train_acc = accuracy_score(y_train,y_predict_train)\n",
        "    \n",
        "    # testing error\n",
        "    y_predict_test = predict_fun(X_test)\n",
        "    test_acc = accuracy_score(y_test,y_predict_test)\n",
        "    \n",
        "    return train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TZ24waD-jxTQ"
      },
      "outputs": [],
      "source": [
        "# a function estimate 95% confidence interval on error\n",
        "\n",
        "# NOTE: based on conversation on stackexchange: \n",
        "# https://stats.stackexchange.com/questions/247551/how-to-determine-the-confidence-of-a-neural-network-prediction\n",
        "# towards bottom of the page.\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "def error_conf(error, n):\n",
        "    term = 1.96*sqrt((error*(1-error))/n)\n",
        "    lb = error - term\n",
        "    ub = error + term\n",
        "    \n",
        "    return lb, ub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q2M0Q6fijxTV",
        "outputId": "f9fbe06a-831d-49c5-9afd-49afeb29f6ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 98817 unique tokens.\n",
            "Shape of data tensor: (6335, 5000)\n",
            "Shape of label tensor: (6335,)\n"
          ]
        }
      ],
      "source": [
        "# vectorize the text samples into a 2D integer tensor \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "data = pad_sequences(sequences, \n",
        "                     maxlen=MAX_SEQUENCE_LENGTH, \n",
        "                     padding='pre', \n",
        "                     truncating='pre')\n",
        "\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vh-UnAyXjxTV"
      },
      "outputs": [],
      "source": [
        "# split the data into a training set and a validation set   \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(data, \n",
        "                                                  labels.apply(lambda x: 0 if x == 'FAKE' else 1), \n",
        "                                                  test_size=TEST_SPLIT,\n",
        "                                                  random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IJrCDwL4jxTV"
      },
      "outputs": [],
      "source": [
        "# build a 1D convnet with global maxpooling                                                                      \n",
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        # part 1: word and sequence processing\n",
        "        layers.Embedding(num_words,\n",
        "                         EMBEDDING_DIM, \n",
        "                         input_length=MAX_SEQUENCE_LENGTH,\n",
        "                         trainable=True),\n",
        "        layers.Conv1D(128, 5, activation='relu'),\n",
        "        layers.GlobalMaxPooling1D(),\n",
        "        \n",
        "        # part 2: classification\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DFIhjjFfo8A"
      },
      "source": [
        "Our DNN can be broken down into two distinct parts. The first part consists of three layers and is responsible for  word and sequence processing:\n",
        "1. The Embedding layer - learn word embeddings.\n",
        "2. The Convolution layer - learn patterns throughout the text sequences.\n",
        "3. The Pooling layer - filter out the interesting sequence patterns.\n",
        "\n",
        "The second part consists of two layers,\n",
        "\n",
        "1. A Dense layer with a ReLU activation function.\n",
        "2. A Dense layer (also the output layer) with a Sigmoid activation function.\n",
        "\n",
        "This part of the DNN can be viewed as a traditional feed-foward, back-propagation neural network with one hidden layer operating on a feature vector of length 128 computed by the first part of the DNN.   In order to see this perhaps a bit clearer, look at the summary of the DNN as compiled by Keras,\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOvcATDrZ5jN",
        "outputId": "598cd0b2-f5d2-4a60-860c-6e8b5611234c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5000, 300)         7500300   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 4996, 128)         192128    \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,709,069\n",
            "Trainable params: 7,709,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `None` in the *Output Shape* column simply denotes the *current batch size default*. That  means the pooling layer computes a feature vector of size 128 which is passed into dense layers of the feedforward network as we mentioned above.\n",
        "\n",
        "The overall structure of the DNN can be understood as a preprocessor defined in the first part that is being trained to map text sequences into feature vectors in such a way that the weights of the second part can be trained to obtain optimal classification results from the overall network. "
      ],
      "metadata": {
        "id": "g3cNT7WMaFkI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nII7SEYHjxTV",
        "outputId": "05605273-6f5f-44b2-ac81-8260b577a4a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "40/40 [==============================] - 48s 877ms/step - loss: 0.4677 - accuracy: 0.7976 - val_loss: 0.2385 - val_accuracy: 0.9037\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 33s 815ms/step - loss: 0.1270 - accuracy: 0.9627 - val_loss: 0.1222 - val_accuracy: 0.9605\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 33s 814ms/step - loss: 0.0335 - accuracy: 0.9929 - val_loss: 0.1034 - val_accuracy: 0.9605\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 33s 814ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0911 - val_accuracy: 0.9669\n",
            "Epoch 5/10\n",
            "10/40 [======>.......................] - ETA: 23s - loss: 8.9273e-04 - accuracy: 1.0000"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train, \n",
        "                    y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all possible parameters history stores\n",
        "history.history.keys()"
      ],
      "metadata": {
        "id": "P2VpEuaDKxTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ns5YRvjxTV"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy values\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQoSKxX1jxTV"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "\n",
        "train_acc, test_acc = evaluate_model(lambda x: np.rint(model.predict(x)),\n",
        "                                     x_train, \n",
        "                                     y_train, \n",
        "                                     x_val, \n",
        "                                     y_val)\n",
        "print(\"Training Accuracy: {:.2f}%\".format(train_acc*100))\n",
        "print(\"Testing Accuracy: {:.2f}%\".format(test_acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSYr6JiojxTW"
      },
      "outputs": [],
      "source": [
        "# estimate 95% confidence interval\n",
        "\n",
        "n = x_val.shape[0]\n",
        "lb, ub = error_conf(1-test_acc, n)\n",
        "\n",
        "print(\"95% confidence interval: {:.2f}%-{:.2f}%\".format((1-ub)*100,(1-lb)*100))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW9MLPzSfo8C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}