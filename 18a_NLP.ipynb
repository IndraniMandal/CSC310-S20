{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "18a-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IndraniMandal/CSC310-S20/blob/master/18a_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkDsXAtFfYZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abc72ea-9a58-4636-d443-4d6bdcb81ae9"
      },
      "source": [
        "###### Set Up #####\n",
        "# verify our folder with the data and module assets is installed\n",
        "# if it is installed make sure it is the latest\n",
        "!test -e ds-assets && cd ds-assets && git pull && cd ..\n",
        "# if it is not installed clone it \n",
        "!test ! -e ds-assets && git clone https://github.com/IndraniMandal/ds-assets.git\n",
        "# point to the folder with the assets\n",
        "home = \"ds-assets/assets/\" \n",
        "import sys\n",
        "sys.path.append(home)      # add home folder to module search path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d756nrBQfYZC"
      },
      "source": [
        "# Natural Language Processing (NLP)\n",
        "\n",
        "Some of the most important data in our society is represented as unstructured text:\n",
        "\n",
        "* Medical records\n",
        "* Court cases\n",
        "* Insurance documents\n",
        "\n",
        "Other data perhaps not as fundamental but that provides interesting insights into trends and mindsets:\n",
        "\n",
        "* Twitter and other online blogs\n",
        "* News feeds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPUb8a1IfYZD"
      },
      "source": [
        "In all of these cases we want to extract meaning from the unstructured text:\n",
        "\n",
        "* Perhaps we want to do classification (medical records - high risk/low risk)\n",
        "* Perhaps we want to do a topic analysis of the twitter feeds\n",
        "* Perhaps we would like to construct a recommendation engine for news feeds\n",
        "\n",
        "Regardless, what the task, we need to convert the unstructured text into something that we can work with and perhaps most importantly, our models can work with.\n",
        "\n",
        "☞ The **Vector Model** of text (sometimes called the **Bag-of-Words model**)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpa2AKAfYZD"
      },
      "source": [
        "## The Vector Model\n",
        "\n",
        "The vector model converts a document with unstructured text into a **point in an n-dimensional coordinate system** where the coordinate system is defined by the words contained in the text.\n",
        "\n",
        "Consider: the quick brown fox jumps over the lazy dog\n",
        "\n",
        "This text can be represented as the tuple rearranged in alphabetical order,\n",
        "```\n",
        "(brown,dog,fox,jumps,lazy,over,quick,the)\n",
        "```\n",
        "\n",
        "Let’s consider the fact that we have multiple documents and represent them as tuples,\n",
        "\n",
        "* Doc 1: the quick brown fox jumps over the lazy dog &rarr; `(brown,dog,fox,jumps,lazy,over,quick,the)`\n",
        "* Doc 2: rudi is a lazy brown dog &rarr; `(a,brown,dog,is,lazy,rudi)`\n",
        "\n",
        "In order to compare the two documents we create a tuple of the **union** of the words appearing in the \n",
        "two sentence tuples,\n",
        "```\n",
        "(a,brown,dog,fox,is,jumps,lazy,over,quick,rudi,the)\n",
        "```\n",
        "and represent each document as bit vectors with the same length as the tuple above and with 1's and 0's\n",
        "indicating if the document contains a word at a particular tuple position or not,\n",
        "\n",
        "* Doc 1: (0,1,1,1,0,1,1,1,1,0,1)\n",
        "* Doc 2: (1,1,1,0,1,0,1,0,0,1,0)\n",
        "\n",
        "Notice that our word tuple now has become our coordinate system, in this case with 11 dimensions, and each document is now a point in this 11-dimensional space.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Rectangular_coordinates.svg/1280px-Rectangular_coordinates.svg.png\" width=\"350\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h4pSnSlfYZE"
      },
      "source": [
        "The nice thing about this vector model representation is that we can do mathematics on the documents!\n",
        "\n",
        "Consider adding another document to our collection\n",
        "\n",
        "* Doc 3: princess jumps over the dog &rarr; `(dog,jumps,over,princess,the)`\n",
        "\n",
        "Here we have the new word `princess`, so we need to extend our coordinate system to 12 dimensions by adding `princess`,\n",
        "```\n",
        "(a,brown,dog,fox,is,jumps,lazy,over,princess,quick,rudi,the)\n",
        "```\n",
        "Our three documents become vectors/points in this coordinate system,\n",
        "\n",
        "* Doc 1: the quick brown fox jumps over the lazy dog &rarr; `(0,1,1,1,0,1,1,1,0,1,0,1)`\n",
        "* Doc 2: rudi is a lazy brown dog &rarr; `(1,1,1,0,1,0,1,0,0,0,1,0)`\n",
        "* Doc 3: princess jumps over the dog &rarr; `(0,0,1,0,0,1,1,1,1,0,0,1)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4YTq8XqfYZE"
      },
      "source": [
        "Given our vector model of the three docs we can ask questions like this, \n",
        "\n",
        "> Is doc2 or doc3 more similar to doc1?\n",
        "\n",
        "Since all three documents are considered points in our coordinate system we can Euclidean distances in that coordinate system to answer that question. More specifically, we can answer this question by considering the Euclidean distances doc1 &harr; doc2 and doc1 &harr; doc3 in our coordinate system.  \n",
        "\n",
        "The Euclidean distance d in n-dimensional space between two points $p$ and $q$ is defined as:\n",
        "\n",
        "$d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\\ldots+(p_n-q_n)^2}$\n",
        "\n",
        "In our case the point $p$ and $q$ are document vectors and $p_i$ and $q_i$ are the components of the respective \n",
        "vectors.\n",
        "\n",
        "In order to answer our question we have to perform the following computations,\n",
        "\n",
        "* $d(doc1, doc2) = \\sqrt{(0-1)^2+(1-1)^2+(1-1)^2+(1-0)^2+(0-1)^2+(1-0)^2+(1-1)^2+(1-0)^2+(0-0)^2+(1-0)^2+(0-1)^2+(1-0)^2}                      = \\sqrt{1+0+0+1+1+1+0+1+0+1+1+1} = \\sqrt{8} = 2.8$\n",
        "\n",
        "* $d(doc1,doc3) = \\sqrt{(0-0)^2+(1-0)^2+(1-1)^2+(1-0)^2+(0-0)^2+(1-1)^2+(1-1)^2+(1-1)^2+(0-1)^2+(1-0)^2+(0-0)^2+(1-1)^2} = \\sqrt{0+1+0+1+0+0+0+0+1+1+0+0} = \\sqrt{4} = 2.0$\n",
        "\n",
        "> So, doc3 is more similar to doc1 than doc2!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-bxFAEufYZF"
      },
      "source": [
        "## The Vector Model in Sklearn\n",
        "\n",
        "Let's try the above in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r28w-P94fYZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e343a3a0-eaac-460b-908f-3de5fad4a065"
      },
      "source": [
        "import pandas\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# set up our documents\n",
        "doc_names = [\"doc1\", \"doc2\", \"doc3\"]\n",
        "docs = [\"the quick brown fox jumps over the lazy dog\",\n",
        "        \"rudi is a lazy brown dog\",\n",
        "        \"princess jumps over the lazy dog\"]\n",
        "\n",
        "# process documents\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", binary = True)\n",
        "docarray = vectorizer.fit_transform(docs).toarray()\n",
        "\n",
        "# print out the coordinate system\n",
        "# NOTE: sklearn filters out single character words -- is drops 'a'\n",
        "print(\"Coordinates:\")\n",
        "coords = vectorizer.get_feature_names()\n",
        "print(coords)\n",
        "\n",
        "# print out how each document is represented in this coordinate system\n",
        "# NOTE: traditional this mapping is called the 'docterm' matrix - the mapping\n",
        "#       of each document into the set of terms/words.\n",
        "print(\"\\nDocterm:\")\n",
        "docterm = pandas.DataFrame(data=docarray,index=doc_names,columns=coords)\n",
        "print(docterm)\n",
        "\n",
        "# print pairwise distances between documents\n",
        "distances = euclidean_distances(docterm)\n",
        "distances_df = pandas.DataFrame(data=distances, index=doc_names, columns=doc_names)\n",
        "print(\"\\nPairwise Distances:\")\n",
        "print(distances_df)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coordinates:\n",
            "['brown', 'dog', 'fox', 'is', 'jumps', 'lazy', 'over', 'princess', 'quick', 'rudi', 'the']\n",
            "\n",
            "Docterm:\n",
            "      brown  dog  fox  is  jumps  lazy  over  princess  quick  rudi  the\n",
            "doc1      1    1    1   0      1     1     1         0      1     0    1\n",
            "doc2      1    1    0   1      0     1     0         0      0     1    0\n",
            "doc3      0    1    0   0      1     1     1         1      0     0    1\n",
            "\n",
            "Pairwise Distances:\n",
            "          doc1      doc2      doc3\n",
            "doc1  0.000000  2.645751  2.000000\n",
            "doc2  2.645751  0.000000  2.645751\n",
            "doc3  2.000000  2.645751  0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l58-sdfIfYZG"
      },
      "source": [
        "> Just as we computed by hand - doc3 is more similar to doc1 than doc2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7CsNjixfYZG"
      },
      "source": [
        "# Real World Data: News Articles\n",
        "\n",
        "The data set we will be using are articles from a newsgroup feed (think chat room before chat rooms existed).\n",
        "\n",
        "We will look at two newsgroups: \n",
        "* Politics\n",
        "* Space\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uzun3y1fYZH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "003b8f07-1ef5-4277-e7ac-805caa293516"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# get the newsgroup data\n",
        "newsgroups = pd.read_csv(home+\"newsgroups.csv\")\n",
        "newsgroups.head(n=10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label\n",
              "0  From: demon@desire.wright.edu (Not a Boomer)\\n...     space\n",
              "1  From: dreitman@oregon.uoregon.edu (Daniel R. R...     space\n",
              "2  From: mcgoy@unicorn.acs.ttu.edu (David McGaugh...     space\n",
              "3  From: blh@uiboise.idbsu.edu (Broward L. Horne)...     space\n",
              "4  From: wiggins@cecer.army.mil (Don Wiggins)\\nSu...     space\n",
              "5  From: nickh@CS.CMU.EDU (Nick Haines)\\nSubject:...  politics\n",
              "6  From: mike@gordian.com (Michael A. Thomas)\\nSu...     space\n",
              "7  From: jbreed@doink.b23b.ingr.com (James B. Ree...  politics\n",
              "8  From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...  politics\n",
              "9  From: DPierce@world.std.com (Richard D Pierce)...  politics"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9215c5c8-a4b2-434c-93b9-1eb0e891f2bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: demon@desire.wright.edu (Not a Boomer)\\n...</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: dreitman@oregon.uoregon.edu (Daniel R. R...</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: mcgoy@unicorn.acs.ttu.edu (David McGaugh...</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: blh@uiboise.idbsu.edu (Broward L. Horne)...</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: wiggins@cecer.army.mil (Don Wiggins)\\nSu...</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>From: nickh@CS.CMU.EDU (Nick Haines)\\nSubject:...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>From: mike@gordian.com (Michael A. Thomas)\\nSu...</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>From: jbreed@doink.b23b.ingr.com (James B. Ree...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>From: DPierce@world.std.com (Richard D Pierce)...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9215c5c8-a4b2-434c-93b9-1eb0e891f2bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9215c5c8-a4b2-434c-93b9-1eb0e891f2bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9215c5c8-a4b2-434c-93b9-1eb0e891f2bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "LP-CqLpgc2ke",
        "outputId": "d6da6973-e9ba-4a49-dd3a-35979eb7c513"
      },
      "source": [
        "newsgroups['text'].iloc[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'From: demon@desire.wright.edu (Not a Boomer)\\nSubject: Re: Supply Side Economic Policy (was Re: David Stockman )\\nArticle-I.D.: desire.1993Apr6.125825.8263\\nDistribution: na\\nOrganization: ACME Products\\nLines: 60\\n\\nIn article <Ufk_Gqu00WBKE7cX5V@andrew.cmu.edu>, Ashish Arora <ashish+@andrew.cmu.edu> writes:\\n> Excerpts from netnews.sci.econ: 5-Apr-93 Re: Supply Side Economic Po..\\n> by Not a Boomer@desire.wrig \\n> [...]\\n> \\n>>    The deficits declined from 84-9, reaching a low of 2.9% of GNP before  \\n>> the tax and spending hike of 1990 reversed the trend.\\n>>  \\n>> Brett\\n> Is this true ?  Some more details would be appreciated.\\n\\nIn billions of dollars (%GNP):\\nyear  GNP    receipts     outlays       deficit     debt    unempl%  admin\\n====  ====   ===========  ============  =========   ======  =======  =======\\n1977  1930   355.6 (18.4) 409.2  (21.2) 53.6 (2.8)   709.1           Carter\\n1978  2174   399.6 (18.4) 458.7  (21.1) 59.2 (2.7)   780.4           Carter\\n1979  2444   463.3 (19.0) 503.5  (20.6) 40.2 (1.6)   833.8           Carter\\n1980  2674   517.1 (19.3) 590.9  (22.1) 73.8 (2.8)   914.3   7.9     Carter\\n1981  2986   599.3 (20.1) 678.2  (22.7) 78.9 (2.6)  1003.9   8.4     Reagan\\n1982  3130   617.8 (19.7) 745.7  (23.8) 127.9 (4.1) 1147.0  11.0     Reagan\\n1983  3325   600.6 (18.1) 808.3  (24.3) 207.8 (6.2) 1381.9  10.9     Reagan\\n1984  3688   666.5 (18.1) 851.8  (23.1) 185.3 (5.0) 1576.7   8.6     Reagan\\n1985  3958   734.1 (18.5) 946.3  (23.9) 212.3 (5.4) 1827.5   8.1     Reagan\\n1986  4177   769.1 (18.4) 989.8  (23.7) 220.7 (5.3) 2129.5   7.9     Reagan\\n1987  4442   854.1 (19.2) 1002.1 (22.6) 148.0 (3.4) 2354.3   7.1     Reagan\\n1988  4771   909.0 (19.1) 1064.1 (22.3) 155.1 (3.2) 2614.6   6.3     Reagan\\n1989  5201   990.8 (19.0) 1142.8 (22.0)\\t152.0 (2.9) 2881.1           Bush\\n1990         1031.2       1251.6        220.4       3190.5           Bush\\n1991\\t     1054.3\\t  1323.0\\t268.7       3599.0           Bush\\n\\n[Source: Statistical Abstract of the US (1990 version), American Almanac \\n(1993 version), Universal Almanac (1993 version), Information Please Almanac\\n(1991 version)]\\n\\n\\t\\tGRAPHICALLY: Deficits as a % of GNP, 1981-89\\n\\n% GNP\\n7|\\n |\\n6|                       X\\n |                                       X       X\\n5|                               X                \\n |                                                \\n4|               X\\n |                                                       X\\n3|                                                               X       X\\n |       X\\n2|\\n |\\n1|\\n |____________________________________________________________________________\\n0\\t1981\\t1982\\t1983\\t1984\\t1985\\t1986\\t1987\\t1988\\t1989\\n\\n\\tIronically, Bush could have frozen spending, kept his \"no new taxes\"\\npledge and balanced the budget.\\n\\nBrett\\n________________________________________________________________________________\\n\\t\"There\\'s nothing so passionate as a vested interest disguised as an\\nintellectual conviction.\"  Sean O\\'Casey in _The White Plague_ by Frank Herbert.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73X06swAGsvD",
        "outputId": "5a4cd30c-d992-4957-838e-647b04df8b1b"
      },
      "source": [
        "newsgroups.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1058, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpYW3V-nGx6P",
        "outputId": "36144baa-2b99-4ade-cca2-06a377e07eaf"
      },
      "source": [
        "newsgroups['label'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "politics    593\n",
              "space       465\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ys46TafYZH"
      },
      "source": [
        "## The Docterm Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpZtIX-pfYZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e806c15-28a4-4102-cb56-4481f52f33bd"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# get the newsgroup data\n",
        "newsgroups = pd.read_csv(home+\"newsgroups.csv\")\n",
        "\n",
        "# process documents                                                                                               \n",
        "vectorizer = CountVectorizer(analyzer = \"word\", binary = True)\n",
        "docarray = vectorizer.fit_transform(newsgroups['text']).toarray()\n",
        "print(\"docarray shape: {}\".format(docarray.shape))\n",
        "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docarray shape: (1058, 23537)\n",
            "first 10 coords: ['00', '000', '0000', '00000', '000000', '000007', '000021', '000062david42', '00041032', '0004136']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HeDnD8lfYZI"
      },
      "source": [
        "Look at at the shape of the docarray, we see that we have about 23,000+ different features.  That means our newsgroup articles \"live\" in a 23,000+ dimensional space. When we look at the features it is clear that there are many \"nonsense\" features.  We need more filtering!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwf5fl50fYZI"
      },
      "source": [
        "## More Filtering\n",
        "\n",
        "From this it is clear that we want to do some additional filtering:\n",
        "* Minimum doc frequency = 2 -- that is, any word has to appear at least twice in the document collection\n",
        "* Delete anything that is not a word - get rid of things like ‘000’ etc., we use the token pattern arg for that.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-ndW7LJfYZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96839d2-73df-45df-c5bd-1406bde24b0a"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# get the newsgroup data\n",
        "newsgroups = pd.read_csv(home+\"newsgroups.csv\")\n",
        "\n",
        "# process documents                                                                                               \n",
        "vectorizer = CountVectorizer(analyzer = \"word\", \n",
        "                             token_pattern = \"[a-zA-Z]+\", # only words\n",
        "                             binary = True, \n",
        "                             min_df=2) # each word has to appear at least twice\n",
        "docarray = vectorizer.fit_transform(newsgroups['text']).toarray()\n",
        "                                                                                                 \n",
        "print(\"docarray shape: {}\".format(docarray.shape))\n",
        "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docarray shape: (1058, 11862)\n",
            "first 10 coords: ['a', 'aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'aas', 'ab', 'abandon', 'abandoned', 'abandonment', 'abbey']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcJ21g0nfYZK"
      },
      "source": [
        "Notice that we cut the number of features in the space to just about half of the  original features and the features look more like words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8p098RZJ5TB"
      },
      "source": [
        "## Stop Words\n",
        "\n",
        "Stop word filtering is a way to reduce dimensionality of the feature space by removing words from the document that do not add to content/concept of the document.  Words like 'its', 'an', 'the', 'for', 'that', etc. are so common in each document that they do not any value during an analysis.  We will filter them out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsQ_Id30J2Cu",
        "outputId": "feac4bc5-927f-4950-b6c0-1cad51392545"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# get the newsgroup data\n",
        "newsgroups = pd.read_csv(home+\"newsgroups.csv\")\n",
        "\n",
        "# process documents                                                                                               \n",
        "vectorizer = CountVectorizer(analyzer = \"word\", \n",
        "                             token_pattern = \"[a-zA-Z]+\", # only words\n",
        "                             binary = True, \n",
        "                             stop_words = 'english',\n",
        "                             min_df=2) # each word has to appear at least twice\n",
        "docarray = vectorizer.fit_transform(newsgroups['text']).toarray()\n",
        "                                                                                                 \n",
        "print(\"docarray shape: {}\".format(docarray.shape))\n",
        "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docarray shape: (1058, 11563)\n",
            "first 10 coords: ['aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'aas', 'ab', 'abandon', 'abandoned', 'abandonment', 'abbey', 'abc']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUHRz4MGK9ut"
      },
      "source": [
        "Notice that stop word filtering reduced the dimensionality by another 300 dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwFK5eqmfYZK"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "The first few coordinates are now:\n",
        "\n",
        "['aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'aas', 'ab', **'abandon'**, **'abandoned'**, **'abandonment'**, 'abbey', 'abc']\n",
        "\n",
        "Here, we see one more issue, three different shapes of the same root word, in this case **abandon**.\n",
        "\n",
        "> Solution: Stemming!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9ZIMmK8fYZK"
      },
      "source": [
        "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem or root form.\n",
        "\n",
        "A stemmer for English, for example, should identify the string \"cats\" (and possibly \"catlike\", \"catty\" etc.) as based on the root \"cat\", and \"stems\", \"stemmer\", \"stemming\", \"stemmed\" as based on \"stem\". \n",
        "\n",
        "A stemming algorithm reduces the words \"fishing\", \"fished\", and \"fisher\" to the root word, \"fish\".\n",
        "\n",
        "The most popular stemming algorithm:\n",
        "\n",
        "> The [Porter Stemmer](https://en.wikipedia.org/wiki/Stemming)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKxEU9OkfYZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e987d9ae-8041-49d2-9ebd-242d6a9252b3"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# get the newsgroup data\n",
        "newsgroups = pd.read_csv(home+\"newsgroups.csv\")\n",
        "\n",
        "# add doc names so that later analysis becomes more readable\n",
        "doc_names = ['doc{}'.format(i) for i in range(newsgroups.shape[0])]\n",
        "newsgroups = pd.DataFrame(newsgroups.values, index=doc_names,columns=newsgroups.columns)\n",
        "print(newsgroups.head(n=10))\n",
        "\n",
        "# build the stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# get the default text analyzer from CountVectorizer\n",
        "analyzer = CountVectorizer(analyzer = \"word\", \n",
        "                           stop_words = 'english',\n",
        "                           token_pattern = \"[a-zA-Z]+\").build_analyzer()\n",
        "\n",
        "# build a new analyzer that stems using the default analyzer to create the words to be stemmed\n",
        "def stemmed_words(doc):\n",
        "    return [stemmer.stem(w) for w in analyzer(doc)]\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
        "                                 binary=True,\n",
        "                                 min_df=2)\n",
        "docarray = vectorizer.fit_transform(newsgroups['text']).toarray()\n",
        "\n",
        "print(\"docarray shape: {}\".format(docarray.shape))\n",
        "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text     label\n",
            "doc0  From: demon@desire.wright.edu (Not a Boomer)\\n...     space\n",
            "doc1  From: dreitman@oregon.uoregon.edu (Daniel R. R...     space\n",
            "doc2  From: mcgoy@unicorn.acs.ttu.edu (David McGaugh...     space\n",
            "doc3  From: blh@uiboise.idbsu.edu (Broward L. Horne)...     space\n",
            "doc4  From: wiggins@cecer.army.mil (Don Wiggins)\\nSu...     space\n",
            "doc5  From: nickh@CS.CMU.EDU (Nick Haines)\\nSubject:...  politics\n",
            "doc6  From: mike@gordian.com (Michael A. Thomas)\\nSu...     space\n",
            "doc7  From: jbreed@doink.b23b.ingr.com (James B. Ree...  politics\n",
            "doc8  From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...  politics\n",
            "doc9  From: DPierce@world.std.com (Richard D Pierce)...  politics\n",
            "docarray shape: (1058, 8437)\n",
            "first 10 coords: ['aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'ab', 'abandon', 'abbey', 'abc', 'abdkw', 'abett', 'abid']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVa-6u4MwcV"
      },
      "source": [
        "Notice that 'abandon', 'abandoned', and 'abandonment' have been mapped into the word 'abandon'.  Also notice that our final dimensionality for our feature space is now around 8,000+ features compared to the original 23,000 features.  That \n",
        "is close to a 50% drop in the number of features.  This also means that we will\n",
        "save 50% of effort during any kind of analysis on this document collection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otgQi4vufYZL"
      },
      "source": [
        "# Doc Similarity in High-Dimensional Spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZurTfQkgfYZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "3cc1a23a-0ccb-443e-fe04-a54b57c13957"
      },
      "source": [
        "distances = euclidean_distances(docarray)\n",
        "doc_names = ['doc{}'.format(i) for i in range(docarray.shape[0])]\n",
        "distances_df = pandas.DataFrame(data=distances,index=doc_names,columns=doc_names)\n",
        "distances_df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              doc0       doc1       doc2       doc3       doc4       doc5  \\\n",
              "doc0      0.000000  12.041595  12.727922  13.711309  11.489125  14.142136   \n",
              "doc1     12.041595   0.000000  12.449900  13.747727  11.269428  13.747727   \n",
              "doc2     12.727922  12.449900   0.000000  13.928388  11.575837  14.352700   \n",
              "doc3     13.711309  13.747727  13.928388   0.000000  12.961481  15.099669   \n",
              "doc4     11.489125  11.269428  11.575837  12.961481   0.000000  13.490738   \n",
              "...            ...        ...        ...        ...        ...        ...   \n",
              "doc1053  11.832160  11.445523  12.000000  12.727922  10.862780  13.416408   \n",
              "doc1054  15.842980  15.099669  15.842980  16.462078  15.000000  16.703293   \n",
              "doc1055  12.649111  11.789826  12.649111  13.856406  11.401754  13.784049   \n",
              "doc1056  11.000000  10.862780  11.618950  12.845233  10.344080  13.379088   \n",
              "doc1057  13.711309  12.767145  13.564660  14.764823  12.489996  14.764823   \n",
              "\n",
              "              doc6       doc7       doc8       doc9  ...    doc1048  \\\n",
              "doc0     15.198684  11.532563  12.449900  11.489125  ...  14.662878   \n",
              "doc1     14.491377  10.677078  11.135529  10.816654  ...  13.711309   \n",
              "doc2     14.525839  11.532563  12.688578  11.661904  ...  14.456832   \n",
              "doc3     15.652476  12.767145  13.601471  13.038405  ...  15.588457   \n",
              "doc4     14.106736  10.148892  11.445523  10.488088  ...  13.601471   \n",
              "...            ...        ...        ...        ...  ...        ...   \n",
              "doc1053  14.317821  10.246951  11.445523  10.583005  ...  13.674794   \n",
              "doc1054  17.435596  14.491377  15.362291  14.730920  ...  17.204651   \n",
              "doc1055  14.594520  10.816654  11.874342  11.045361  ...   9.848858   \n",
              "doc1056  14.212670  10.000000  11.313708  10.246951  ...  13.784049   \n",
              "doc1057  15.394804  12.288206  13.228757  12.247449  ...  14.798649   \n",
              "\n",
              "           doc1049    doc1050    doc1051    doc1052    doc1053    doc1054  \\\n",
              "doc0     11.313708  13.601471  26.570661  12.529964  11.832160  15.842980   \n",
              "doc1     10.344080  12.649111  26.589472  12.000000  11.445523  15.099669   \n",
              "doc2     11.401754  13.379088  26.570661  12.609520  12.000000  15.842980   \n",
              "doc3     12.727922  14.594520  26.944387  13.674794  12.727922  16.462078   \n",
              "doc4     10.099505  12.609520  26.267851  11.357817  10.862780  15.000000   \n",
              "...            ...        ...        ...        ...        ...        ...   \n",
              "doc1053  10.392305  12.529964  26.457513  11.357817   0.000000  14.933185   \n",
              "doc1054  14.456832  16.186414  27.658633  15.427249  14.933185   0.000000   \n",
              "doc1055  10.392305  13.076697  26.683328  12.124356  11.575837  15.716234   \n",
              "doc1056   9.643651  12.083046  26.210685  11.045361  10.630146  14.899664   \n",
              "doc1057  11.916375  13.747727  26.153394  13.304135  12.884099  16.155494   \n",
              "\n",
              "           doc1055    doc1056    doc1057  \n",
              "doc0     12.649111  11.000000  13.711309  \n",
              "doc1     11.789826  10.862780  12.767145  \n",
              "doc2     12.649111  11.618950  13.564660  \n",
              "doc3     13.856406  12.845233  14.764823  \n",
              "doc4     11.401754  10.344080  12.489996  \n",
              "...            ...        ...        ...  \n",
              "doc1053  11.575837  10.630146  12.884099  \n",
              "doc1054  15.716234  14.899664  16.155494  \n",
              "doc1055   0.000000  11.445523  13.114877  \n",
              "doc1056  11.445523   0.000000  12.288206  \n",
              "doc1057  13.114877  12.288206   0.000000  \n",
              "\n",
              "[1058 rows x 1058 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37e61ad9-0bef-4d3b-953d-b84d134be575\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc0</th>\n",
              "      <th>doc1</th>\n",
              "      <th>doc2</th>\n",
              "      <th>doc3</th>\n",
              "      <th>doc4</th>\n",
              "      <th>doc5</th>\n",
              "      <th>doc6</th>\n",
              "      <th>doc7</th>\n",
              "      <th>doc8</th>\n",
              "      <th>doc9</th>\n",
              "      <th>...</th>\n",
              "      <th>doc1048</th>\n",
              "      <th>doc1049</th>\n",
              "      <th>doc1050</th>\n",
              "      <th>doc1051</th>\n",
              "      <th>doc1052</th>\n",
              "      <th>doc1053</th>\n",
              "      <th>doc1054</th>\n",
              "      <th>doc1055</th>\n",
              "      <th>doc1056</th>\n",
              "      <th>doc1057</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>doc0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.041595</td>\n",
              "      <td>12.727922</td>\n",
              "      <td>13.711309</td>\n",
              "      <td>11.489125</td>\n",
              "      <td>14.142136</td>\n",
              "      <td>15.198684</td>\n",
              "      <td>11.532563</td>\n",
              "      <td>12.449900</td>\n",
              "      <td>11.489125</td>\n",
              "      <td>...</td>\n",
              "      <td>14.662878</td>\n",
              "      <td>11.313708</td>\n",
              "      <td>13.601471</td>\n",
              "      <td>26.570661</td>\n",
              "      <td>12.529964</td>\n",
              "      <td>11.832160</td>\n",
              "      <td>15.842980</td>\n",
              "      <td>12.649111</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>13.711309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc1</th>\n",
              "      <td>12.041595</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.449900</td>\n",
              "      <td>13.747727</td>\n",
              "      <td>11.269428</td>\n",
              "      <td>13.747727</td>\n",
              "      <td>14.491377</td>\n",
              "      <td>10.677078</td>\n",
              "      <td>11.135529</td>\n",
              "      <td>10.816654</td>\n",
              "      <td>...</td>\n",
              "      <td>13.711309</td>\n",
              "      <td>10.344080</td>\n",
              "      <td>12.649111</td>\n",
              "      <td>26.589472</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>11.445523</td>\n",
              "      <td>15.099669</td>\n",
              "      <td>11.789826</td>\n",
              "      <td>10.862780</td>\n",
              "      <td>12.767145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc2</th>\n",
              "      <td>12.727922</td>\n",
              "      <td>12.449900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.928388</td>\n",
              "      <td>11.575837</td>\n",
              "      <td>14.352700</td>\n",
              "      <td>14.525839</td>\n",
              "      <td>11.532563</td>\n",
              "      <td>12.688578</td>\n",
              "      <td>11.661904</td>\n",
              "      <td>...</td>\n",
              "      <td>14.456832</td>\n",
              "      <td>11.401754</td>\n",
              "      <td>13.379088</td>\n",
              "      <td>26.570661</td>\n",
              "      <td>12.609520</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>15.842980</td>\n",
              "      <td>12.649111</td>\n",
              "      <td>11.618950</td>\n",
              "      <td>13.564660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc3</th>\n",
              "      <td>13.711309</td>\n",
              "      <td>13.747727</td>\n",
              "      <td>13.928388</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.961481</td>\n",
              "      <td>15.099669</td>\n",
              "      <td>15.652476</td>\n",
              "      <td>12.767145</td>\n",
              "      <td>13.601471</td>\n",
              "      <td>13.038405</td>\n",
              "      <td>...</td>\n",
              "      <td>15.588457</td>\n",
              "      <td>12.727922</td>\n",
              "      <td>14.594520</td>\n",
              "      <td>26.944387</td>\n",
              "      <td>13.674794</td>\n",
              "      <td>12.727922</td>\n",
              "      <td>16.462078</td>\n",
              "      <td>13.856406</td>\n",
              "      <td>12.845233</td>\n",
              "      <td>14.764823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc4</th>\n",
              "      <td>11.489125</td>\n",
              "      <td>11.269428</td>\n",
              "      <td>11.575837</td>\n",
              "      <td>12.961481</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.490738</td>\n",
              "      <td>14.106736</td>\n",
              "      <td>10.148892</td>\n",
              "      <td>11.445523</td>\n",
              "      <td>10.488088</td>\n",
              "      <td>...</td>\n",
              "      <td>13.601471</td>\n",
              "      <td>10.099505</td>\n",
              "      <td>12.609520</td>\n",
              "      <td>26.267851</td>\n",
              "      <td>11.357817</td>\n",
              "      <td>10.862780</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>11.401754</td>\n",
              "      <td>10.344080</td>\n",
              "      <td>12.489996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc1053</th>\n",
              "      <td>11.832160</td>\n",
              "      <td>11.445523</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>12.727922</td>\n",
              "      <td>10.862780</td>\n",
              "      <td>13.416408</td>\n",
              "      <td>14.317821</td>\n",
              "      <td>10.246951</td>\n",
              "      <td>11.445523</td>\n",
              "      <td>10.583005</td>\n",
              "      <td>...</td>\n",
              "      <td>13.674794</td>\n",
              "      <td>10.392305</td>\n",
              "      <td>12.529964</td>\n",
              "      <td>26.457513</td>\n",
              "      <td>11.357817</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.933185</td>\n",
              "      <td>11.575837</td>\n",
              "      <td>10.630146</td>\n",
              "      <td>12.884099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc1054</th>\n",
              "      <td>15.842980</td>\n",
              "      <td>15.099669</td>\n",
              "      <td>15.842980</td>\n",
              "      <td>16.462078</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>16.703293</td>\n",
              "      <td>17.435596</td>\n",
              "      <td>14.491377</td>\n",
              "      <td>15.362291</td>\n",
              "      <td>14.730920</td>\n",
              "      <td>...</td>\n",
              "      <td>17.204651</td>\n",
              "      <td>14.456832</td>\n",
              "      <td>16.186414</td>\n",
              "      <td>27.658633</td>\n",
              "      <td>15.427249</td>\n",
              "      <td>14.933185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.716234</td>\n",
              "      <td>14.899664</td>\n",
              "      <td>16.155494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc1055</th>\n",
              "      <td>12.649111</td>\n",
              "      <td>11.789826</td>\n",
              "      <td>12.649111</td>\n",
              "      <td>13.856406</td>\n",
              "      <td>11.401754</td>\n",
              "      <td>13.784049</td>\n",
              "      <td>14.594520</td>\n",
              "      <td>10.816654</td>\n",
              "      <td>11.874342</td>\n",
              "      <td>11.045361</td>\n",
              "      <td>...</td>\n",
              "      <td>9.848858</td>\n",
              "      <td>10.392305</td>\n",
              "      <td>13.076697</td>\n",
              "      <td>26.683328</td>\n",
              "      <td>12.124356</td>\n",
              "      <td>11.575837</td>\n",
              "      <td>15.716234</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.445523</td>\n",
              "      <td>13.114877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc1056</th>\n",
              "      <td>11.000000</td>\n",
              "      <td>10.862780</td>\n",
              "      <td>11.618950</td>\n",
              "      <td>12.845233</td>\n",
              "      <td>10.344080</td>\n",
              "      <td>13.379088</td>\n",
              "      <td>14.212670</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>11.313708</td>\n",
              "      <td>10.246951</td>\n",
              "      <td>...</td>\n",
              "      <td>13.784049</td>\n",
              "      <td>9.643651</td>\n",
              "      <td>12.083046</td>\n",
              "      <td>26.210685</td>\n",
              "      <td>11.045361</td>\n",
              "      <td>10.630146</td>\n",
              "      <td>14.899664</td>\n",
              "      <td>11.445523</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.288206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc1057</th>\n",
              "      <td>13.711309</td>\n",
              "      <td>12.767145</td>\n",
              "      <td>13.564660</td>\n",
              "      <td>14.764823</td>\n",
              "      <td>12.489996</td>\n",
              "      <td>14.764823</td>\n",
              "      <td>15.394804</td>\n",
              "      <td>12.288206</td>\n",
              "      <td>13.228757</td>\n",
              "      <td>12.247449</td>\n",
              "      <td>...</td>\n",
              "      <td>14.798649</td>\n",
              "      <td>11.916375</td>\n",
              "      <td>13.747727</td>\n",
              "      <td>26.153394</td>\n",
              "      <td>13.304135</td>\n",
              "      <td>12.884099</td>\n",
              "      <td>16.155494</td>\n",
              "      <td>13.114877</td>\n",
              "      <td>12.288206</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1058 rows × 1058 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37e61ad9-0bef-4d3b-953d-b84d134be575')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-37e61ad9-0bef-4d3b-953d-b84d134be575 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-37e61ad9-0bef-4d3b-953d-b84d134be575');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAsIp0TefYZL"
      },
      "source": [
        "## Find out which stories are most similar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeTMXSF6fYZL"
      },
      "source": [
        "import sys\n",
        "\n",
        "# map 0.0 across the major diagonal into FLOAT_MAX\n",
        "newdist_df = distances_df.apply(lambda c: c.apply(lambda x: sys.float_info.max if x == 0.0 else x))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtddwlkafYZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac57edc-2e0a-472f-89e4-2ba821a5b2fb"
      },
      "source": [
        "# find the column with the minimal value\n",
        "cix = newdist_df.min().idxmin()\n",
        "print(cix)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rNhC2YDfYZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79872ce4-2969-499a-e1e4-daff0d575ad5"
      },
      "source": [
        "# find the row with the minimal value\n",
        "rix = newdist_df.loc[:,cix].idxmin()\n",
        "print(rix)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FFTSYBAfYZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a81a6d-152c-437b-9239-238560167bbb"
      },
      "source": [
        "# these two news stories are most similar\n",
        "newdist_df.loc[rix, cix]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka12S_s3fYZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb43080-7082-4fe1-d8c6-1a5f34416c10"
      },
      "source": [
        "print(newsgroups['label'].loc[rix])\n",
        "print(newsgroups['label'].loc[cix])\n",
        "\n",
        "#print(newsgroups_train.target_names[newsgroups_train.target[930]])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "politics\n",
            "politics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkuOJ0dpfYZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6231adc8-acad-4304-a257-a14cc178b694"
      },
      "source": [
        "print(newsgroups['text'].loc[rix])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: nsmca@aurora.alaska.edu\n",
            "Subject: 30826\n",
            "Article-I.D.: aurora.1993Apr25.151108.1\n",
            "Organization: University of Alaska Fairbanks\n",
            "Lines: 14\n",
            "Nntp-Posting-Host: acad3.alaska.edu\n",
            "\n",
            "I like option C of the new space station design.. \n",
            "It needs some work, but it is simple and elegant..\n",
            "\n",
            "Its about time someone got into simple construction versus overly complex...\n",
            "\n",
            "Basically just strap some rockets and a nose cone on the habitat and go for\n",
            "it..\n",
            "\n",
            "Might be an idea for a Moon/Mars base to.. \n",
            "\n",
            "Where is Captain Eugenia(sp) when you need it (reference to russian heavy\n",
            "lifter, I think).\n",
            "==\n",
            "Michael Adams, nsmca@acad3.alaska.edu -- I'm not high, just jacked\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN1bFP48fYZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f57b1898-5cce-45ee-ea79-994e07b4d302"
      },
      "source": [
        "print(newsgroups['text'].loc[cix])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: nsmca@aurora.alaska.edu\n",
            "Subject: Space Station Redesign (30826) Option C\n",
            "Article-I.D.: aurora.1993Apr25.214653.1\n",
            "Organization: University of Alaska Fairbanks\n",
            "Lines: 22\n",
            "Nntp-Posting-Host: acad3.alaska.edu\n",
            "\n",
            "In article <1993Apr25.151108.1@aurora.alaska.edu>, nsmca@aurora.alaska.edu writes:\n",
            "> I like option C of the new space station design.. \n",
            "> It needs some work, but it is simple and elegant..\n",
            "> \n",
            "> Its about time someone got into simple construction versus overly complex...\n",
            "> \n",
            "> Basically just strap some rockets and a nose cone on the habitat and go for\n",
            "> it..\n",
            "> \n",
            "> Might be an idea for a Moon/Mars base to.. \n",
            "> \n",
            "> Where is Captain Eugenia(sp) when you need it (reference to russian heavy\n",
            "> lifter, I think).\n",
            "> ==\n",
            "> Michael Adams, nsmca@acad3.alaska.edu -- I'm not high, just jacked\n",
            "> \n",
            "> \n",
            "> \n",
            "> \n",
            "\n",
            "\n",
            "This is a report, I got the subject messed up..\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62xrseScfYZN"
      },
      "source": [
        "> It is a reposting of the message!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp3noSpOfYZN"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}