{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble to be able to run notebooks in Jupyter and Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    notes_home = \"/content/drive/Shared drives/CSC310/notes/\"\n",
    "    user_home = \"/content/drive/My Drive/\"\n",
    "    \n",
    "    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n",
    "    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "## Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "['brown', 'dog', 'fox', 'is', 'jumps', 'lazy', 'over', 'princess', 'quick', 'rudi', 'the']\n",
      "\n",
      "Docterm:\n",
      "      brown  dog  fox  is  jumps  lazy  over  princess  quick  rudi  the\n",
      "doc1      1    1    1   0      1     1     1         0      1     0    1\n",
      "doc2      1    1    0   1      0     1     0         0      0     1    0\n",
      "doc3      0    1    0   0      1     1     1         1      0     0    1\n",
      "\n",
      "Pairwise Distances:\n",
      "          doc1      doc2      doc3\n",
      "doc1  0.000000  2.645751  2.000000\n",
      "doc2  2.645751  0.000000  2.645751\n",
      "doc3  2.000000  2.645751  0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "doc_names = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "docs = [\"the quick brown fox jumps over the lazy dog\",\n",
    "        \"Rudi is a lazy brown dog\",\n",
    "        \"Princess jumps over the lazy dog\"]\n",
    "\n",
    "# process documents\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True)\n",
    "docarray = vectorizer.fit_transform(docs).toarray()\n",
    "coords = vectorizer.get_feature_names()\n",
    "docterm = pandas.DataFrame(data=docarray,index=doc_names,columns=coords)\n",
    "print(\"Coordinates:\")\n",
    "print(coords)\n",
    "print(\"\\nDocterm:\")\n",
    "print(docterm)\n",
    "\n",
    "# pairwise distances\n",
    "distances = euclidean_distances(docterm)\n",
    "distances_df = pandas.DataFrame(data=distances, index=doc_names, columns=doc_names)\n",
    "print(\"\\nPairwise Distances:\")\n",
    "print(distances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n",
      "['sci.space', 'talk.politics.misc']\n",
      "(1058,)\n",
      "From: nickh@CS.CMU.EDU (Nick Haines)\n",
      "Subject: Re: Vandalizing the sky.\n",
      "In-Reply-To: todd@phad.la.locus.com's message of Wed, 21 Apr 93 16:28:00 GMT\n",
      "Originator: nickh@SNOW.FOX.CS.CMU.EDU\n",
      "Nntp-Posting-Host: snow.fox.cs.cmu.edu\n",
      "Organization: School of Computer Science, Carnegie Mellon University\n",
      "\t<1993Apr21.162800.168967@locus.com>\n",
      "Lines: 33\n",
      "\n",
      "In article <1993Apr21.162800.168967@locus.com> todd@phad.la.locus.com (Todd Johnson) writes:\n",
      "\n",
      "   As for advertising -- sure, why not?  A NASA friend and I spent one\n",
      "   drunken night figuring out just exactly how much gold mylar we'd need\n",
      "   to put the golden arches of a certain American fast food organization\n",
      "   on the face of the Moon.  Fortunately, we sobered up in the morning.\n",
      "\n",
      "Hmmm. It actually isn't all that much, is it? Like about 2 million\n",
      "km^2 (if you think that sounds like a lot, it's only a few tens of m^2\n",
      "per burger that said organization sold last year). You'd be best off\n",
      "with a reflective substance that could be sprayed thinly by an\n",
      "unmanned craft in lunar orbit (or, rather, a large set of such craft).\n",
      "If you can get a reasonable albedo it would be visible even at new\n",
      "moon (since the moon itself is quite dark), and _bright_ at full moon.\n",
      "You might have to abandon the colour, though.\n",
      "\n",
      "Buy a cheap launch system, design reusable moon -> lunar orbit\n",
      "unmanned spraying craft, build 50 said craft, establish a lunar base\n",
      "to extract TiO2 (say: for colour you'd be better off with a sulphur\n",
      "compound, I suppose) and some sort of propellant, and Bob's your\n",
      "uncle.  I'll do it for, say, 20 billion dollars (plus changes of\n",
      "identity for me and all my loved ones). Delivery date 2010.\n",
      "\n",
      "Can we get the fast-food chain bidding against the fizzy-drink\n",
      "vendors? Who else might be interested?\n",
      "\n",
      "Would they buy it, given that it's a _lot_ more expensive, and not\n",
      "much more impressive, than putting a large set of several-km\n",
      "inflatable billboards in LEO (or in GEO, visible 24 hours from your\n",
      "key growth market). I'll do _that_ for only $5bn (and the changes of\n",
      "identity).\n",
      "\n",
      "Nick Haines nickh@cmu.edu\n",
      "\n",
      "sci.space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lutz/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "print(len(newsgroups_train.data))\n",
    "print(list(newsgroups_train.target_names))\n",
    "print(newsgroups_train.target.shape)\n",
    "print(newsgroups_train.data[5])\n",
    "print(newsgroups_train.target_names[newsgroups_train.target[5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us compute the docterm matrix for the news articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docarray shape: (1058, 23537)\n",
      "first 10 coords: ['00', '000', '0000', '00000', '000000', '000007', '000021', '000062david42', '00041032', '0004136']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# process documents                                                                                               \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True)\n",
    "docarray = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "print(\"docarray shape: {}\".format(docarray.shape))\n",
    "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us do more filtering: min word freq = 2, only words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docarray shape: (1058, 11836)\n",
      "first 10 coords: ['aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'aas', 'ab', 'abandon', 'abandoned', 'abandonment', 'abbey', 'abc']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from re import sub\n",
    "\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# process documents                                                                                               \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             binary = True, \n",
    "                             min_df=2)\n",
    "new_data = []\n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    new_data.append(sub(\"[^a-zA-Z]\", \" \", newsgroups_train.data[i]))\n",
    "docarray = vectorizer.fit_transform(new_data).toarray()\n",
    "                                                                                                 \n",
    "print(\"docarray shape: {}\".format(docarray.shape))\n",
    "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docarray shape: (1058, 8631)\n",
      "first 10 coords: ['aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'ab', 'abandon', 'abbey', 'abc', 'abdkw', 'abett', 'abid']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from re import sub\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "new_data = []\n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    new_data.append(sub(\"[^a-zA-Z]\", \" \", newsgroups_train.data[i]))\n",
    "\n",
    "lowercase_data = []\n",
    "for i in range(len(new_data)):\n",
    "    lowercase_data.append(new_data[i].lower())\n",
    "\n",
    "stemmed_data = []\n",
    "for i in range(len(lowercase_data)):\n",
    "    words = lowercase_data[i].split()\n",
    "    stemmed_words = []\n",
    "    for w in words:\n",
    "        stemmed_words.append(stemmer.stem(w))\n",
    "    stemmed_data.append(\" \".join(stemmed_words))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, min_df=2)\n",
    "docarray = vectorizer.fit_transform(stemmed_data).toarray()\n",
    "\n",
    "print(\"docarray shape: {}\".format(docarray.shape))\n",
    "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now look at the distances in 8000+ dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "      <th>1051</th>\n",
       "      <th>1052</th>\n",
       "      <th>1053</th>\n",
       "      <th>1054</th>\n",
       "      <th>1055</th>\n",
       "      <th>1056</th>\n",
       "      <th>1057</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>14.730920</td>\n",
       "      <td>15.362291</td>\n",
       "      <td>12.609520</td>\n",
       "      <td>15.779734</td>\n",
       "      <td>16.852300</td>\n",
       "      <td>12.727922</td>\n",
       "      <td>13.711309</td>\n",
       "      <td>12.449900</td>\n",
       "      <td>...</td>\n",
       "      <td>16.462078</td>\n",
       "      <td>12.649111</td>\n",
       "      <td>15.198684</td>\n",
       "      <td>28.195744</td>\n",
       "      <td>13.747727</td>\n",
       "      <td>13.416408</td>\n",
       "      <td>17.860571</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>12.369317</td>\n",
       "      <td>15.231546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.038405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.820275</td>\n",
       "      <td>15.099669</td>\n",
       "      <td>12.206556</td>\n",
       "      <td>15.132746</td>\n",
       "      <td>16.062378</td>\n",
       "      <td>11.916375</td>\n",
       "      <td>12.165525</td>\n",
       "      <td>11.958261</td>\n",
       "      <td>...</td>\n",
       "      <td>15.329710</td>\n",
       "      <td>11.661904</td>\n",
       "      <td>13.964240</td>\n",
       "      <td>28.053520</td>\n",
       "      <td>13.152946</td>\n",
       "      <td>12.727922</td>\n",
       "      <td>17.058722</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>11.704700</td>\n",
       "      <td>13.856406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.730920</td>\n",
       "      <td>13.820275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.716234</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>16.248077</td>\n",
       "      <td>16.155494</td>\n",
       "      <td>13.453624</td>\n",
       "      <td>14.387495</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>...</td>\n",
       "      <td>16.186414</td>\n",
       "      <td>13.453624</td>\n",
       "      <td>15.099669</td>\n",
       "      <td>27.784888</td>\n",
       "      <td>14.628739</td>\n",
       "      <td>13.964240</td>\n",
       "      <td>17.832555</td>\n",
       "      <td>14.594520</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>15.132746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.362291</td>\n",
       "      <td>15.099669</td>\n",
       "      <td>15.716234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.798649</td>\n",
       "      <td>16.703293</td>\n",
       "      <td>17.320508</td>\n",
       "      <td>14.764823</td>\n",
       "      <td>15.362291</td>\n",
       "      <td>14.866069</td>\n",
       "      <td>...</td>\n",
       "      <td>17.233688</td>\n",
       "      <td>14.628739</td>\n",
       "      <td>16.522712</td>\n",
       "      <td>28.231188</td>\n",
       "      <td>15.198684</td>\n",
       "      <td>14.696938</td>\n",
       "      <td>18.193405</td>\n",
       "      <td>15.684387</td>\n",
       "      <td>14.866069</td>\n",
       "      <td>16.186414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.609520</td>\n",
       "      <td>12.206556</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>14.798649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.165751</td>\n",
       "      <td>15.968719</td>\n",
       "      <td>11.357817</td>\n",
       "      <td>12.529964</td>\n",
       "      <td>11.661904</td>\n",
       "      <td>...</td>\n",
       "      <td>15.684387</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>14.560220</td>\n",
       "      <td>27.856777</td>\n",
       "      <td>12.806248</td>\n",
       "      <td>12.369317</td>\n",
       "      <td>17.378147</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.661904</td>\n",
       "      <td>13.964240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>13.416408</td>\n",
       "      <td>12.727922</td>\n",
       "      <td>13.964240</td>\n",
       "      <td>14.696938</td>\n",
       "      <td>12.369317</td>\n",
       "      <td>15.264338</td>\n",
       "      <td>15.874508</td>\n",
       "      <td>11.832160</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>12.288206</td>\n",
       "      <td>...</td>\n",
       "      <td>15.524175</td>\n",
       "      <td>12.165525</td>\n",
       "      <td>14.177447</td>\n",
       "      <td>28.017851</td>\n",
       "      <td>13.304135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.117243</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>12.288206</td>\n",
       "      <td>14.491377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>17.860571</td>\n",
       "      <td>17.058722</td>\n",
       "      <td>17.832555</td>\n",
       "      <td>18.193405</td>\n",
       "      <td>17.378147</td>\n",
       "      <td>18.493242</td>\n",
       "      <td>19.209373</td>\n",
       "      <td>16.822604</td>\n",
       "      <td>17.349352</td>\n",
       "      <td>16.970563</td>\n",
       "      <td>...</td>\n",
       "      <td>19.078784</td>\n",
       "      <td>16.643317</td>\n",
       "      <td>17.720045</td>\n",
       "      <td>28.879058</td>\n",
       "      <td>17.320508</td>\n",
       "      <td>17.117243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.691806</td>\n",
       "      <td>17.029386</td>\n",
       "      <td>18.193405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>14.594520</td>\n",
       "      <td>15.684387</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.394804</td>\n",
       "      <td>16.248077</td>\n",
       "      <td>12.649111</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>12.609520</td>\n",
       "      <td>...</td>\n",
       "      <td>11.532563</td>\n",
       "      <td>12.247449</td>\n",
       "      <td>14.730920</td>\n",
       "      <td>28.160256</td>\n",
       "      <td>13.747727</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>17.691806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.922848</td>\n",
       "      <td>14.422205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>12.369317</td>\n",
       "      <td>11.704700</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>14.866069</td>\n",
       "      <td>11.661904</td>\n",
       "      <td>15.165751</td>\n",
       "      <td>16.093477</td>\n",
       "      <td>11.445523</td>\n",
       "      <td>12.369317</td>\n",
       "      <td>11.224972</td>\n",
       "      <td>...</td>\n",
       "      <td>15.748016</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.928388</td>\n",
       "      <td>27.712813</td>\n",
       "      <td>12.328828</td>\n",
       "      <td>12.288206</td>\n",
       "      <td>17.029386</td>\n",
       "      <td>12.922848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.601471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>15.231546</td>\n",
       "      <td>13.856406</td>\n",
       "      <td>15.132746</td>\n",
       "      <td>16.186414</td>\n",
       "      <td>13.964240</td>\n",
       "      <td>16.401219</td>\n",
       "      <td>16.970563</td>\n",
       "      <td>13.928388</td>\n",
       "      <td>14.491377</td>\n",
       "      <td>13.964240</td>\n",
       "      <td>...</td>\n",
       "      <td>16.278821</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>15.652476</td>\n",
       "      <td>27.331301</td>\n",
       "      <td>14.730920</td>\n",
       "      <td>14.491377</td>\n",
       "      <td>18.193405</td>\n",
       "      <td>14.422205</td>\n",
       "      <td>13.601471</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1058 rows × 1058 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5     \\\n",
       "0      0.000000  13.038405  14.730920  15.362291  12.609520  15.779734   \n",
       "1     13.038405   0.000000  13.820275  15.099669  12.206556  15.132746   \n",
       "2     14.730920  13.820275   0.000000  15.716234  13.564660  16.248077   \n",
       "3     15.362291  15.099669  15.716234   0.000000  14.798649  16.703293   \n",
       "4     12.609520  12.206556  13.564660  14.798649   0.000000  15.165751   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1053  13.416408  12.727922  13.964240  14.696938  12.369317  15.264338   \n",
       "1054  17.860571  17.058722  17.832555  18.193405  17.378147  18.493242   \n",
       "1055  14.000000  13.038405  14.594520  15.684387  13.000000  15.394804   \n",
       "1056  12.369317  11.704700  13.564660  14.866069  11.661904  15.165751   \n",
       "1057  15.231546  13.856406  15.132746  16.186414  13.964240  16.401219   \n",
       "\n",
       "           6          7          8          9     ...       1048       1049  \\\n",
       "0     16.852300  12.727922  13.711309  12.449900  ...  16.462078  12.649111   \n",
       "1     16.062378  11.916375  12.165525  11.958261  ...  15.329710  11.661904   \n",
       "2     16.155494  13.453624  14.387495  13.564660  ...  16.186414  13.453624   \n",
       "3     17.320508  14.764823  15.362291  14.866069  ...  17.233688  14.628739   \n",
       "4     15.968719  11.357817  12.529964  11.661904  ...  15.684387  11.180340   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1053  15.874508  11.832160  13.190906  12.288206  ...  15.524175  12.165525   \n",
       "1054  19.209373  16.822604  17.349352  16.970563  ...  19.078784  16.643317   \n",
       "1055  16.248077  12.649111  13.564660  12.609520  ...  11.532563  12.247449   \n",
       "1056  16.093477  11.445523  12.369317  11.224972  ...  15.748016  11.000000   \n",
       "1057  16.970563  13.928388  14.491377  13.964240  ...  16.278821  13.564660   \n",
       "\n",
       "           1050       1051       1052       1053       1054       1055  \\\n",
       "0     15.198684  28.195744  13.747727  13.416408  17.860571  14.000000   \n",
       "1     13.964240  28.053520  13.152946  12.727922  17.058722  13.038405   \n",
       "2     15.099669  27.784888  14.628739  13.964240  17.832555  14.594520   \n",
       "3     16.522712  28.231188  15.198684  14.696938  18.193405  15.684387   \n",
       "4     14.560220  27.856777  12.806248  12.369317  17.378147  13.000000   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1053  14.177447  28.017851  13.304135   0.000000  17.117243  13.190906   \n",
       "1054  17.720045  28.879058  17.320508  17.117243   0.000000  17.691806   \n",
       "1055  14.730920  28.160256  13.747727  13.190906  17.691806   0.000000   \n",
       "1056  13.928388  27.712813  12.328828  12.288206  17.029386  12.922848   \n",
       "1057  15.652476  27.331301  14.730920  14.491377  18.193405  14.422205   \n",
       "\n",
       "           1056       1057  \n",
       "0     12.369317  15.231546  \n",
       "1     11.704700  13.856406  \n",
       "2     13.564660  15.132746  \n",
       "3     14.866069  16.186414  \n",
       "4     11.661904  13.964240  \n",
       "...         ...        ...  \n",
       "1053  12.288206  14.491377  \n",
       "1054  17.029386  18.193405  \n",
       "1055  12.922848  14.422205  \n",
       "1056   0.000000  13.601471  \n",
       "1057  13.601471   0.000000  \n",
       "\n",
       "[1058 rows x 1058 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = euclidean_distances(docarray)\n",
    "distances_df = pandas.DataFrame(data=distances)\n",
    "distances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out which stories are most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# map 0.0 across the major diagonal into FLOAT_MAX\n",
    "new_df = distances_df.apply(lambda c: c.apply(lambda x: sys.float_info.max if x == 0.0 else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the column with the minimal value\n",
    "new_df.min().idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the row with the minimal value\n",
    "new_df.iloc[:,930].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these two news stories are most similar\n",
    "new_df.iloc[1036, 930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n",
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.target_names[newsgroups_train.target[1036]])\n",
    "print(newsgroups_train.target_names[newsgroups_train.target[930]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: <None>\n",
      "From: bioccnt@otago.ac.nz\n",
      "Organization: University of Otago, Dunedin, New Zealand\n",
      "Nntp-Posting-Host: thorin.otago.ac.nz\n",
      "Lines: 12\n",
      "\n",
      "\n",
      "Can someone please remind me who said a well known quotation? \n",
      "\n",
      "He was sitting atop a rocket awaiting liftoff and afterwards, in answer to\n",
      "the question what he had been thinking about, said (approximately) \"half a\n",
      "million components, each has to work perfectly, each supplied by the lowest\n",
      "bidder.....\" \n",
      "\n",
      "Attribution and correction of the quote would be much appreciated. \n",
      "\n",
      "Clive Trotman\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[1036])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Quotation? Lowest bidder...\n",
      "From: bioccnt@otago.ac.nz\n",
      "Organization: University of Otago, Dunedin, New Zealand\n",
      "Nntp-Posting-Host: thorin.otago.ac.nz\n",
      "Lines: 12\n",
      "\n",
      "\n",
      "Can someone please remind me who said a well known quotation? \n",
      "\n",
      "He was sitting atop a rocket awaiting liftoff and afterwards, in answer to\n",
      "the question what he had been thinking about, said (approximately) \"half a\n",
      "million components, each has to work perfectly, each supplied by the lowest\n",
      "bidder.....\" \n",
      "\n",
      "Attribution and correction of the quote would be much appreciated. \n",
      "\n",
      "Clive Trotman\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[930])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is a reposting where just the subject of the message changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
