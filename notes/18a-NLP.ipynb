{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble to be able to run notebooks in Jupyter and Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    notes_home = \"/content/drive/Shared drives/CSC310/notes/\"\n",
    "    user_home = \"/content/drive/My Drive/\"\n",
    "    \n",
    "    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n",
    "    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "Some of the most important data in our society is represented as unstructured text:\n",
    "\n",
    "* Medical records\n",
    "* Court cases\n",
    "* Insurance documents\n",
    "\n",
    "Other data perhaps not as fundamental but that provides interesting insights into trends and mindsets:\n",
    "\n",
    "* Twitter and other online blogs\n",
    "* News feeds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of these cases we want to extract meaning from the unstructured text:\n",
    "\n",
    "* Perhaps we want to do classification (medical records - high risk/low risk)\n",
    "* Perhaps we want to do a topic analysis of the twitter feeds\n",
    "* Perhaps we would like to construct a recommendation engine for news feeds\n",
    "\n",
    "Regardless, what the task, we need to convert the unstructured text into something that we can work with and perhaps most importantly, our models can work with.\n",
    "\n",
    "☞ The **Vector Model** of text (sometimes called the **Bag-of-Words model**)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Model\n",
    "\n",
    "The vector model converts a document with unstructured text into a **point in an n-dimensional coordinate system** where the coordinate system is defined by the words contained in the text.\n",
    "\n",
    "Consider: the quick brown fox jumps over the lazy dog\n",
    "\n",
    "This text can be represented as the tuple rearranged in alphabetical order,\n",
    "```\n",
    "(brown,dog,fox,jumps,lazy,over,quick,the)\n",
    "```\n",
    "\n",
    "Let’s consider the fact that we have multiple documents and represent them as tuples,\n",
    "\n",
    "* Doc 1: the quick brown fox jumps over the lazy dog &rarr; `(brown,dog,fox,jumps,lazy,over,quick,the)`\n",
    "* Doc 2: rudi is a lazy brown dog &rarr; `(a,brown,dog,is,lazy,rudi)`\n",
    "\n",
    "In order to compare the two documents we create a tuple of the **union** of the words appearing in the \n",
    "two sentence tuples,\n",
    "```\n",
    "(a,brown,dog,fox,is,jumps,lazy,over,quick,rudi,the)\n",
    "```\n",
    "and represent each document as bit vectors with the same length as the tuple above and with 1's and 0's\n",
    "indicating if the document contains a word at a particular tuple position or not,\n",
    "\n",
    "* Doc 1: (0,1,1,1,0,1,1,1,1,0,1)\n",
    "* Doc 2: (1,1,1,0,1,0,1,0,0,1,0)\n",
    "\n",
    "Notice that our word tuple now has become our coordinate system, in this case with 11 dimensions, and each document is now a point in this 11-dimensional space.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Rectangular_coordinates.svg/1280px-Rectangular_coordinates.svg.png\" width=\"350\" height=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about this vector model representation is that we can do mathematics on the documents!\n",
    "\n",
    "Consider adding another document to our collection\n",
    "\n",
    "* Doc 3: princess jumps over the dog &rarr; `(dog,jumps,over,princess,the)`\n",
    "\n",
    "Here we have the new word `princess`, so we need to extend our coordinate system to 12 dimensions by adding `princess`,\n",
    "```\n",
    "(a,brown,dog,fox,is,jumps,lazy,over,princess,quick,rudi,the)\n",
    "```\n",
    "Our three documents become vectors/points in this coordinate system,\n",
    "\n",
    "* Doc 1: the quick brown fox jumps over the lazy dog &rarr; `(0,1,1,1,0,1,1,1,0,1,0,1)`\n",
    "* Doc 2: rudi is a lazy brown dog &rarr; `(1,1,1,0,1,0,1,0,0,0,1,0)`\n",
    "* Doc 3: princess jumps over the dog &rarr; `(0,0,1,0,0,1,1,1,1,0,0,1)`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our vector model of the three docs we can ask questions like this, \n",
    "\n",
    "> Is doc2 or doc3 more similar to doc1?\n",
    "\n",
    "Since all three documents are considered points in our coordinate system we can Euclidean distances in that coordinate system to answer that question. More specifically, we can answer this question by considering the Euclidean distances doc1 &harr; doc2 and doc1 &harr; doc3 in our coordinate system.  \n",
    "\n",
    "The Euclidean distance d in n-dimensional space between two points $p$ and $q$ is defined as:\n",
    "\n",
    "$d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\\ldots+(p_n-q_n)^2}$\n",
    "\n",
    "In our case the point $p$ and $q$ are document vectors and $p_i$ and $q_i$ are the components of the respective \n",
    "vectors.\n",
    "\n",
    "In order to answer our question we have to perform the following computations,\n",
    "\n",
    "* $d(doc1, doc2) = \\sqrt{(0-1)^2+(1-1)^2+(1-1)^2+(1-0)^2+(0-1)^2+(1-0)^2+(1-1)^2+(1-0)^2+(0-0)^2+(1-0)^2+(0-1)^2+(1-0)^2}                      = \\sqrt{1+0+0+1+1+1+0+1+0+1+1+1} = \\sqrt{8} = 2.8$\n",
    "\n",
    "* $d(doc1,doc3) = \\sqrt{(0-0)^2+(1-0)^2+(1-1)^2+(1-0)^2+(0-0)^2+(1-1)^2+(1-1)^2+(1-1)^2+(0-1)^2+(1-0)^2+(0-0)^2+(1-1)^2} = \\sqrt{0+1+0+1+0+0+0+0+1+1+0+0} = \\sqrt{4} = 2.0$\n",
    "\n",
    "> So, doc3 is more similar to doc1 than doc2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Model in Sklearn\n",
    "\n",
    "Let's try the above in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "['brown', 'dog', 'fox', 'is', 'jumps', 'lazy', 'over', 'princess', 'quick', 'rudi', 'the']\n",
      "\n",
      "Docterm:\n",
      "      brown  dog  fox  is  jumps  lazy  over  princess  quick  rudi  the\n",
      "doc1      1    1    1   0      1     1     1         0      1     0    1\n",
      "doc2      1    1    0   1      0     1     0         0      0     1    0\n",
      "doc3      0    1    0   0      1     1     1         1      0     0    1\n",
      "\n",
      "Pairwise Distances:\n",
      "          doc1      doc2      doc3\n",
      "doc1  0.000000  2.645751  2.000000\n",
      "doc2  2.645751  0.000000  2.645751\n",
      "doc3  2.000000  2.645751  0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# set up our documents\n",
    "doc_names = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "docs = [\"the quick brown fox jumps over the lazy dog\",\n",
    "        \"rudi is a lazy brown dog\",\n",
    "        \"princess jumps over the lazy dog\"]\n",
    "\n",
    "# process documents\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True)\n",
    "docarray = vectorizer.fit_transform(docs).toarray()\n",
    "\n",
    "# print out the coordinate system\n",
    "# NOTE: sklearn filters out single character words -- is drops 'a'\n",
    "print(\"Coordinates:\")\n",
    "coords = vectorizer.get_feature_names()\n",
    "print(coords)\n",
    "\n",
    "# print out how each document is represented in this coordinate system\n",
    "# NOTE: traditional this mapping is called the 'docterm' matrix - the mapping\n",
    "#       of each document into the set of terms/words.\n",
    "print(\"\\nDocterm:\")\n",
    "docterm = pandas.DataFrame(data=docarray,index=doc_names,columns=coords)\n",
    "print(docterm)\n",
    "\n",
    "# print pairwise distances between documents\n",
    "distances = euclidean_distances(docterm)\n",
    "distances_df = pandas.DataFrame(data=distances, index=doc_names, columns=doc_names)\n",
    "print(\"\\nPairwise Distances:\")\n",
    "print(distances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just as we computed by hand - doc3 is more similar to doc1 than doc2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Data\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "“The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.”\n",
    "\n",
    "```\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "rec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "misc.forsale\n",
    "talk.politics.misc\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\n",
    "talk.religion.misc\n",
    "alt.atheism\n",
    "soc.religion.christian\n",
    "```\n",
    "\n",
    "Each news item has two fields: \n",
    "* Data - the actual text\n",
    "* Target - index of the category the news item belongs to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1058\n",
      "Target labels: ['sci.space', 'talk.politics.misc']\n",
      "Number of labels: (1058,)\n",
      "Print 5th training instance:\n",
      "From: nickh@CS.CMU.EDU (Nick Haines)\n",
      "Subject: Re: Vandalizing the sky.\n",
      "In-Reply-To: todd@phad.la.locus.com's message of Wed, 21 Apr 93 16:28:00 GMT\n",
      "Originator: nickh@SNOW.FOX.CS.CMU.EDU\n",
      "Nntp-Posting-Host: snow.fox.cs.cmu.edu\n",
      "Organization: School of Computer Science, Carnegie Mellon University\n",
      "\t<1993Apr21.162800.168967@locus.com>\n",
      "Lines: 33\n",
      "\n",
      "In article <1993Apr21.162800.168967@locus.com> todd@phad.la.locus.com (Todd Johnson) writes:\n",
      "\n",
      "   As for advertising -- sure, why not?  A NASA friend and I spent one\n",
      "   drunken night figuring out just exactly how much gold mylar we'd need\n",
      "   to put the golden arches of a certain American fast food organization\n",
      "   on the face of the Moon.  Fortunately, we sobered up in the morning.\n",
      "\n",
      "Hmmm. It actually isn't all that much, is it? Like about 2 million\n",
      "km^2 (if you think that sounds like a lot, it's only a few tens of m^2\n",
      "per burger that said organization sold last year). You'd be best off\n",
      "with a reflective substance that could be sprayed thinly by an\n",
      "unmanned craft in lunar orbit (or, rather, a large set of such craft).\n",
      "If you can get a reasonable albedo it would be visible even at new\n",
      "moon (since the moon itself is quite dark), and _bright_ at full moon.\n",
      "You might have to abandon the colour, though.\n",
      "\n",
      "Buy a cheap launch system, design reusable moon -> lunar orbit\n",
      "unmanned spraying craft, build 50 said craft, establish a lunar base\n",
      "to extract TiO2 (say: for colour you'd be better off with a sulphur\n",
      "compound, I suppose) and some sort of propellant, and Bob's your\n",
      "uncle.  I'll do it for, say, 20 billion dollars (plus changes of\n",
      "identity for me and all my loved ones). Delivery date 2010.\n",
      "\n",
      "Can we get the fast-food chain bidding against the fizzy-drink\n",
      "vendors? Who else might be interested?\n",
      "\n",
      "Would they buy it, given that it's a _lot_ more expensive, and not\n",
      "much more impressive, than putting a large set of several-km\n",
      "inflatable billboards in LEO (or in GEO, visible 24 hours from your\n",
      "key growth market). I'll do _that_ for only $5bn (and the changes of\n",
      "identity).\n",
      "\n",
      "Nick Haines nickh@cmu.edu\n",
      "\n",
      "\n",
      "Label of 5th:\n",
      "sci.space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lutz/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# the categories we want to use\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# print some meta-info about the data\n",
    "print(\"Number of training samples: {}\".format(len(newsgroups_train.data)))\n",
    "print(\"Target labels: {}\".format(list(newsgroups_train.target_names)))\n",
    "print(\"Number of labels: {}\".format(newsgroups_train.target.shape))\n",
    "print(\"Print 5th training instance:\")\n",
    "print(newsgroups_train.data[5])\n",
    "print(\"\\nLabel of 5th:\")\n",
    "print(newsgroups_train.target_names[newsgroups_train.target[5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us compute the docterm matrix for the news articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docarray shape: (1058, 20590)\n",
      "first 10 coords: ['a', 'aa', 'aaa', 'aaaaaaaaaaaa', 'aammmaaaazzzzzziinnnnggggg', 'aams', 'aan', 'aangegeven', 'aantal', 'aao']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# process documents                                                                                               \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", token_pattern = \"[a-zA-Z]+\", binary = True)\n",
    "docarray = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "print(\"docarray shape: {}\".format(docarray.shape))\n",
    "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at at the shape of the docarray, we see that we have 23,000+ different features.  We we look at the features it is clear that there are many \"nonsense\" features.  We need more filtering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us do more filtering\n",
    "\n",
    "From this it is clear that we want to do some additional filtering:\n",
    "* Minimum doc frequency = 2 -- that is, any word has to appear at least twice in the document collection\n",
    "* Delete anything that is not a word - get rid of things like ‘000’ etc., we use the token pattern arg for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docarray shape: (1058, 11862)\n",
      "first 10 coords: ['a', 'aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'aas', 'ab', 'abandon', 'abandoned', 'abandonment', 'abbey']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from re import sub\n",
    "\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# process documents                                                                                               \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             token_pattern = \"[a-zA-Z]+\",\n",
    "                             binary = True, \n",
    "                             min_df=2)\n",
    "docarray = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "                                                                                                 \n",
    "print(\"docarray shape: {}\".format(docarray.shape))\n",
    "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we cut the number of features in the space in half and the features look more like words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "The first few coordinates are now:\n",
    "['aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'aas', 'ab', **'abandon'**, **'abandoned'**, **'abandonment'**, 'abbey', 'abc']\n",
    "\n",
    "Here, we see one more issue, three different shapes of the same root word, in this case *abandon*.\n",
    "\n",
    "> Solution: Stemming!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem or root form.\n",
    "\n",
    "A stemmer for English, for example, should identify the string \"cats\" (and possibly \"catlike\", \"catty\" etc.) as based on the root \"cat\", and \"stems\", \"stemmer\", \"stemming\", \"stemmed\" as based on \"stem\". \n",
    "\n",
    "A stemming algorithm reduces the words \"fishing\", \"fished\", and \"fisher\" to the root word, \"fish\".\n",
    "\n",
    "The most popular stemming algorithm:\n",
    "\n",
    "> The [Porter Stemmer](https://en.wikipedia.org/wiki/Stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docarray shape: (1058, 8657)\n",
      "first 10 coords: ['a', 'aa', 'aammmaaaazzzzzziinnnnggggg', 'aaron', 'ab', 'abandon', 'abbey', 'abc', 'abdkw', 'abett']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# build the stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "# get the default text analyzer from CountVectorizer\n",
    "analyzer = vectorizer = CountVectorizer(analyzer = \"word\", token_pattern = \"[a-zA-Z]+\").build_analyzer()\n",
    "\n",
    "# build a new analyzer that stems using the default analyzer to create the words to be stemmed\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
    "                                 binary=True,\n",
    "                                 min_df=2)\n",
    "docarray = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "\n",
    "print(\"docarray shape: {}\".format(docarray.shape))\n",
    "print(\"first 10 coords: {}\".format(vectorizer.get_feature_names()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now look at the distances in 8000+ dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "      <th>1051</th>\n",
       "      <th>1052</th>\n",
       "      <th>1053</th>\n",
       "      <th>1054</th>\n",
       "      <th>1055</th>\n",
       "      <th>1056</th>\n",
       "      <th>1057</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.228757</td>\n",
       "      <td>14.899664</td>\n",
       "      <td>15.524175</td>\n",
       "      <td>12.884099</td>\n",
       "      <td>15.937377</td>\n",
       "      <td>17.146428</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>13.964240</td>\n",
       "      <td>12.609520</td>\n",
       "      <td>...</td>\n",
       "      <td>16.613248</td>\n",
       "      <td>12.884099</td>\n",
       "      <td>15.394804</td>\n",
       "      <td>28.407745</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.601471</td>\n",
       "      <td>17.972201</td>\n",
       "      <td>14.247807</td>\n",
       "      <td>12.609520</td>\n",
       "      <td>15.524175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.228757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.035669</td>\n",
       "      <td>15.362291</td>\n",
       "      <td>12.529964</td>\n",
       "      <td>15.329710</td>\n",
       "      <td>16.278821</td>\n",
       "      <td>12.288206</td>\n",
       "      <td>12.409674</td>\n",
       "      <td>12.165525</td>\n",
       "      <td>...</td>\n",
       "      <td>15.459625</td>\n",
       "      <td>11.874342</td>\n",
       "      <td>14.212670</td>\n",
       "      <td>28.319605</td>\n",
       "      <td>13.453624</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>17.262677</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>12.083046</td>\n",
       "      <td>14.212670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.899664</td>\n",
       "      <td>14.035669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.779734</td>\n",
       "      <td>13.638182</td>\n",
       "      <td>16.309506</td>\n",
       "      <td>16.370706</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>14.525839</td>\n",
       "      <td>13.674794</td>\n",
       "      <td>...</td>\n",
       "      <td>16.248077</td>\n",
       "      <td>13.564660</td>\n",
       "      <td>15.198684</td>\n",
       "      <td>28.053520</td>\n",
       "      <td>14.696938</td>\n",
       "      <td>14.035669</td>\n",
       "      <td>17.916473</td>\n",
       "      <td>14.662878</td>\n",
       "      <td>13.747727</td>\n",
       "      <td>15.394804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.524175</td>\n",
       "      <td>15.362291</td>\n",
       "      <td>15.779734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.933185</td>\n",
       "      <td>16.822604</td>\n",
       "      <td>17.521415</td>\n",
       "      <td>14.933185</td>\n",
       "      <td>15.556349</td>\n",
       "      <td>15.033296</td>\n",
       "      <td>...</td>\n",
       "      <td>17.349352</td>\n",
       "      <td>14.798649</td>\n",
       "      <td>16.673332</td>\n",
       "      <td>28.460499</td>\n",
       "      <td>15.329710</td>\n",
       "      <td>14.764823</td>\n",
       "      <td>18.275667</td>\n",
       "      <td>15.811388</td>\n",
       "      <td>15.033296</td>\n",
       "      <td>16.492423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.884099</td>\n",
       "      <td>12.529964</td>\n",
       "      <td>13.638182</td>\n",
       "      <td>14.933185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.297059</td>\n",
       "      <td>16.248077</td>\n",
       "      <td>11.575837</td>\n",
       "      <td>12.767145</td>\n",
       "      <td>11.874342</td>\n",
       "      <td>...</td>\n",
       "      <td>15.811388</td>\n",
       "      <td>11.313708</td>\n",
       "      <td>14.730920</td>\n",
       "      <td>28.124722</td>\n",
       "      <td>12.961481</td>\n",
       "      <td>12.449900</td>\n",
       "      <td>17.521415</td>\n",
       "      <td>13.152946</td>\n",
       "      <td>11.874342</td>\n",
       "      <td>14.317821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>13.601471</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>14.035669</td>\n",
       "      <td>14.764823</td>\n",
       "      <td>12.449900</td>\n",
       "      <td>15.394804</td>\n",
       "      <td>16.155494</td>\n",
       "      <td>12.041595</td>\n",
       "      <td>13.416408</td>\n",
       "      <td>12.489996</td>\n",
       "      <td>...</td>\n",
       "      <td>15.652476</td>\n",
       "      <td>12.288206</td>\n",
       "      <td>14.352700</td>\n",
       "      <td>28.284271</td>\n",
       "      <td>13.453624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.262677</td>\n",
       "      <td>13.341664</td>\n",
       "      <td>12.409674</td>\n",
       "      <td>14.832397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>17.972201</td>\n",
       "      <td>17.262677</td>\n",
       "      <td>17.916473</td>\n",
       "      <td>18.275667</td>\n",
       "      <td>17.521415</td>\n",
       "      <td>18.574176</td>\n",
       "      <td>19.364917</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.549929</td>\n",
       "      <td>17.029386</td>\n",
       "      <td>...</td>\n",
       "      <td>19.157244</td>\n",
       "      <td>16.822604</td>\n",
       "      <td>17.888544</td>\n",
       "      <td>29.086079</td>\n",
       "      <td>17.464249</td>\n",
       "      <td>17.262677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.832555</td>\n",
       "      <td>17.262677</td>\n",
       "      <td>18.384776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>14.247807</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>14.662878</td>\n",
       "      <td>15.811388</td>\n",
       "      <td>13.152946</td>\n",
       "      <td>15.524175</td>\n",
       "      <td>16.401219</td>\n",
       "      <td>12.845233</td>\n",
       "      <td>13.711309</td>\n",
       "      <td>12.727922</td>\n",
       "      <td>...</td>\n",
       "      <td>11.618950</td>\n",
       "      <td>12.369317</td>\n",
       "      <td>14.899664</td>\n",
       "      <td>28.425341</td>\n",
       "      <td>13.892444</td>\n",
       "      <td>13.341664</td>\n",
       "      <td>17.832555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>14.696938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>12.609520</td>\n",
       "      <td>12.083046</td>\n",
       "      <td>13.747727</td>\n",
       "      <td>15.033296</td>\n",
       "      <td>11.874342</td>\n",
       "      <td>15.394804</td>\n",
       "      <td>16.401219</td>\n",
       "      <td>11.789826</td>\n",
       "      <td>12.649111</td>\n",
       "      <td>11.575837</td>\n",
       "      <td>...</td>\n",
       "      <td>15.968719</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>14.071247</td>\n",
       "      <td>28.035692</td>\n",
       "      <td>12.529964</td>\n",
       "      <td>12.409674</td>\n",
       "      <td>17.262677</td>\n",
       "      <td>13.190906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.071247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>15.524175</td>\n",
       "      <td>14.212670</td>\n",
       "      <td>15.394804</td>\n",
       "      <td>16.492423</td>\n",
       "      <td>14.317821</td>\n",
       "      <td>16.583124</td>\n",
       "      <td>17.233688</td>\n",
       "      <td>14.317821</td>\n",
       "      <td>14.832397</td>\n",
       "      <td>14.142136</td>\n",
       "      <td>...</td>\n",
       "      <td>16.522712</td>\n",
       "      <td>13.892444</td>\n",
       "      <td>15.937377</td>\n",
       "      <td>27.531800</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.832397</td>\n",
       "      <td>18.384776</td>\n",
       "      <td>14.696938</td>\n",
       "      <td>14.071247</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1058 rows × 1058 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5     \\\n",
       "0      0.000000  13.228757  14.899664  15.524175  12.884099  15.937377   \n",
       "1     13.228757   0.000000  14.035669  15.362291  12.529964  15.329710   \n",
       "2     14.899664  14.035669   0.000000  15.779734  13.638182  16.309506   \n",
       "3     15.524175  15.362291  15.779734   0.000000  14.933185  16.822604   \n",
       "4     12.884099  12.529964  13.638182  14.933185   0.000000  15.297059   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1053  13.601471  13.038405  14.035669  14.764823  12.449900  15.394804   \n",
       "1054  17.972201  17.262677  17.916473  18.275667  17.521415  18.574176   \n",
       "1055  14.247807  13.190906  14.662878  15.811388  13.152946  15.524175   \n",
       "1056  12.609520  12.083046  13.747727  15.033296  11.874342  15.394804   \n",
       "1057  15.524175  14.212670  15.394804  16.492423  14.317821  16.583124   \n",
       "\n",
       "           6          7          8          9     ...       1048       1049  \\\n",
       "0     17.146428  13.038405  13.964240  12.609520  ...  16.613248  12.884099   \n",
       "1     16.278821  12.288206  12.409674  12.165525  ...  15.459625  11.874342   \n",
       "2     16.370706  13.564660  14.525839  13.674794  ...  16.248077  13.564660   \n",
       "3     17.521415  14.933185  15.556349  15.033296  ...  17.349352  14.798649   \n",
       "4     16.248077  11.575837  12.767145  11.874342  ...  15.811388  11.313708   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1053  16.155494  12.041595  13.416408  12.489996  ...  15.652476  12.288206   \n",
       "1054  19.364917  17.000000  17.549929  17.029386  ...  19.157244  16.822604   \n",
       "1055  16.401219  12.845233  13.711309  12.727922  ...  11.618950  12.369317   \n",
       "1056  16.401219  11.789826  12.649111  11.575837  ...  15.968719  11.180340   \n",
       "1057  17.233688  14.317821  14.832397  14.142136  ...  16.522712  13.892444   \n",
       "\n",
       "           1050       1051       1052       1053       1054       1055  \\\n",
       "0     15.394804  28.407745  14.000000  13.601471  17.972201  14.247807   \n",
       "1     14.212670  28.319605  13.453624  13.038405  17.262677  13.190906   \n",
       "2     15.198684  28.053520  14.696938  14.035669  17.916473  14.662878   \n",
       "3     16.673332  28.460499  15.329710  14.764823  18.275667  15.811388   \n",
       "4     14.730920  28.124722  12.961481  12.449900  17.521415  13.152946   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1053  14.352700  28.284271  13.453624   0.000000  17.262677  13.341664   \n",
       "1054  17.888544  29.086079  17.464249  17.262677   0.000000  17.832555   \n",
       "1055  14.899664  28.425341  13.892444  13.341664  17.832555   0.000000   \n",
       "1056  14.071247  28.035692  12.529964  12.409674  17.262677  13.190906   \n",
       "1057  15.937377  27.531800  15.000000  14.832397  18.384776  14.696938   \n",
       "\n",
       "           1056       1057  \n",
       "0     12.609520  15.524175  \n",
       "1     12.083046  14.212670  \n",
       "2     13.747727  15.394804  \n",
       "3     15.033296  16.492423  \n",
       "4     11.874342  14.317821  \n",
       "...         ...        ...  \n",
       "1053  12.409674  14.832397  \n",
       "1054  17.262677  18.384776  \n",
       "1055  13.190906  14.696938  \n",
       "1056   0.000000  14.071247  \n",
       "1057  14.071247   0.000000  \n",
       "\n",
       "[1058 rows x 1058 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = euclidean_distances(docarray)\n",
    "distances_df = pandas.DataFrame(data=distances)\n",
    "distances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out which stories are most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# map 0.0 across the major diagonal into FLOAT_MAX\n",
    "new_df = distances_df.apply(lambda c: c.apply(lambda x: sys.float_info.max if x == 0.0 else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the column with the minimal value\n",
    "new_df.min().idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the row with the minimal value\n",
    "new_df.iloc[:,930].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these two news stories are most similar\n",
    "new_df.iloc[1036, 930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n",
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.target_names[newsgroups_train.target[1036]])\n",
    "print(newsgroups_train.target_names[newsgroups_train.target[930]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: <None>\n",
      "From: bioccnt@otago.ac.nz\n",
      "Organization: University of Otago, Dunedin, New Zealand\n",
      "Nntp-Posting-Host: thorin.otago.ac.nz\n",
      "Lines: 12\n",
      "\n",
      "\n",
      "Can someone please remind me who said a well known quotation? \n",
      "\n",
      "He was sitting atop a rocket awaiting liftoff and afterwards, in answer to\n",
      "the question what he had been thinking about, said (approximately) \"half a\n",
      "million components, each has to work perfectly, each supplied by the lowest\n",
      "bidder.....\" \n",
      "\n",
      "Attribution and correction of the quote would be much appreciated. \n",
      "\n",
      "Clive Trotman\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[1036])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Quotation? Lowest bidder...\n",
      "From: bioccnt@otago.ac.nz\n",
      "Organization: University of Otago, Dunedin, New Zealand\n",
      "Nntp-Posting-Host: thorin.otago.ac.nz\n",
      "Lines: 12\n",
      "\n",
      "\n",
      "Can someone please remind me who said a well known quotation? \n",
      "\n",
      "He was sitting atop a rocket awaiting liftoff and afterwards, in answer to\n",
      "the question what he had been thinking about, said (approximately) \"half a\n",
      "million components, each has to work perfectly, each supplied by the lowest\n",
      "bidder.....\" \n",
      "\n",
      "Attribution and correction of the quote would be much appreciated. \n",
      "\n",
      "Clive Trotman\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[930])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is a reposting where just the subject of the message changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
