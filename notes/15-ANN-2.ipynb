{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"15-ANN-2.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"eYWaaaNJfNH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638417353921,"user_tz":300,"elapsed":584,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"dd8c03f3-9acb-40b3-d8e8-493c6c222a29"},"source":["###### Set Up #####\n","# verify our folder with the data and module assets is installed\n","# if it is installed make sure it is the latest\n","!test -e ds-assets && cd ds-assets && git pull && cd ..\n","# if it is not installed clone it \n","!test ! -e ds-assets && git clone https://github.com/lutzhamel/ds-assets.git\n","# point to the folder with the assets\n","home = \"ds-assets/assets/\" \n","import sys\n","sys.path.append(home)      # add home folder to module search path"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"markdown","metadata":{"id":"hLDTyHiyfNH9"},"source":["# Constructing a basic ANN/MLP\n","\n","Let's build some MLPs.  A fundamental problem with MLP design is the sheer number of design possibilities of these models.  The MLP classisfier as part of the sklearn package has 23 (!) tunable paramters.  The good news is that all of these parameters except for the architectural parameters have good default values. For the architectural parameters a good starting point is an MLP  with a single hidden layer where the number of nodes in the hidden layer is computed as follows,\n","\n","$ \\#\\mbox{nodes} = 2 \\times \\#\\mbox{vars}$\n","\n","That is the number of hidden nodes is twice the number of independent variables in the training data.  Let's try this using the breast cancer dataset,"]},{"cell_type":"code","metadata":{"id":"74XIcu6lLroz","executionInfo":{"status":"ok","timestamp":1638417354328,"user_tz":300,"elapsed":409,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}}},"source":["# set up\n","import pandas as pd\n","import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from confint import classification_confint"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FUyuHeUiLvn8","executionInfo":{"status":"ok","timestamp":1638417354329,"user_tz":300,"elapsed":8,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"a2512e10-d4b7-4d47-b13d-801b5029d320"},"source":["# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","df = df.drop(['ID'],axis=1)\n","\n","\n","X  = df.drop(['Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","print(\"Shape: {}\".format(X.shape))\n","\n","# sestup training data\n","datasets = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=3)\n","train_X, test_X, train_y, test_y = datasets"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape: (569, 30)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6bakptkLkR3Y"},"source":["A look at the shape of the training data we see that there are 30 independent variables. Applying our rule from above means that we should construct an MLP with a single hidden layer that contains 60 nodes."]},{"cell_type":"code","metadata":{"id":"xcnxVmy4fNH-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638417354893,"user_tz":300,"elapsed":568,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"c13bada2-ba71-45ea-c69e-e48bfb599b40"},"source":["# neural network\n","model = MLPClassifier(hidden_layer_sizes=(60,), random_state=1)\n","\n","# train and test the model\n","model.fit(train_X, train_y)\n","predict_y = model.predict(test_X)\n","acc = accuracy_score(test_y, predict_y)\n","lb, ub = classification_confint(acc, test_X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.93 (0.88, 0.98)\n"]}]},{"cell_type":"markdown","metadata":{"id":"rUORcmWDfNIA"},"source":["## MLP Grid Search\n","\n","We can also perform a grid search to find the optimal network. However, beware that a grid search over all possible parameters of an MLP is almost impossible:  Too many different combinations possible and training MLPs is sloooowwww.  To mitigate this we concentrate on a couple of key parameters to search over."]},{"cell_type":"code","metadata":{"id":"655pgMsffNIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638417454577,"user_tz":300,"elapsed":99688,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"1ed1e128-4103-4a8b-a525-20a42e938c6d"},"source":["# set up\n","import pandas as pd\n","import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","from confint import classification_confint\n","\n","# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","df = df.drop(['ID'],axis=1)\n","X  = df.drop(['Diagnosis'],axis=1)\n","actual_y = df['Diagnosis']\n","\n","# neural network\n","model = MLPClassifier(max_iter=10000, random_state=1)\n","\n","# grid search\n","param_grid = {'hidden_layer_sizes': [ (30,), (60,), (120,),\n","                                      (30,30), (30, 60), (30, 120),\n","                                      (60, 30), (60,60), (60, 120),\n","                                      (120, 30), (120, 60), (120, 120)\n","                                    ],\n","              'activation' : ['logistic', 'tanh', 'relu']\n","             }\n","grid = GridSearchCV(model, param_grid, cv=3) # 3-fold cross-validation\n","grid.fit(X, actual_y)\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n","\n","# evaluate the best model\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(X)\n","acc = accuracy_score(actual_y, predict_y)\n","lb,ub = classification_confint(acc,X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Grid Search: best parameters: {'activation': 'logistic', 'hidden_layer_sizes': (30, 30)}\n","Accuracy: 0.96 (0.94,0.97)\n"]}]},{"cell_type":"markdown","metadata":{"id":"j8RWVg-GTmGf"},"source":["Interestingly enough, this network constructed using the graidsearch is a network with two hidden layers year with 30 nodes in it."]},{"cell_type":"markdown","metadata":{"id":"CsUu-8bMfNIB"},"source":["## Model Comparison\n","\n","The accuracy of the network we constructed using our rule of thumb was,\n","```\n","0.93 (0.88, 0.98)\n","```\n","and the accuracy of our network constructed using a gridsearch was,\n","```\n","0.96 (0.94,0.97)\n","```\n","Notice that even though our first instinct is that the optimized MLP is much better than the straight forward MLP using our rule of thumb the difference in accuracy between these two models is statistically not significant because their confidence intervals overlap!  In this case we might look at other criteria such as the confusion matrix to make use model selection choice."]},{"cell_type":"markdown","metadata":{"id":"mpK8RU0qfNIB"},"source":["# Team Exercise\n","\n","In this exercise we use a data set to predict cervical cancer risk based\n","on social and behavior characteristics.  For details on the dataset please see the [template-ca-cervix.ipynb](https://colab.research.google.com/github/lutzhamel/ds-notes/blob/master/templates/template-ca-cervix.ipynb) template.  You can use this template as the starting point of your analysis.\n","\n","Do the following:\n","\n","* Build a 1-hidden-layer MLP according to our rule of thumb and using the 'relu' activation function (train and test on full data set).\n","* Build a best 2-layer MLP using grid-search to search over layer sizes and activation functions.  For the activation functions use 'logistic' and 'relu'. For more details see the [MLP documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n","* Build a best decision tree using grid-search for this data set.\n","\n","Evaluation:\n","* Which one of the above models has the best accuracy?\n","* Are the differences in accuracy between the three models statistically significant?\n","\n","For more details please see BrightSpace Assignment #4"]}]}