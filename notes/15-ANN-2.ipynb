{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"eYWaaaNJfNH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709650292841,"user_tz":300,"elapsed":509,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"d6210fcb-796b-42c3-96c7-ca6f800925f3"},"source":["###### Set Up #####\n","# verify our folder with the data and module assets is installed\n","# if it is installed make sure it is the latest\n","!test -e ds-assets && cd ds-assets && git pull && cd ..\n","# if it is not installed clone it\n","!test ! -e ds-assets && git clone https://github.com/lutzhamel/ds-assets.git\n","# point to the folder with the assets\n","home = \"ds-assets/assets/\"\n","import sys\n","sys.path.append(home)      # add home folder to module search path"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"markdown","metadata":{"id":"hLDTyHiyfNH9"},"source":["# Constructing a basic ANN/MLP\n","\n","Let's build some MLPs.  A fundamental problem with MLP design is the sheer number of design possibilities of these models.  The MLP classisfier as part of the sklearn package has 23 (!) tunable paramters.  The good news is that all of these parameters except for the architectural parameters and the maximum number of training iterations have good default values. For the architectural parameters a good starting point is an MLP  with a single hidden layer where the number of nodes in the hidden layer is computed as follows,\n","\n"," $ \\#\\mbox{hidden nodes} = 2 \\times \\#\\mbox{vars}$\n","\n","That is the number of hidden nodes is twice the number of independent variables in the training data.  For the maximum number of training iterations we simply choose a very large value, e.g. 10,000. Let's try this using the breast cancer dataset,"]},{"cell_type":"code","metadata":{"id":"74XIcu6lLroz","executionInfo":{"status":"ok","timestamp":1709650292842,"user_tz":300,"elapsed":3,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["# set up\n","import pandas as pd\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score\n","from confint import classification_confint"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUyuHeUiLvn8","executionInfo":{"status":"ok","timestamp":1709650292842,"user_tz":300,"elapsed":2,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["# get data\n","df = pd.read_csv(home+\"wdbc.csv\").drop(columns=['ID'])\n","\n","X  = df.drop(columns=['Diagnosis'])\n","y = df['Diagnosis']"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6bakptkLkR3Y"},"source":["Looking at the shape of the training data we see that there are 30 independent variables. Applying our rule from above means that we should construct an MLP with a single hidden layer that contains 60 nodes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsFdJqQr8Za1","executionInfo":{"status":"ok","timestamp":1709650293553,"user_tz":300,"elapsed":713,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"4509a539-87c6-43ac-d287-65ad1c08bcee"},"source":["# neural network\n","nnodes = 2*len(list(X.columns))\n","print(\"We are using {} nodes\".format(nnodes))\n","\n","# buils model object\n","model = MLPClassifier(hidden_layer_sizes=(nnodes,), max_iter=10000, random_state=1)\n","\n","# train the model\n","model.fit(X, y) # ANN wants a series as the target\n","\n","# test the model with resubstitution error (use training data for testing)\n","predict_y = model.predict(X)\n","acc = accuracy_score(y, predict_y)\n","lb, ub = classification_confint(acc, X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["We are using 60 nodes\n","Accuracy: 0.95 (0.93, 0.96)\n"]}]},{"cell_type":"markdown","metadata":{"id":"191fBfjf32tc"},"source":["The accuracy of this classifier is encouraging given that we constructed it using just our rule of thumb.  Also, you might be surprised in that we are using the whole data set both as training as well as testing data.  In this instance that is ok because we are not performing a model search, we simply want to see how our rule of thumb performs.  If we were performing a model search then we would have to resort to train-test splits or cross-validation as we do in the grid search below."]},{"cell_type":"markdown","metadata":{"id":"rUORcmWDfNIA"},"source":["# MLP Grid Search\n","\n","We have to perform a grid search to find the optimal network.\n","\n","Beware that a grid search over all possible parameters of an MLP is almost impossible:  Too many different combinations possible and training MLPs is sloooowwww.  To mitigate this we concentrate on a couple of key parameters to search over (see the comments in the code)."]},{"cell_type":"code","metadata":{"id":"655pgMsffNIA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dff0c685-1acf-4778-fdb3-19e37c65ea7d","executionInfo":{"status":"ok","timestamp":1709650423907,"user_tz":300,"elapsed":130356,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["# set up\n","import pandas as pd\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","from confint import classification_confint\n","\n","# get data\n","df = pd.read_csv(home+\"wdbc.csv\").drop(columns=['ID'])\n","X  = df.drop(columns=['Diagnosis'])\n","y = df['Diagnosis'] # ANN wants this to be a series\n","\n","# neural network object\n","model = MLPClassifier(max_iter=10000, random_state=1)\n","\n","# grid search\n","# We set up a grid search over the architecture and activation functions.\n","# In the architecture search we limit ourselves to node values that are multiples\n","# of the number of independent variables in the training data.  Also, we\n","# limit ourselves to a maximum of two hidden layers.\n","\n","nnodes = 2*len(list(X.columns))\n","\n","param_grid = {\n","    # search over different architectures\n","    'hidden_layer_sizes':\n","      [\n","      # single layer MLP: vary size by nnodes with multipliers of 2\n","      (nnodes//2,), (nnodes,), (nnodes*2,),\n","      # 2 layers: first fixed at nnodes/2, second varying\n","      (nnodes//2,nnodes//2), (nnodes//2, nnodes), (nnodes//2, nnodes*2),\n","      # 2 layers: first fixed at nnodes, second varying\n","      (nnodes, nnodes//2), (nnodes,nnodes), (nnodes, nnodes*2),\n","      # 2 layers: first nnodes*2, second varying\n","      (nnodes*2, nnodes//2), (nnodes*2, nnodes), (nnodes*2, nnodes*2)\n","      ],\n","    # search different activation functions\n","    'activation' : ['logistic', 'tanh', 'relu']\n","}\n","\n","# use 3-fold cross-validation otherwse grid search takes too long\n","grid = GridSearchCV(model, param_grid, cv=3)\n","grid.fit(X, y)\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n","\n","# evaluate the best model\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(X)\n","acc = accuracy_score(y, predict_y)\n","lb,ub = classification_confint(acc,X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Grid Search: best parameters: {'activation': 'logistic', 'hidden_layer_sizes': (30, 30)}\n","Accuracy: 0.96 (0.94,0.97)\n"]}]},{"cell_type":"markdown","metadata":{"id":"j8RWVg-GTmGf"},"source":["Interestingly enough, this network constructed using the grid search is a network with two hidden layers each with 30 nodes in it."]},{"cell_type":"markdown","metadata":{"id":"CsUu-8bMfNIB"},"source":["# Model Comparison\n","\n","The accuracy of the network we constructed using our rule of thumb was,\n","```\n","0.95 (0.93, 0.96)\n","```\n","and the accuracy of our network constructed using a grid search was,\n","```\n","96% (94%, 97%)\n","```\n","Our first observation is that our rule of thumb got us pretty close to the performance of our optimized network.\n","The second observation is that **the difference in accuracy between these two models is not statistically significant** because their confidence intervals overlap.  "]},{"cell_type":"markdown","metadata":{"id":"mpK8RU0qfNIB"},"source":["# Project\n","\n","\n","Please see BrightSpace."]}]}