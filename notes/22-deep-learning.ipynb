{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble to be able to run notebooks in Jupyter and Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    notes_home = \"/content/drive/Shared drives/CSC310/notes/\"\n",
    "    user_home = \"/content/drive/My Drive/\"\n",
    "    \n",
    "    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n",
    "    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is part of a broader family of machine learning methods based on the layers used in artificial neural networks.  Here is how deep learning fits into the broader AI picture,\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/AI-ML-DL.png\" height='1000' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. \n",
    "\n",
    "DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network\n",
    "\n",
    "DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network doed accurately recognize a particular pattern, an algorithm (backpropagation) will adjust the weights appropriately.\n",
    "\n",
    "<img src='https://i.stack.imgur.com/OH3gI.png' height='250' width='750'>\n",
    "\n",
    "The difference between ANNs and DNNs is the number of hidden layers in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Deep Learning\n",
    "\n",
    "DNNs are particularly well suited for NLP.  But before we look at this application we need to talk about `word embeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Vector Model to Word Embeddings\n",
    "\n",
    "### The Vector Model\n",
    "\n",
    "In the [document vector model](https://en.wikipedia.org/wiki/Vector_space_model) of a collection of documents each word that appears in the collection is defined as a dimension in the corresponding vector model and each document appears as a feature vector in this model.  Consider the following figure,\n",
    "\n",
    "<!-- ![](https://ahmedbesbes.com/images/article_5/tfidf.jpg) -->\n",
    "\n",
    "<!-- ![](https://raw.githubusercontent.com/lutzhamel/fake-news/master/term-doc.jpg) -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lutzhamel/fake-news/master/term-doc.jpg\" height=\"350\" width=\"450\">\n",
    "\n",
    "Here each column represents the feature vector of one of the documents in the collection and the rows are the features or dimensions of the vectors. Notice that there is one feature for each word that appears in the collection of documents. The column vectors can be used for training a text classifier, that is, the transpose of the term-doc matrix shown here can be used directly as a training set for a classifier. \n",
    " \n",
    "The fields in the term-doc matrix are the counts of how many times a word appears in a document.  However, there are many ways to encode the occurences of words in the collection within this matrix. In the binary `CountVectorizer`  the fields are just 0 and 1 indicating whether a particular word appears in a document or not. Perhaps the most famous encoding is [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf), short for term frequency–inverse document frequency.\n",
    "\n",
    "### Disadvantages of the Vector Model\n",
    "\n",
    "The vector representation of documents has two important consequences for document classification problems: \n",
    "\n",
    "* The order and contexts of words are lost. To see the importance of the word context consider these [two sentences](https://jair.org/index.php/jair/article/view/11030): “it was not good, it was actually quite bad” and “it was not bad, it was actually quite good”.  The vector representation of these sentences is exactly the same but they obviously have very different meanings or classifications.  The vector representation of  documents is often  called the *bag of words* representation referring to the fact that it loses all order and context information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Docterm:\n",
      "      actually  bad  good  it  not  quite  was\n",
      "sen1         1    1     1   1    1      1    1\n",
      "sen2         1    1     1   1    1      1    1\n"
     ]
    }
   ],
   "source": [
    "# show the vector models of our two sentences\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "doc_names = [\"sen1\", \"sen2\"]\n",
    "docs = [\"it was not good, it was actually quite bad\",\n",
    "        \"it was not bad, it was actually quite good\"]\n",
    "\n",
    "# process documents\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True)\n",
    "docarray = vectorizer.fit_transform(docs).toarray()\n",
    "coords = vectorizer.get_feature_names()\n",
    "docterm = pd.DataFrame(data=docarray,index=doc_names,columns=coords)\n",
    "print(\"\\nDocterm:\")\n",
    "print(docterm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Semantic similarities between words cannot be represented. To see the importance of semantic similarity consider one document that discusses dogs and another document that discusses puppies. From a vector model perspective the feature set for these two documents will not intersect in terms of the notion of dog because the vector model simply considers dogs and puppies to be two different features and the similarity of these documents will not be apparent to a machine learning algorithm.\n",
    "```\n",
    "Docterm:\n",
    "           ...  dogs  puppies  ...\n",
    "sen1    ...     1        0  ...\n",
    "sen2    ...     0        1  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "Here words are represented as [*embedding vectors*](https://en.wikipedia.org/wiki/Word_embedding) with the idea that two words that are semantically similar to each other have similar vectors. Consider the following figure,\n",
    "\n",
    "\n",
    " <img src=\"https://www.researchgate.net/profile/Tom_Kenter/publication/325451970/figure/fig1/AS:632023664304129@1527697592901/Visualization-of-3-dimensional-word-embeddings.png\" height=\"200\" width=\"350\"/>\n",
    "\n",
    "This figure represents a 3D embedding space and we can see that concepts that are similar to each other are close together in this embedding space.  Therefore the similarity of our two documents talking about dogs and puppies is expressed as a \"vector simililarity\" which is most often computed as the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) rather than comparing features directly,\n",
    "\n",
    "<img src=\"http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png\" height=\"200\" width=\"800\">\n",
    "\n",
    "In other words, we are comparing the semantic notion of dogs and puppies rather than the precise syntax of the words.\n",
    "\n",
    "Here is another example of five embedding vectors in a 2D embedding space,\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lutzhamel/word2vec-simplified/master/word-vectors.png\">\n",
    "\n",
    "The five vectors represent the words,\n",
    "\n",
    "* Red – Queen\n",
    "* Blue – King\n",
    "* Green – Man\n",
    "* Black – Woman\n",
    "* Yellow – Oil\n",
    "\n",
    "Applying vector similarity here it becomes obvious that the vectors representing 'Man' and 'Woman' are most similar to each other, that is, they are semantically most closely related.  It is also easy to see that the vector representing 'Oil' is most dissimilar to all the other vectors.\n",
    "\n",
    "One of the more popular word embeddings is [word2vec](https://en.wikipedia.org/wiki/Word2vec) created by Google which embeds words in a 300D vector space.  Sentences are now represented as a `1en x 300` matrix (or tensor in DNN terminology) where `len` is the number of words in the sentence and `300` is the embedding dimension.\n",
    "\n",
    "A GitHub repository that explores word2vec a little bit further can be found [here](https://github.com/lutzhamel/word2vec-simplified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Documents for DNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep neural networks documents are no longer compressed into a vector representation of just word occurences.  Instead, deep neural networks process actual sequences of words (coded as a integers) as they appear in the documents thereby *maintaining the order and contexts* of words. Consider the following code snippet using the [Keras](https://keras.io) tokenizer applied to our two sentences from above,\n",
    " ```python\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()\n",
    "# train tokenizer\n",
    "tok.fit_on_texts([\"it was not good, it was actually quite bad\"])\n",
    "# print sequences\n",
    "print(tok.texts_to_sequences([\"it was not good, it was actually quite bad\"])[0])\n",
    "print(tok.texts_to_sequences([\"it was not bad, it was actually quite good\"])[0])\n",
    " ```\n",
    " This will print out the following sequences,\n",
    " ```\n",
    "[1, 2, 3, 4, 1, 2, 5, 6, 7]\n",
    "[1, 2, 3, 7, 1, 2, 5, 6, 4]\n",
    " ```\n",
    " with a `word_index` of,\n",
    " ```\n",
    " {'it': 1, 'was': 2, 'not': 3, 'good': 4, 'actually': 5, 'quite': 6, 'bad': 7}\n",
    "```\n",
    "These sequences can be directly fed into a deep neural network for training and classification. Notice that word order and context are nicely preserved in this representation.  This is very different from the Naive Bayes training from our previous NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deep Neural Network for Fake-News\n",
    "\n",
    "The deep neural network we are using for our experiment can be seen here as a Python implementation using the [Keras](https://keras.io) deep learning library,\n",
    "```python\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        # part 1: word and sequence processing\n",
    "        layers.Embedding(num_words,\n",
    "                         EMBEDDING_DIM,\n",
    "                         input_length=MAX_SEQUENCE_LENGTH,\n",
    "                         trainable=True),\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "\n",
    "        # part 2: classification\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "Our DNN can be broken down into two distinct parts. The first part consists of three layers and is responsible for  word and sequence processing:\n",
    "1. The Embedding layer - learn word embeddings.\n",
    "2. The Convolution layer - learn patterns throughout the text sequences.\n",
    "3. The Pooling layer - filter out the interesting sequence patterns.\n",
    "\n",
    "The second part consists of two layers,\n",
    "\n",
    "1. A Dense layer with a ReLU activation function.\n",
    "2. A Dense layer (also the output layer) with a Sigmoid activation function.\n",
    "\n",
    "This part of the DNN can be viewed as a traditional feed-foward, back-propagation neural network with one hidden layer operating on a feature vector of length 128 computed by the first part of the DNN.   In order to see this perhaps a bit clearer, here is the summary of the DNN as compiled by Keras,\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 5000, 300)         7500300   \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 4996, 128)         192128    \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 128)               16512     \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 7,709,069\n",
    "Trainable params: 7,709,069\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "The `None` in the *Output Shape* column simply denotes the *current batch size default*. That  means the pooling layer computes a feature vector of size 128 which is passed into dense layers of the feedforward network as we mentioned above.\n",
    "\n",
    "The overall structure of the DNN can be understood as a preprocessor defined in the first part that is being trained to map text sequences into feature vectors in such a way that the weights of the second part can be trained to obtain optimal classification results from the overall network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "This network was trained for 10 epochs with a batch size of 128 using a 80-20 training/hold-out set. A couple of notes on additional parameters:  The vast majority of documents in this collection is of length 5000 or less. So for the maximum input sequence length for the DNN we chose 5000 words.  There are roughly 100,000 unique words in this collection of documents. We arbitrarily limited the dictionary that the DNN can learn to 25% of that: 25,000 words.  Finally, for the embedding dimension we chose 300 simply because that is the default embedding dimension for both word2vec and GloVe.\n",
    "\n",
    "The results were quite impressive,\n",
    "\n",
    "> A 97% accuracy with a 95% confidence interval of (96%, 98%).\n",
    "\n",
    "The performance increase can be shown to be statistically significant compared to the performance of the Naive Bayes classifier and perhaps a bit surprising given the relative simplicity of the DNN.  One conclusion that one might draw is that semantic similarity between words and word order or context are crucial for document classification. More details on this experiment can be found [here](https://github.com/lutzhamel/fake-news)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
