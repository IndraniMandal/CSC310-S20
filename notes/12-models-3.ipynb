{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"12-models-3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"LlVbRD8lY5nI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638295430682,"user_tz":300,"elapsed":502,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"3d7555ef-b9a8-498b-c66b-32e5a897aab7"},"source":["###### Set Up #####\n","# verify our folder with the data and module assets is installed\n","# if it is installed make sure it is the latest\n","!test -e ds-assets && cd ds-assets && git pull && cd ..\n","# if it is not installed clone it \n","!test ! -e ds-assets && git clone https://github.com/lutzhamel/ds-assets.git\n","# point to the folder with the assets\n","home = \"ds-assets/assets/\" \n","import sys\n","sys.path.append(home)      # add home folder to module search path"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"markdown","metadata":{"id":"az04_f5PY5nQ"},"source":["# Classification Confidence Intervals"]},{"cell_type":"markdown","metadata":{"id":"Br5Q9ND9Y5nR"},"source":["**Observation:** It does not matter how careful we are with our model evaluation techniques, there remains a fundamental uncertainty about the ability of our training data to effectively represent our (possibly infinite) data universe. This uncertainty can be observed during cross-validation: just partitioning the training data in different ways gives rise to drastic differences in model accuracy.  Here is the Iris example again from  the previous notebook."]},{"cell_type":"code","metadata":{"id":"GmnqrVMnY5nS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638295430861,"user_tz":300,"elapsed":181,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"997021bf-0a91-4040-d885-c1292aa2002e"},"source":["# cross-validation Iris\n","import pandas as pd\n","import numpy as np\n","np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n","from sklearn import tree\n","# grab cross validation code\n","from sklearn.model_selection import cross_val_score\n","\n","# get data\n","df = pd.read_csv(home+\"iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# set up the model\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2)\n","\n","# do the 5-fold cross validation\n","scores = cross_val_score(model, X, y, cv=5)\n","print(\"Fold Accuracies: {}\".format(scores))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold Accuracies: [0.93 0.97 0.90 0.87 1.00]\n"]}]},{"cell_type":"markdown","metadata":{"id":"o8-tHIRMY5nT"},"source":["This uncertainty reflects into our model evaluation. If our training data is a poor representation of the data universe then the models we construct using it will generalize poorly to the rest of the data universe. If our training data is a good representation of the data universe then we can expect that our model will generalize well.\n","\n","Here we will deal with this uncertainty using *confidence intervals.*\n","First, let us define confidence intervals formally. Given a model accuracy, *acc*, then the confidence interval is defined as the probability *p* that our model accuracy *acc* lies between some lower bound *lb* and some upper bound *ub*,\n","\n","> $Pr(lb ≤ acc ≤ ub) = p.$"]},{"cell_type":"markdown","metadata":{"id":"sadhMGxlY5nU"},"source":["Paraphrasing this equation with *p = 95%*:\n","\n","> We are 95% percent sure that our model accuracy is not worse than *lb* and not better than *ub*.\n"]},{"cell_type":"markdown","metadata":{"id":"0AU9ejtiY5nU"},"source":["Ultimitely we are interested in the lower and upper bounds of the 95% confidence interval.  We can use the following formula to compute the bounds:\n","\n","> $ub = acc + 1.96 \\sqrt \\frac{acc (1 - acc)}{n}$\n","\n","> $lb = acc - 1.96 \\sqrt \\frac{acc (1 - acc)}{n}$\n","\n","Here, *n* is the number of observations in the testing dataset used to estimate *acc*. The constant 1.96 is called the *z-score* and expresses the fact that we are computing the 95% confidence interval."]},{"cell_type":"markdown","metadata":{"id":"dNpyW7qlY5nU"},"source":["\n","Let's do a simple example using the function `classification_confint`."]},{"cell_type":"code","metadata":{"id":"a74qYZXmY5nV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638295430862,"user_tz":300,"elapsed":14,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"4aa5366e-c098-418e-d213-adc01160ee14"},"source":["from confint import classification_confint\n","\n","observations = 100\n","acc = .88\n","lb,ub = classification_confint(acc,observations)\n","print('Accuracy: {} ({:3.2f},{:3.2f})'.format(acc,lb, ub))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.88 (0.82,0.94)\n"]}]},{"cell_type":"markdown","metadata":{"id":"BZrS6gvHY5nV"},"source":["Now, let's do an actual example using the Wisconsin breast cancer dataset.  We want to print out the testing accuracy together with it's 95% confidence interval."]},{"cell_type":"code","metadata":{"id":"KdHvboh8Y5nV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638295430862,"user_tz":300,"elapsed":11,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"f45e5e06-d2a4-4828-ca43-14b7b6dc4171"},"source":["import pandas as pd\n","from treeviz import tree_print\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from confint import classification_confint\n","\n","# read the data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","\n","# set up the feature matrix and target vector\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=2)\n","\n","# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=2)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","acc = accuracy_score(y_test, y_test_model)\n","observations = X_test.shape[0]\n","lb,ub = classification_confint(acc, observations)\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.93 (0.88,0.98)\n"]}]},{"cell_type":"markdown","metadata":{"id":"pxsPRGorY5nW"},"source":["# Regression Confidence Intervals"]},{"cell_type":"markdown","metadata":{"id":"2tL0IOclY5nW"},"source":["When performing regression we use the $R^2$ score to examine the quality of our models.  Given that we only use a small training dataset for fitting the model compared to the rest of the data universe it is only natural to ask what the 95% confidence interval for this score might be.  We have a formula for that -- it is not as straight forward as the confidence interval for classification,\n","\n","> $lb = R^2 - 2\\sqrt{\\frac{4R^{2}(1-R^{2})^{2}(n-k-1)^{2}}{(n^2 - 1)(n+3)}}$\n","\n","> $ub = R^2 + 2\\sqrt{\\frac{4R^{2}(1-R^{2})^{2}(n-k-1)^{2}}{(n^2 - 1)(n+3)}}$\n","\n","Here, *n* is the number of observations in the validation/testing dataset and *k* is the number of independent variables."]},{"cell_type":"code","metadata":{"id":"2-YDIVcCY5nX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638295430862,"user_tz":300,"elapsed":9,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"60a0d876-9878-4861-af4b-34413eef36fd"},"source":["from confint import regression_confint\n","\n","rs_score = .75\n","observations = 100\n","variables = 4 # independent variables\n","\n","lb,ub = regression_confint(rs_score, observations, variables)\n","print(\"R^2 Score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs_score,lb,ub))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["R^2 Score: 0.75 (0.67, 0.83)\n"]}]},{"cell_type":"markdown","metadata":{"id":"RzIOt6KJY5nX"},"source":["Let's look at an actual regression problem and compute the $R^2$ score and it's 95% confidence interval. We will use the cars problem from before."]},{"cell_type":"code","metadata":{"id":"XEw-Ok0XY5nX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638295430863,"user_tz":300,"elapsed":9,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"579a00b8-6382-4a3f-e6a9-108406a6afae"},"source":["import numpy as np\n","import pandas\n","from sklearn.tree import DecisionTreeRegressor\n","from confint import regression_confint\n","\n","# get our dataset\n","cars_df = pandas.read_csv(home+\"cars.csv\")\n","\n","# build model object\n","model = DecisionTreeRegressor(max_depth=None)\n","\n","# fit model\n","# We have to reshape the values array to make 'fit' happy because\n","# the array only has a single feature\n","model.fit(cars_df['speed'].values.reshape(-1,1),cars_df['dist'])\n","\n","# R^2 score\n","rs_score = model.score(cars_df['speed'].values.reshape(-1,1),cars_df['dist'])\n","observations = cars_df.shape[0]\n","variables = 1\n","lb,ub = regression_confint(rs_score, observations, variables)\n","\n","# print out R^2 score with its 95% confidence interval\n","print(\"R^2 Score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs_score,lb,ub))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["R^2 Score: 0.79 (0.69, 0.89)\n"]}]},{"cell_type":"markdown","metadata":{"id":"IcIGuoRqY5nY"},"source":["# Statistical Significance\n","\n","Besides giving us an idea of the uncertainty of our model the 95% confidence intervals also have something to say about the significance of scores of different models.  That is, if the confidence intervals overlap then the difference in model performance of two different models on the same dataset is not statistically significant.\n","\n","Consider the following,"]},{"cell_type":"code","metadata":{"id":"uVH7oXjUY5nY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638295431011,"user_tz":300,"elapsed":156,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"232baf68-a826-4295-aadc-64058f5ed652"},"source":["from confint import classification_confint\n","\n","observations = 100\n","\n","# first classifier\n","acc1 = .88\n","lb1,ub1 = classification_confint(acc1,observations)\n","print('Accuracy: {} ({:3.2f},{:3.2f})'.format(acc1,lb1, ub1))\n","\n","# second classifier\n","acc2 = .92\n","lb2,ub2 = classification_confint(acc2,observations)\n","print('Accuracy: {} ({:3.2f},{:3.2f})'.format(acc2,lb2, ub2))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.88 (0.82,0.94)\n","Accuracy: 0.92 (0.87,0.97)\n"]}]},{"cell_type":"markdown","metadata":{"id":"HaUTqEY9Y5nZ"},"source":["Even though the second classifier has a better raw accuracy when we look at the confidence intervals of the two classifiers we see that they overlap.  Here we see that the first classifier could potentially have an accuracy of .94 (even better than the raw accuracy of the second classifier).  Furthermore, the confidence interval of the second classifier tells us that that classifier could potentially have an accuracy of .87 which is worse than the raw accuracy of the first classifier.  For this reason we say that the difference in accuracy of two classifiers is not statistically significant if their confidence intervals overlap."]},{"cell_type":"code","metadata":{"id":"PdMUM-x-Y5na","executionInfo":{"status":"ok","timestamp":1638295431012,"user_tz":300,"elapsed":2,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}}},"source":[""],"execution_count":12,"outputs":[]}]}