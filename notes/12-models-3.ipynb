{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preamble to be able to run notebooks in Jupyter and Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    notes_home = \"/content/drive/Shared drives/CSC310/notes/\"\n",
    "    user_home = \"/content/drive/My Drive/\"\n",
    "    \n",
    "    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n",
    "    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** It does not matter how careful we are with our model evaluation techniques, there remains a fundamental uncertainty about the ability of our training data to effectively represent our (possibly infinite) data universe. This uncertainty can be observed during cross-validation: just partitioning the training data in different ways gives rise to drastic differences in model accuracy.  Here is the Iris example again from  the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracies: [0.93 0.97 0.90 0.87 1.00]\n"
     ]
    }
   ],
   "source": [
    "# cross-validation Iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n",
    "from sklearn import tree\n",
    "# grab cross validation code\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# get data\n",
    "df = pd.read_csv(notes_home+\"assets/iris.csv\")\n",
    "X  = df.drop(['id','Species'],axis=1)\n",
    "y = df['Species']\n",
    "\n",
    "# set up the model\n",
    "model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2)\n",
    "\n",
    "# do the 5-fold cross validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Fold Accuracies: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uncertainty reflects into our model evaluation. If our training data is a poor representation of the data universe then the models we construct using it will generalize poorly to the rest of the data universe. If our training data is a good representation of the data universe then we can expect that our model will generalize well.\n",
    "\n",
    "Here we will deal with this uncertainty using *confidence intervals.*\n",
    "First, let us define confidence intervals formally. Given a model accuracy, *acc*, then the confidence interval is defined as the probability *p* that our model accuracy *acc* lies between some lower bound *lb* and some upper bound *ub*,\n",
    "\n",
    "> $Pr(lb ≤ acc ≤ ub) = p.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paraphrasing this equation with *p = 95%*:\n",
    "\n",
    "> We are 95% percent sure that our model accuracy is not worse than *lb* and not better than *ub*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimitely we are interested in the lower and upper bounds of the 95% confidence interval.  We can use the following formula to compute the bounds:\n",
    "\n",
    "> $ub = acc + 1.96 \\sqrt \\frac{acc (1 - acc)}{n}$\n",
    "\n",
    "> $lb = acc - 1.96 \\sqrt \\frac{acc (1 - acc)}{n}$\n",
    "\n",
    "Here, *n* is the number of observations in the testing dataset used to estimate *acc*. The constant 1.96 is called the *z-score* and expresses the fact that we are computing the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Confidence Intervals in Python\n",
    "\n",
    "Let's do a simple example using the function `classification_confint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88 (0.82,0.94)\n"
     ]
    }
   ],
   "source": [
    "from assets.confint import classification_confint\n",
    "\n",
    "observations = 100\n",
    "acc = .88\n",
    "lb,ub = classification_confint(acc,observations)\n",
    "print('Accuracy: {} ({:3.2f},{:3.2f})'.format(acc,lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do an actual example using the Wisconsin breast cancer dataset.  We want to print out the testing accuracy together with it's 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91 (0.86,0.94)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from assets.treeviz import tree_print\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from assets.confint import classification_confint\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(notes_home+\"assets/wdbc.csv\")\n",
    "\n",
    "# set up the feature matrix and target vector\n",
    "X  = df.drop(['ID','Diagnosis'],axis=1)\n",
    "y = df['Diagnosis']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=2)\n",
    "\n",
    "# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n",
    "model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=1)\n",
    "\n",
    "# fit the model on the training set of data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test results: evaluate the model on the testing set of data\n",
    "y_test_model = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_test_model)\n",
    "observations = X_test.shape[0]\n",
    "lb,up = classification_confint(acc, observations)\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing regression we use the $R^2$ score to examine the quality of our models.  Given that we only use a small training dataset for fitting the model compared to the rest of the data universe it is only natural to ask what the 95% confidence interval for this score might be.  We have a formula for that -- it is not as straight forward as the confidence interval for classification,\n",
    "\n",
    "> $lb = R^2 - 2\\sqrt{\\frac{4R^{2}(1-R^{2})^{2}(n-k-1)^{2}}{(n^2 - 1)(n+3)}}$\n",
    "\n",
    "> $ub = R^2 + 2\\sqrt{\\frac{4R^{2}(1-R^{2})^{2}(n-k-1)^{2}}{(n^2 - 1)(n+3)}}$\n",
    "\n",
    "Here, *n* is the number of observations in the validation/testing dataset and *k* is the number of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.75 (0.67, 0.83)\n"
     ]
    }
   ],
   "source": [
    "from assets.confint import regression_confint\n",
    "\n",
    "rs_score = .75\n",
    "observations = 100\n",
    "variables = 4 # independent variables\n",
    "\n",
    "lb,ub = regression_confint(rs_score, observations, variables)\n",
    "print(\"R^2 Score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs_score,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an actual regression problem and compute the $R^2$ score and it's 95% confidence interval. We will use the cars problem from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.79 (0.69, 0.89)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from assets.confint import regression_confint\n",
    "\n",
    "# get our dataset\n",
    "cars_df = pandas.read_csv(notes_home+\"assets/cars.csv\")\n",
    "\n",
    "# build model object\n",
    "model = DecisionTreeRegressor(max_depth=None)\n",
    "\n",
    "# fit model\n",
    "# We have to reshape the values array to make 'fit' happy because\n",
    "# the array only has a single feature\n",
    "model.fit(cars_df['speed'].values.reshape(-1,1),cars_df['dist'])\n",
    "\n",
    "# R^2 score\n",
    "rs_score = model.score(cars_df['speed'].values.reshape(-1,1),cars_df['dist'])\n",
    "observations = cars_df.shape[0]\n",
    "variables = 1\n",
    "lb,ub = regression_confint(rs_score, observations, variables)\n",
    "\n",
    "# print out R^2 score with its 95% confidence interval\n",
    "print(\"R^2 Score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs_score,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance\n",
    "\n",
    "Besides giving us an idea of the uncertainty of our model the 95% confidence intervals also have something to say about the significance of scores of different models.  That is, if the confidence intervals overlap then the difference in model performance of two different models on the same dataset is not statistically significant.\n",
    "\n",
    "Consider the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88 (0.82,0.94)\n",
      "Accuracy: 0.92 (0.87,0.97)\n"
     ]
    }
   ],
   "source": [
    "from assets.confint import classification_confint\n",
    "\n",
    "observations = 100\n",
    "\n",
    "# first classifier\n",
    "acc1 = .88\n",
    "lb1,ub1 = classification_confint(acc1,observations)\n",
    "print('Accuracy: {} ({:3.2f},{:3.2f})'.format(acc1,lb1, ub1))\n",
    "\n",
    "# second classifier\n",
    "acc2 = .92\n",
    "lb2,ub2 = classification_confint(acc2,observations)\n",
    "print('Accuracy: {} ({:3.2f},{:3.2f})'.format(acc2,lb2, ub2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the second classifier has a better raw accuracy when we look at the confidence intervals of the two classifiers we see that they overlap.  Here we see that the first classifier could potentially have an accuracy of .94 (even better than the raw accuracy of the second classifier).  Furthermore, the confidence interval of the second classifier tells us that that classifier could potentially have an accuracy of .87 which is worse than the raw accuracy of the first classifier.  For this reason we say that the difference in accuracy of two classifiers is not statistically significant if their confidence intervals overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Exercise\n",
    "\n",
    "Using the `abalone.csv` dataset from the `assets` folder do the following:\n",
    "\n",
    "* Find the best decision tree for the dataset using grid search with 5-fold cross-validation.  Recall that the decision tree has two free parameters: criterion and tree depth\n",
    "\n",
    "* Apply the best decision tree you have found to the whole training dataset and compute its accuracy on that dataset.\n",
    "\n",
    "* Compute the 95% confidence interval of the accuracy compute in the previous step.\n",
    "\n",
    "* Build a decision tree wit max_depth=1 on the whole training data set. Compute its accuracy and 95% confidence interval on that training data.\n",
    "\n",
    "* Build a decision tree wit max_depth=None on the whole training data set. Compute its accuracy and 95% confidence interval on that training data.\n",
    "\n",
    "* Do the confidence intervals of all three decision trees overlap? If so, what are the conclusions you can draw?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teams\n",
    "\n",
    "```\n",
    "team 1:  Kenney A, Stephanie, Phidias\n",
    "team 2:  Korakot, Julio, Camren Joseph\n",
    "team 3:  Michael Russell, Sofia R, Jared P\n",
    "team 4:  Emmely, C.J., Jaeke R\n",
    "team 5:  Luca G, Evan Jonathan, Shannon Patrice\n",
    "team 6:  Yeury, Timothy Terence, Cody Rithysan\n",
    "team 7:  Joshua Patrick, Patrick M, John Francis\n",
    "team 8:  Samantha N, Cole, Andrew Michael\n",
    "team 9:  Jake Adam, Timothy, Hennjer\n",
    "team 10: Zachary T, Giulia, Tony Levada\n",
    "team 11: William Jordan, Dan Steven, Joshua D\n",
    "team 12: Joey, Ryan Richard\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
