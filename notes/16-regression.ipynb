{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM8hPCpbfRFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799b8ac0-6a77-4d02-c117-b74c7569a808"
      },
      "source": [
        "###### Set Up #####\n",
        "# verify our folder with the data and module assets is installed\n",
        "# if it is installed make sure it is the latest\n",
        "!test -e ds-assets && cd ds-assets && git pull && cd ..\n",
        "# if it is not installed clone it\n",
        "!test ! -e ds-assets && git clone https://github.com/IndraniMandal/ds-assets.git\n",
        "# point to the folder with the assets\n",
        "home = \"ds-assets/assets/\"\n",
        "import sys\n",
        "sys.path.append(home)      # add home folder to module search path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ds-assets'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 205 (delta 54), reused 50 (delta 50), pack-reused 147 (from 1)\u001b[K\n",
            "Receiving objects: 100% (205/205), 12.58 MiB | 8.52 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64J_wUX4fRFN"
      },
      "source": [
        "# Regression\n",
        "\n",
        "In regression we predict **values** rather than discrete labels.\n",
        "In the simplest case we want to fit a line through a set of points - simple linear regression.\n",
        "A straight-line is a model of the form,\n",
        "\n",
        "$$y=mx+b$$\n",
        "\n",
        "where m is the *slope* and b is the *intercept*.  \n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/linreg-scatter.png\"  height=\"400\" width=\"450\">\n",
        "</center>\n",
        "\n",
        "[source](https://towardsdatascience.com/mathematics-hidden-behind-linear-regression-431fe4d11969)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this with a synthetic dataset that we generate with a linear model in mind plus some noise: given an $x$ value we compute the corresponding $y$ as\n",
        "\n",
        "$$y=mx+b+\\Delta_y$$\n",
        "\n",
        "where $m$ is the slope, $b$ the intercept, and $\\Delta_y$ the noise in the $y$ coordinate.\n"
      ],
      "metadata": {
        "id": "RqCAamFjBiV4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huuj8V4gfRFO"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# data parameters\n",
        "# implicit model parameters!\n",
        "m = 2        # slope\n",
        "b = -1       # intercept\n",
        "# data set specs\n",
        "n = 50       # number of points to generate\n",
        "y_range = 30 # governs the level of noise on the y coordinate\n",
        "delta_y = \\\n",
        "  pd.Series([random.uniform(-y_range,y_range) for _ in range(n)])\n",
        "\n",
        "# generate a dataframe for our data points\n",
        "# generate values on the x-axis\n",
        "x = pd.DataFrame({'x':[i for i in range(n)]})\n",
        "# generate values on the y-axis using x-values\n",
        "y = pd.DataFrame({'y': m * x['x'] + b + delta_y})\n",
        "# each row represents the point (x,y)\n",
        "points_df = pd.concat([x,y], axis=1)\n",
        "\n",
        "# linear regression model\n",
        "# fit the model to our noisy data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "# x is the independ variable, y the dependent variable\n",
        "model.fit(points_df[['x']], points_df['y'])\n",
        "\n",
        "# plot the model together with the data\n",
        "# create a dataframe that represents the model using two points\n",
        "xfit = pd.DataFrame({'x':[0,n]})\n",
        "yfit = pd.DataFrame({'y':model.predict(xfit)})\n",
        "# each row represents a point on the model\n",
        "model_df = pd.concat([xfit, yfit], axis=1)\n",
        "\n",
        "sns.scatterplot(data=points_df, x='x', y='y')  # data\n",
        "sns.lineplot(data=model_df, x='x', y='y', color='red')  # model\n",
        "\n",
        "# print model parameters\n",
        "print(\"Slope: {:3.2f}\".format(model.coef_[0]))\n",
        "print(\"Intercept: {:3.2f}\".format(model.intercept_))\n",
        "\n",
        "# compute the R^2 score\n",
        "rs = model.score(x,y)\n",
        "print(\"R^2 score: {:3.2f}\".format(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7qQX-xGfRFP"
      },
      "source": [
        "# Another Look at the $R^2$ Score\n",
        "\n",
        "The $R^2$ score is a score that compares the errors squared of the regression model to the errors squared of a default model.\n",
        "The default model is just the average value $\\bar{y}$ of all y-values.  That is, for any x-value the model always returns the same answer: $\\bar{y}$.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/rs.png\" height=\"400\" width=\"460\">\n",
        "</center>\n",
        "\n",
        "(Source: [Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination))\n",
        "\n",
        "Let's explore that figure a little bit more assuming that we have a training dataset that consists of the set of points,\n",
        "\n",
        "$$\\{(x_1,y_1), (x_2,y_2),\\dots,(x_n,y_n)\\}$$\n",
        "\n",
        "where $x_i$ are the values of the independent variable and $y_i$ are the values of the dependent variable. Then the we have the following definitions,\n",
        "\n",
        "1. The **default model** is defined as $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$, the average of the expected outcomes $y_i$ of the training data.\n",
        "1. The regression model is $f$ and the **residuals** are defined as $y_i - f(x_i)$ for $i=1\\ldots n$, that is, a residual is the difference between the expected outcome $y_i$ at point $x_i$ minus the outcome computed by the model $f(x_i)$.\n",
        "1. $SS_{\\rm res}$ is the squared sum of the residuals of the model $f$, more precisely, $SS_{\\rm res} = \\sum_{i=1}^n (y_i - f(x_i))^2$.\n",
        "1. $SS_{\\rm tot}$ is the squared sum of the residuals of the default model $\\bar{y}$, more precisely, $SS_{\\rm tot} = \\sum_{i=1}^n (y_i - \\bar{y})^2$.\n",
        "\n",
        "With these definitions we can now compute the $R^2$ score as\n",
        "\n",
        "$$R^2\n",
        "= 1 - \\frac{SS_{\\rm res}}{SS_{\\rm tot}}\n",
        "= 1 - \\frac{\\sum_{i=1}^n (y_i - f(x_i))^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "Notice that if the performance of our regression model $f$ approaches the performance of our default model $\\bar{y}$ then the\n",
        "score will be,\n",
        "\n",
        "$$R^2 \\approx 1 - 1 = 0$$\n",
        "\n",
        "If our model $f$ is perfect, that is, all residuals $y_i - f(x_i)$ are equal to zero then we obtain the score,\n",
        "\n",
        "$$R^2 = 1 - 0 = 1$$\n",
        "\n",
        "That means, the closer an $R^2$ score is to one the better the model.  It turns out that if our model $f$ is worse than the default model $\\bar{y}$ then it is possible to obtain negative $R^2$ values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK8kg8e1fRFP"
      },
      "source": [
        "# Regression Models\n",
        "\n",
        "Turns out that all the classification models we have covered so far also support regression models:\n",
        "\n",
        "* Regression Trees (we have seen these before when we discussed non-linear regression)\n",
        "* KNN Regression Models\n",
        "* MLP Regression Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hWlvKb9fRFP"
      },
      "source": [
        "## Regression Trees\n",
        "\n",
        "Below is the code to construct a regression tree on the cars dataset.  Here we restrict the complexity of the tree to a depth of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ONrCMDfRFQ"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from confint import regression_confint\n",
        "\n",
        "# get our dataset\n",
        "cars_df = pd.read_csv(home+\"cars.csv\")\n",
        "\n",
        "# pick model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model = DecisionTreeRegressor(max_depth=2)\n",
        "model.fit(cars_df[['speed']],cars_df['dist'])\n",
        "\n",
        "# plot the data\n",
        "sns.scatterplot(data=cars_df, x='speed', y='dist')\n",
        "\n",
        "# plot the model\n",
        "# 0 to 25 in .1 increments\n",
        "x_ticks = pd.DataFrame(list(range(0,251)), columns=['speed']) / 10.0\n",
        "y_ticks = pd.DataFrame(model.predict(x_ticks), columns=['dist'])\n",
        "model_df = pd.concat([x_ticks, y_ticks], axis=1)\n",
        "sns.lineplot(model_df, x='speed', y='dist', color='red')\n",
        "\n",
        "# compute the R^2 score\n",
        "rs = model.score(cars_df[['speed']],cars_df['dist'])\n",
        "obs = cars_df.shape[0]\n",
        "vars = 1\n",
        "lb, ub = regression_confint(rs, obs, vars)\n",
        "print(\"R^2 score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs,lb,ub))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En378I7HfRFQ"
      },
      "source": [
        "## KNN Regression\n",
        "\n",
        "KNN regression is interesting in that we are not asking the neighbors for a majority label but instead we compute the average value associated with the neighbors and that becomes the value of the current point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XkbG_jTfRFR"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from confint import regression_confint\n",
        "\n",
        "# get our dataset\n",
        "cars_df = pd.read_csv(home+\"cars.csv\")\n",
        "\n",
        "# pick model\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "model = KNeighborsRegressor(n_neighbors=3)\n",
        "model.fit(cars_df[['speed']],cars_df['dist'])\n",
        "\n",
        "# plot the data\n",
        "sns.scatterplot(data=cars_df, x='speed', y='dist')\n",
        "\n",
        "# plot the model\n",
        "# 0 to 25 in .1 increments\n",
        "x_ticks = pd.DataFrame(list(range(0,251)), columns=['speed']) / 10.0\n",
        "y_ticks = pd.DataFrame(model.predict(x_ticks), columns=['dist'])\n",
        "model_df = pd.concat([x_ticks, y_ticks], axis=1)\n",
        "sns.lineplot(model_df, x='speed', y='dist', color='red')\n",
        "\n",
        "# compute the R^2 score\n",
        "rs = model.score(cars_df[['speed']],cars_df['dist'])\n",
        "obs = cars_df.shape[0]\n",
        "vars = 1\n",
        "lb, ub = regression_confint(rs, obs, vars)\n",
        "print(\"R^2 score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs,lb,ub))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_HvPlOpfRFR"
      },
      "source": [
        "## MLP Regression\n",
        "\n",
        "In MLP regression we ask the network to model a continuous value.  This turns out to be just a change in interpretation of the output value.  Rather than applying a thresholding function to the output like the `sign` function we just use the raw output value as the model output value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7mgrEVcfRFS"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from confint import regression_confint\n",
        "\n",
        "# get our dataset\n",
        "cars_df = pd.read_csv(home+\"cars.csv\")\n",
        "\n",
        "# pick model\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "model = MLPRegressor(hidden_layer_sizes=(2,),\n",
        "                     activation='tanh',\n",
        "                     max_iter=100000)\n",
        "model.fit(cars_df[['speed']],cars_df['dist'])\n",
        "\n",
        "# plot the data\n",
        "sns.scatterplot(data=cars_df, x='speed', y='dist')\n",
        "\n",
        "# plot the model\n",
        "# 0 to 25 in .1 increments\n",
        "x_ticks = pd.DataFrame(list(range(0,251)), columns=['speed']) / 10.0\n",
        "y_ticks = pd.DataFrame(model.predict(x_ticks), columns=['dist'])\n",
        "model_df = pd.concat([x_ticks, y_ticks], axis=1)\n",
        "sns.lineplot(model_df, x='speed', y='dist', color='red')\n",
        "\n",
        "# compute the R^2 score\n",
        "rs = model.score(cars_df[['speed']],cars_df['dist'])\n",
        "obs = cars_df.shape[0]\n",
        "vars = 1\n",
        "lb, ub = regression_confint(rs, obs, vars)\n",
        "print(\"R^2 score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs,lb,ub))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90n3xU6fRFS"
      },
      "source": [
        "# Regression and Grid Search: Tree Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWKVgkjffRFS"
      },
      "source": [
        "Just like in classification, regression models are built using a small sample of a possibly infinite data universe and we have to estimate the model parameters for the best model using this small sample...that means we have to perform a grid search over parameter ranges and we want to use cross-validation to minimize the bias of any one particular training-test split of the data.\n",
        "\n",
        "Let's try this with tree regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqx_tzmpfRFT"
      },
      "source": [
        "# Import the necessary modules and libraries\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from confint import regression_confint\n",
        "import pandas as pd\n",
        "\n",
        "# get our dataset\n",
        "cars_df = pd.read_csv(home+\"cars.csv\")\n",
        "\n",
        "# setting up grid search\n",
        "model = DecisionTreeRegressor()\n",
        "param_grid = {'max_depth': list(range(1,11))}\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "# performing grid search\n",
        "grid.fit(cars_df[['speed']], cars_df['dist'])\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# plot the data\n",
        "sns.scatterplot(data=cars_df, x='speed', y='dist')\n",
        "\n",
        "# plot the model\n",
        "# 0 to 25 in .1 increments\n",
        "x_ticks = pd.DataFrame(list(range(0,251)), columns=['speed']) / 10.0\n",
        "y_ticks = pd.DataFrame(best_model.predict(x_ticks), columns=['dist'])\n",
        "model_df = pd.concat([x_ticks, y_ticks], axis=1)\n",
        "sns.lineplot(model_df, x='speed', y='dist', color='red')\n",
        "\n",
        "# compute the R^2 score\n",
        "rs = best_model.score(cars_df[['speed']],cars_df['dist'])\n",
        "obs = cars_df.shape[0]\n",
        "vars = 1\n",
        "lb, ub = regression_confint(rs, obs, vars)\n",
        "print(\"R^2 score: {:3.2f} ({:3.2f}, {:3.2f})\".format(rs,lb,ub))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to do a grid search for k-NN and ANNs."
      ],
      "metadata": {
        "id": "Z0c22xNUria7"
      }
    }
  ]
}