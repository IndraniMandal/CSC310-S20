{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"11-models-2.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"n8WKWoVMYqMy"},"source":["# preamble to be able to run notebooks in Jupyter and Colab\n","try:\n","    from google.colab import drive\n","    import sys\n","    \n","    drive.mount('/content/drive')\n","    notes_home = \"/content/drive/Shared drives/CSC310/ds/notes/\"\n","    user_home = \"/content/drive/My Drive/\"\n","    \n","    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n","\n","except ModuleNotFoundError:\n","    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n","    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gjsCjE04YqM6"},"source":["# Evaluating Models"]},{"cell_type":"markdown","metadata":{"id":"GpGoX5wvYqM6"},"source":["Learning Curves\n","\n","* It can be shown that *any model* can learn its training data perfectly - “memorize it”.  That is what the blue curve shows below. Any model can achieve a perfect score on the training data as long as it is allowed to be complex enough. Maximum complexity: the model has memorized the entire dataset.\n","\n","* But memorizing is not the same as learning inherent patterns and use those patterns to make predictions!  Memorization is extremely bad at predicting future outcomes.  See what happens to the red line below as the model starts to memorize the dataset -- the score actually falls!\n","\n","> Memorization does not generalize well!\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"3cUe2BZKYqM7"},"source":["If we train a model using *training data* and then apply the model to a *validation/test data set* then we obtain these following typical curves:\n","\n","<!-- ![model curves](assets/model-performance-curves.png) -->\n","\n","<img src=\"https://github.com/lutzhamel/ds/raw/master/notes/assets/train-test-curves.png\"  height=\"250\" width=\"400\">\n","\n","Note: Validation data is data that the model has not seen yet."]},{"cell_type":"markdown","metadata":{"id":"FpfBnlIgYqM8"},"source":["Simply put:\n","\n","1. Undertrained models make a lot of errors on validation data because they have not learned any of the patterns yet.\n","\n","2. Overtrained models (models that have memorized their training data) make a lot of errors on validation because memorization is extremely bad at predicting the future outcomes.\n","\n","3. The best models make a trade-off between errors and recognizing important patterns. Notice that for the best models the training score is not 100%!\n","\n","> In order to find the best model we have to search its *parameter space* in order to find just the right complexity level."]},{"cell_type":"markdown","metadata":{"id":"0OEfoZhwYqM8"},"source":["**Note**: The code in this notebook takes advantage of the `random_state` parameter in a lot of scikit-learn functions.  This is only done to keep the results of this notebook deterministic.  This variable is strictly not necessary in general applications."]},{"cell_type":"markdown","metadata":{"id":"JYglFKc2YqM9"},"source":["# Train and Test (Validate)"]},{"cell_type":"markdown","metadata":{"id":"CZC_GX96YqM9"},"source":["In order to simulate the fact that a model is not able to see all possible data points during training we split our training data into two parts:\n","\n","* Training data\n","* Testing (validation) data\n","\n","We will train our model on the training data as before but we will now test the model performance on the testing data which the model has not seen yet.\n","\n","> That is, we force the model to make some generalizations.\n"]},{"cell_type":"markdown","metadata":{"id":"sDAMtz1qYqM-"},"source":["# Decision Trees: Train and Test"]},{"cell_type":"code","metadata":{"id":"eTWm0JbmYqM-"},"source":["import pandas as pd\n","from assets.treeviz import tree_print\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","# sklearn provides manipulation of training sets\n","# here we do train/test split\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yx-cHZu7YqM-"},"source":["# The Iris Data Set"]},{"cell_type":"code","metadata":{"id":"Gkq8K9pYYqM-"},"source":["# set up our sklearn data shape for the iris data\n","df = pd.read_csv(notes_home+\"assets/iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# split the data - 70% training 30% testing\n","datasets = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=2)\n","X_train, X_test, y_train, y_test = datasets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZgiKoiORYqM_"},"source":["Let's see if we can recreate the results given by the learning curves above."]},{"cell_type":"markdown","metadata":{"id":"XhtZu3EbYqM_"},"source":["### Low Complexity Tree"]},{"cell_type":"code","metadata":{"id":"bX5Blp7tYqM_","outputId":"f7262c91-4d80-4fa4-8555-9346617b8198"},"source":["# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["if Petal.Width =< 0.800000011920929: \n","  |then setosa\n","  |else virginica\n","<->\n","Tree Depth:  1\n","Train Accuracy: 0.67\n","Test Accuracy: 0.67\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_n_3glDSYqNB"},"source":["### Medium Complexity Tree"]},{"cell_type":"code","metadata":{"id":"TkwcT_FLYqNB","outputId":"b0f0ad81-880d-48ad-8856-d04bf8404165"},"source":["# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["if Petal.Length =< 2.350000023841858: \n","  |then setosa\n","  |else if Petal.Width =< 1.6500000357627869: \n","  |  |then if Petal.Length =< 4.950000047683716: \n","  |  |  |then versicolor\n","  |  |  |else virginica\n","  |  |else if Petal.Length =< 4.8500001430511475: \n","  |  |  |then virginica\n","  |  |  |else virginica\n","<------->\n","Tree Depth:  3\n","Train Accuracy: 0.98\n","Test Accuracy: 0.98\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y5k7y6tNYqNB"},"source":["### High Complexity Tree"]},{"cell_type":"code","metadata":{"id":"PeUOtP8qYqNB","outputId":"0387d459-1831-4f1c-85a2-8008eaa3f517"},"source":["# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["if Petal.Width =< 0.800000011920929: \n","  |then setosa\n","  |else if Petal.Width =< 1.6500000357627869: \n","  |  |then if Petal.Length =< 4.950000047683716: \n","  |  |  |then versicolor\n","  |  |  |else if Petal.Width =< 1.550000011920929: \n","  |  |  |  |then virginica\n","  |  |  |  |else versicolor\n","  |  |else if Petal.Length =< 4.8500001430511475: \n","  |  |  |then if Sepal.Width =< 3.100000023841858: \n","  |  |  |  |then virginica\n","  |  |  |  |else versicolor\n","  |  |  |else virginica\n","<---------->\n","Tree Depth:  4\n","Train Accuracy: 1.00\n","Test Accuracy: 0.96\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2Is2IM9NYqNC"},"source":["# Wisconsin Breast Cancer Dataset\n","\n","The data set is available at <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">UCI</a>.\n","The data set describes benign and malignent tumors based on image measurements."]},{"cell_type":"code","metadata":{"id":"Y9Z8F6qrYqNC","outputId":"a33f9545-0aa8-4a42-eacb-f7b2716b1ca9"},"source":["# set up our sklearn data shape for the iris data\n","df = pd.read_csv(notes_home+\"assets/wdbc.csv\")\n","print(df.shape)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(569, 32)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>radius1</th>\n","      <th>texture1</th>\n","      <th>perimeter1</th>\n","      <th>area1</th>\n","      <th>smoothness1</th>\n","      <th>compactness1</th>\n","      <th>concavity1</th>\n","      <th>concave_points1</th>\n","      <th>symmetry1</th>\n","      <th>...</th>\n","      <th>texture3</th>\n","      <th>perimeter3</th>\n","      <th>area3</th>\n","      <th>smoothness3</th>\n","      <th>compactness3</th>\n","      <th>concavity3</th>\n","      <th>concave_points3</th>\n","      <th>symmetry3</th>\n","      <th>fractal_dimension3</th>\n","      <th>Diagnosis</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.3001</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>...</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.1622</td>\n","      <td>0.6656</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.0869</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>...</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.1238</td>\n","      <td>0.1866</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.1974</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>...</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.1444</td>\n","      <td>0.4245</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.2414</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>...</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.2098</td>\n","      <td>0.8663</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.1980</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>...</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.1374</td>\n","      <td>0.2050</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","      <td>M</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 32 columns</p>\n","</div>"],"text/plain":["   ID  radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n","0   1    17.99     10.38      122.80  1001.0      0.11840       0.27760   \n","1   2    20.57     17.77      132.90  1326.0      0.08474       0.07864   \n","2   3    19.69     21.25      130.00  1203.0      0.10960       0.15990   \n","3   4    11.42     20.38       77.58   386.1      0.14250       0.28390   \n","4   5    20.29     14.34      135.10  1297.0      0.10030       0.13280   \n","\n","   concavity1  concave_points1  symmetry1  ...  texture3  perimeter3   area3  \\\n","0      0.3001          0.14710     0.2419  ...     17.33      184.60  2019.0   \n","1      0.0869          0.07017     0.1812  ...     23.41      158.80  1956.0   \n","2      0.1974          0.12790     0.2069  ...     25.53      152.50  1709.0   \n","3      0.2414          0.10520     0.2597  ...     26.50       98.87   567.7   \n","4      0.1980          0.10430     0.1809  ...     16.67      152.20  1575.0   \n","\n","   smoothness3  compactness3  concavity3  concave_points3  symmetry3  \\\n","0       0.1622        0.6656      0.7119           0.2654     0.4601   \n","1       0.1238        0.1866      0.2416           0.1860     0.2750   \n","2       0.1444        0.4245      0.4504           0.2430     0.3613   \n","3       0.2098        0.8663      0.6869           0.2575     0.6638   \n","4       0.1374        0.2050      0.4000           0.1625     0.2364   \n","\n","   fractal_dimension3  Diagnosis  \n","0             0.11890          M  \n","1             0.08902          M  \n","2             0.08758          M  \n","3             0.17300          M  \n","4             0.07678          M  \n","\n","[5 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"BBAt8H9RYqNE","outputId":"00652d6c-c2dd-4ccc-9942-108f79bd3e85"},"source":["# see if our data set is balanced\n","df['Diagnosis'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["B    357\n","M    212\n","Name: Diagnosis, dtype: int64"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"4YNNuJijYqNF"},"source":["X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DO0NPMwrYqNF"},"source":["### Low Complexity Tree"]},{"cell_type":"code","metadata":{"id":"UpY6qRv2YqNF","outputId":"fac95639-98f6-4c6f-f6a4-4650cb5182cf"},"source":["# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=1)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["if perimeter3 =< 104.10000228881836: \n","  |then B\n","  |else M\n","<->\n","Tree Depth:  1\n","Train Accuracy: 0.94\n","Test Accuracy: 0.85\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uNqiiOLBYqNG"},"source":["### Medium Complexity Tree"]},{"cell_type":"code","metadata":{"id":"MV6Sv3yEYqNG","outputId":"ac2325de-2237-42a1-a00a-44c7636446d6"},"source":["# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=2)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["if perimeter3 =< 104.10000228881836: \n","  |then if concave_points3 =< 0.13505000621080399: \n","  |  |then if area2 =< 48.97500038146973: \n","  |  |  |then B\n","  |  |  |else if concavity2 =< 0.01723999995738268: \n","  |  |  |  |then M\n","  |  |  |  |else B\n","  |  |else if texture3 =< 29.454999923706055: \n","  |  |  |then B\n","  |  |  |else M\n","  |else if concave_points3 =< 0.14159999787807465: \n","  |  |then if texture3 =< 19.90999984741211: \n","  |  |  |then B\n","  |  |  |else if area2 =< 35.290000915527344: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |else if radius3 =< 15.87000036239624: \n","  |  |  |then if smoothness3 =< 0.1388000026345253: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","<---------->\n","Tree Depth:  4\n","Train Accuracy: 0.98\n","Test Accuracy: 0.92\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2Ilhu1CMYqNG"},"source":["### High Complexity Tree"]},{"cell_type":"code","metadata":{"id":"kolozCbJYqNG","outputId":"d48feee9-9af8-471f-d70f-3fa8a9bd638f"},"source":["# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=1)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["if perimeter3 =< 104.10000228881836: \n","  |then if concave_points3 =< 0.13505000621080399: \n","  |  |then if area2 =< 48.97500038146973: \n","  |  |  |then B\n","  |  |  |else if smoothness3 =< 0.10676500201225281: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |else if texture3 =< 29.454999923706055: \n","  |  |  |then B\n","  |  |  |else M\n","  |else if concave_points3 =< 0.14159999787807465: \n","  |  |then if texture3 =< 19.90999984741211: \n","  |  |  |then B\n","  |  |  |else if radius2 =< 0.37575000524520874: \n","  |  |  |  |then if perimeter2 =< 2.0149999856948853: \n","  |  |  |  |  |then if concave_points3 =< 0.08526499941945076: \n","  |  |  |  |  |  |then B\n","  |  |  |  |  |  |else M\n","  |  |  |  |  |else B\n","  |  |  |  |else M\n","  |  |else if radius3 =< 15.87000036239624: \n","  |  |  |then if smoothness3 =< 0.1388000026345253: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","<---------------->\n","Tree Depth:  6\n","Train Accuracy: 1.0\n","Test Accuracy: 0.9122807017543859\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X6aOtYkFDsJ0"},"source":["# Model Search\n","\n","From our discussion above it is clear that in order to find the best model we have to perform a **search** over the model space using parameters that dictate the complexity of the model.\n","\n","Demo: try this with the decision tree on the iris data set.\n"]},{"cell_type":"markdown","metadata":{"id":"Tb6ecfT_YqNH"},"source":["# Train and Test\n","\n","We have already seen that just using a training set for model evaluation does not work!\n","\n","Our solution was to split the training data into a training and a test/validation set.\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds/master/notes/assets/train-test-data.png\" height=\"200\" width=\"400\">"]},{"cell_type":"markdown","metadata":{"id":"C027zEXTYqNH"},"source":["# Problem!\n","\n","* Train-testing relies on randomly splitting the training data into two parts.\n","\n","* If this split just happens to be a ‘bad’ split our results might be biased,\n","\n","**Solution:** cross-validation\n"]},{"cell_type":"markdown","metadata":{"id":"J8QuQ5OoYqNH"},"source":["# Cross-Validation\n","\n","* In cross-validation we switch the roles of the two sets\n","* We build and evaluate a model on each trial and then take the average\n","* This will eliminate a lot of the bias\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds/master/notes/assets/2fold-xval.png\" height=\"200\" width=\"400\">\n"]},{"cell_type":"markdown","metadata":{"id":"TT_q2zlrYqNI"},"source":["BUT, what if is the split was really bad: e.g. one of the sets does not contain any examples of one of the classes.\n","\n","**Solution:** create more trials - *n-fold cross-validation*"]},{"cell_type":"markdown","metadata":{"id":"BGoJM3nCYqNI"},"source":["As a solution to a single bad split -- perform the split multiple times -- then train and test -- take the average\n","\n","Example: \n","* 5-fold CV - split the training data into 5 partitions (folds)\n","* Use each fold as a test/validation set and the other folds as training set\n","* Multiple splits - even if one is bad it will be balanced out by the others.\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds/master/notes/assets/5fold-xval.png\" height=\"200\" width=\"400\">\n","\n","**Note**: 5-CV is interesting because each trial essentially has an 80-20 split: 80% of the data for training and 20% for testing.\n","\n","**Note**: We have to train and test models five times in 5-CV."]},{"cell_type":"markdown","metadata":{"id":"oPyEgFNmEo0R"},"source":["# Model Search\n","\n","Once we switch to a cross-validation approach our model search becomes something different.  Instead of search over the model space we search over \n","the **parameter space** for the best set of parameters.\n","\n","<img src=\"https://github.com/lutzhamel/ds/raw/master/notes/assets/cross-validated-curve.png\"  height=\"250\" width=\"400\">\n","\n","This is due to the fact that cross-validation builds **multiple** models and the cross-validated performance is the **mean performance** of the models built on the various folds.\n","\n","Searching over the parameter space is called **grid search**.\n"]},{"cell_type":"code","metadata":{"id":"LJJCbpFdYqNI","outputId":"138a0aa9-7337-4586-9546-2ffcf25123ca"},"source":["# cross-validation Iris\n","import pandas as pd\n","import numpy as np\n","np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n","from sklearn import tree\n","# grab cross validation code\n","from sklearn.model_selection import cross_val_score\n","\n","# get data\n","df = pd.read_csv(notes_home+\"assets/iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# set up the model\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2)\n","\n","# do the 5-fold cross validation\n","scores = cross_val_score(model, X, y, cv=5)\n","print(\"Fold Accuracies: {}\".format(scores))\n","print(\"Accuracy: {:3.2f}\".format(scores.mean()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fold Accuracies: [0.93 0.97 0.90 0.87 1.00]\n","Accuracy: 0.93\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jp9f_GnKYqNJ","outputId":"3433524b-ca5a-4975-b4d9-f8924aceb23e"},"source":["# cross-validation WDBC\n","import pandas as pd\n","import numpy as np\n","np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n","from sklearn import tree\n","# grab cross validation code\n","from sklearn.model_selection import cross_val_score\n","\n","# get data\n","df = pd.read_csv(notes_home+\"assets/wdbc.csv\")\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# set up the model\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n","\n","# do the 5-fold cross validation\n","scores = cross_val_score(model, X, y, cv=5)\n","print(\"Fold Accuracies: {}\".format(scores))\n","print(\"Accuracy: {:3.2f}\".format(scores.mean()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fold Accuracies: [0.90 0.91 0.96 0.94 0.96]\n","Accuracy: 0.93\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CIJS_MTJYqNJ"},"source":["# Model Evaluation - the Grid Search\n","\n","You probably figured out by now that the only way to find the best model for a particular dataset is to search for it by trying different (hyper-)parameters that control the complexity of the models.  Therefore:\n","\n","* Finding the best model involves searching for (hyper-)parameter values that give you the best testing/cross-validation accuracy.\n","* This is usually referred to as the *grid search*.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gd2jmal2YqNK"},"source":["Sklearn helps us do that efficiently:\n","\n","Sklearn has a built-in grid search that can optimize the model parameters.  In our case the decision tree classifiere has two parameters: criterion and depth. The grid search will find the optimal value for both of these parameters. The grid search function will return two things:\n","\n","1. the optimal parameter set\n","2. the optimal classifier\n","\n","Consider,"]},{"cell_type":"code","metadata":{"id":"hJ3L6Tf2YqNK","outputId":"de170168-5fdc-4a2c-d0e2-b7575de922fd"},"source":["# Grid search with cross-validation for iris dataset\n","import pandas as pd\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","\n","# get data\n","df = pd.read_csv(notes_home+\"assets/iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# setting up grid search\n","model = model = tree.DecisionTreeClassifier(random_state=1)\n","param_grid = {\n","    'max_depth': list(range(1,11)), # search 1..10\n","    'criterion': ['entropy', 'gini']\n","    }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","\n","# performing grid search \n","grid.fit(X,y)\n","\n","# print out best parameters\n","print(\"Best parameters: {}\".format(grid.best_params_))\n","\n","# print out the best model\n","print(\"Best tree:\")\n","tree_print(grid.best_estimator_,X)\n","\n","# compute the accuracy       \n","predict_y = grid.best_estimator_.predict(X)\n","acc = accuracy_score(y, predict_y)\n","\n","# print accuracy          \n","print(\"Accuracy: {:3.2f}\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best parameters: {'criterion': 'gini', 'max_depth': 4}\n","Best tree:\n","if Petal.Width =< 0.800000011920929: \n","  |then setosa\n","  |else if Petal.Width =< 1.75: \n","  |  |then if Petal.Length =< 4.950000047683716: \n","  |  |  |then if Petal.Width =< 1.6500000357627869: \n","  |  |  |  |then versicolor\n","  |  |  |  |else virginica\n","  |  |  |else if Petal.Width =< 1.550000011920929: \n","  |  |  |  |then virginica\n","  |  |  |  |else versicolor\n","  |  |else if Petal.Length =< 4.8500001430511475: \n","  |  |  |then if Sepal.Length =< 5.950000047683716: \n","  |  |  |  |then versicolor\n","  |  |  |  |else virginica\n","  |  |  |else virginica\n","<---------->\n","Tree Depth:  4\n","Accuracy: 0.99\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZPWg3VJqYqNK","outputId":"ef36924f-5b94-4414-a07d-e89fbddfbbd1"},"source":["# Grid search with cross-validation for wisconsin breast cancer dataset\n","import pandas as pd\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","\n","# get data\n","df = pd.read_csv(notes_home+\"assets/wdbc.csv\")\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# setting up grid search\n","model = model = tree.DecisionTreeClassifier()\n","param_grid = {\n","    'max_depth': list(range(1,11)), # search 1..10\n","    'criterion': ['entropy', 'gini']\n","    }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","\n","# performing grid search \n","grid.fit(X,y)\n","\n","# print out what we found\n","print(\"Best parameters: {}\".format(grid.best_params_))\n","\n","# print out the best model\n","print(\"Best tree:\")\n","tree_print(grid.best_estimator_,X)\n","\n","# Get the accuracy\n","# Evaluate the tree\n","# predicting        \n","predict_y = grid.best_estimator_.predict(X)\n","\n","# accuracy          \n","print(\"Accuracy: {:3.2f}\".format(accuracy_score(y, predict_y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best parameters: {'criterion': 'entropy', 'max_depth': 4}\n","Best tree:\n","if perimeter3 =< 105.95000076293945: \n","  |then if concave_points3 =< 0.13505000621080399: \n","  |  |then if area2 =< 48.97500038146973: \n","  |  |  |then if texture3 =< 30.145000457763672: \n","  |  |  |  |then B\n","  |  |  |  |else B\n","  |  |  |else if fractal_dimension2 =< 0.004316499922424555: \n","  |  |  |  |then M\n","  |  |  |  |else B\n","  |  |else if texture3 =< 27.575000762939453: \n","  |  |  |then if symmetry3 =< 0.35785000026226044: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","  |else if perimeter3 =< 117.44999694824219: \n","  |  |then if smoothness3 =< 0.13610000163316727: \n","  |  |  |then if texture3 =< 25.670000076293945: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else if symmetry3 =< 0.2572999894618988: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |else if concave_points3 =< 0.08586500212550163: \n","  |  |  |then if fractal_dimension1 =< 0.05287500098347664: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","<---------->\n","Tree Depth:  4\n","Accuracy: 0.98\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nC4tZjaJYqNL"},"source":["**Note**: Grid search is computationally very expensive! In the two cases above we had two types of splitting criteria (gini and entropy) and we had 10 levels of complexity.  In addition we performed 5-CV for each parameter combination.  Doing the math we built $2{\\times}10{\\times}5 = 100$ models for each grid search."]},{"cell_type":"markdown","metadata":{"id":"-dAakZWcYqNL"},"source":["# Model Accuracy Reexamined\n","\n","Consider a classification problem with two classes, then we observe the following outcomes of a prediction of a suitable classification model:\n","\n",">true positive (TP) -- predicted positive coincides with actual positive\n","\n",">true negative (TN) -- predicted negative coincides with actual negative\n","\n",">false positive (FP), Type I error -- predicted positive but actual negative\n","\n",">false negative (FN), Type II error -- predicted negative but actual positive\n","\n","Two types of errors possible!\n"]},{"cell_type":"markdown","metadata":{"id":"W3LEtcjRYqNL"},"source":["* We can arrange the predictions in a matrix form\n","* Errors will show up as values outside the major diagonal"]},{"cell_type":"markdown","metadata":{"id":"Ws6mbE8TYqNL"},"source":["<img src=\"https://raw.githubusercontent.com/lutzhamel/ds/master/notes/assets/confusion2.png\" height=\"400\" width=\"800\">"]},{"cell_type":"markdown","metadata":{"id":"b0g6hVA9YqNM"},"source":["# The Wisconsin Breast Cancer Data Set\n","\n","Let's apply everything we have learned so far: build the best model, and then evaluate it."]},{"cell_type":"code","metadata":{"id":"f3a_kG21YqNM","outputId":"1ff21d84-5a0a-4c66-c855-d9007cbc71f5"},"source":["# set up\n","import pandas as pd\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix\n","\n","# get data \n","df = pd.read_csv(notes_home+\"assets/wdbc.csv\")\n","\n","# create our sklearn data\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# setting up grid search using 5-fold CV\n","model = tree.DecisionTreeClassifier(random_state=1)\n","param_grid = {\n","    'max_depth': list(range(1,11)),\n","    'criterion': ['entropy', 'gini']\n","    }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","\n","# performing grid search \n","null = grid.fit(X,y)\n","\n","# print out what we found\n","print(\"Best parameters: {}\".format(grid.best_params_))\n","\n","# print out the best model\n","print(\"Best tree:\")\n","tree_print(grid.best_estimator_,X)\n","\n","# compute and print the accuracy\n","predict_y = grid.best_estimator_.predict(X)\n","acc = accuracy_score(y, predict_y)\n","print(\"Accuracy: {:3.2f}\".format(acc))\n","\n","# build and print the confusion matrix \n","labels = ['M','B']\n","cm = confusion_matrix(y, predict_y, labels=labels)\n","cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best parameters: {'criterion': 'entropy', 'max_depth': 4}\n","Best tree:\n","if perimeter3 =< 105.95000076293945: \n","  |then if concave_points3 =< 0.13505000621080399: \n","  |  |then if area2 =< 48.97500038146973: \n","  |  |  |then if texture3 =< 30.145000457763672: \n","  |  |  |  |then B\n","  |  |  |  |else B\n","  |  |  |else if compactness3 =< 0.08476000279188156: \n","  |  |  |  |then M\n","  |  |  |  |else B\n","  |  |else if texture3 =< 27.575000762939453: \n","  |  |  |then if symmetry3 =< 0.35785000026226044: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","  |else if perimeter3 =< 117.44999694824219: \n","  |  |then if smoothness3 =< 0.13610000163316727: \n","  |  |  |then if texture3 =< 25.670000076293945: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else if texture1 =< 13.420000076293945: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |else if smoothness3 =< 0.09975999966263771: \n","  |  |  |then if fractal_dimension1 =< 0.053039999678730965: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","<---------->\n","Tree Depth:  4\n","Accuracy: 0.98\n","Confusion Matrix:\n","     M    B\n","M  210    2\n","B    7  350\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"84F_LVAOYqNM"},"source":["# Reading\n","\n","[Hyperparameters and Model Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html)"]},{"cell_type":"markdown","metadata":{"id":"ZloW_ew9YqNN"},"source":["# Midterm \n","\n","Please refer to the midterm BrightSpace page."]},{"cell_type":"code","metadata":{"id":"DyEiuRR4YqNN"},"source":[""],"execution_count":null,"outputs":[]}]}