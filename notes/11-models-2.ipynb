{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"n8WKWoVMYqMy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101464471,"user_tz":300,"elapsed":533,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"07f301b6-11a3-4378-8dc8-71f5a3fa90f4"},"source":["###### Set Up #####\n","# verify our folder with the data and module assets is installed\n","# if it is installed make sure it is the latest\n","!test -e ds-assets && cd ds-assets && git pull && cd ..\n","# if it is not installed clone it\n","!test ! -e ds-assets && git clone https://github.com/lutzhamel/ds-assets.git\n","# point to the folder with the assets\n","home = \"ds-assets/assets/\"\n","import sys\n","sys.path.append(home)      # add home folder to module search path"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"code","source":["# format output from library calls\n","import numpy as np\n","np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})"],"metadata":{"id":"j-Vbdm-RKsFE","executionInfo":{"status":"ok","timestamp":1707101464472,"user_tz":300,"elapsed":2,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gjsCjE04YqM6"},"source":["# Evaluating Models"]},{"cell_type":"markdown","metadata":{"id":"GpGoX5wvYqM6"},"source":["Learning Curves\n","\n","* It can be shown that *any model* can learn its training data perfectly - “memorize it”.  That is what the blue curve shows below. Any model can achieve a perfect score on the training data as long as it is allowed to be complex enough. Maximum complexity: the model has memorized the entire dataset.\n","\n","* But memorizing is not the same as learning inherent patterns and use those patterns to make predictions!  Memorization is extremely bad at predicting future outcomes.  See what happens to the red line below as the model starts to memorize the training dataset -- the score on the test dataset actually falls!\n","\n","> Memorization does not generalize well!\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"3cUe2BZKYqM7"},"source":["If we train a model using *training data* and then apply the model to a *validation/test data set* then we obtain these following typical curves:\n","\n","<!-- ![model curves](assets/model-performance-curves.png) -->\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/train-test-curves.png\"  height=\"300\" width=\"450\">\n","\n","Note: Validation/test data is data that the model has not seen yet as part of its training."]},{"cell_type":"markdown","metadata":{"id":"FpfBnlIgYqM8"},"source":["Simply put:\n","\n","1. Undertrained models make a lot of errors on test data because they have not learned any of the patterns yet.\n","\n","2. Overtrained models (models that have memorized their training data) make a lot of errors on test data because memorization is extremely bad at predicting the future outcomes.\n","\n","3. The best models make a trade-off between errors and recognizing important patterns. Notice that for the best models the training score is not 100%!\n","\n","> In order to find the best model we have to search its *parameter space* to find just the right complexity level."]},{"cell_type":"markdown","metadata":{"id":"0OEfoZhwYqM8"},"source":["**Note**: The code in this notebook takes advantage of the `random_state` parameter in a lot of scikit-learn functions.  This is only done to keep the results of this notebook deterministic.  This variable is strictly not necessary in general applications."]},{"cell_type":"markdown","metadata":{"id":"JYglFKc2YqM9"},"source":["# Train and Test/Validate"]},{"cell_type":"markdown","metadata":{"id":"CZC_GX96YqM9"},"source":["In order to simulate the fact that a model is not able to see all possible data points during training we split our training data into two parts:\n","\n","* Training data\n","* Testing/validation data\n","\n","We will train our model on the training data as before but we will now test the model performance on the testing data which the model has not seen yet.\n","\n","> That is, we force the model to make some generalizations.\n"]},{"cell_type":"markdown","metadata":{"id":"sDAMtz1qYqM-"},"source":["## Decision Trees\n","\n","Let's see if we can observe this learning behavior in decision trees.  We apply decision tree models to two different datasets where we build models of different complexities.  \n","\n","The following is the block of imports that we need for our model building."]},{"cell_type":"code","metadata":{"id":"eTWm0JbmYqM-","executionInfo":{"status":"ok","timestamp":1707101466572,"user_tz":300,"elapsed":2102,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["import pandas as pd\n","from treeviz import tree_print\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","# sklearn provides manipulation of training sets\n","# here we do train/test split\n","from sklearn.model_selection import train_test_split"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yx-cHZu7YqM-"},"source":["### The Iris Dataset\n","\n","We start with the iris dataset.  We would expect a lower accuracy from both the low-complexity and high-complexity models compared to a medium-complexity model which most likely would be close to the ideal model.\n"]},{"cell_type":"code","metadata":{"id":"Gkq8K9pYYqM-","executionInfo":{"status":"ok","timestamp":1707101466572,"user_tz":300,"elapsed":8,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["# set up our sklearn data shape for the iris data\n","df = pd.read_csv(home+\"iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# split the data - 70% training 30% testing\n","(X_train, X_test, y_train, y_test) = \\\n","    train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=2)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhtZu3EbYqM_"},"source":["#### Low Complexity Tree\n","\n","Here we limit the depth of decision tree to 1."]},{"cell_type":"code","metadata":{"id":"bX5Blp7tYqM_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101466572,"user_tz":300,"elapsed":6,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"9c6e4682-2216-4f8f-dfb5-b66b941df0e2"},"source":["# set up the tree model object\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["if Petal.Width =< 0.800000011920929: \n","  |then setosa\n","  |else virginica\n","<->\n","Tree Depth:  1\n","Train Accuracy: 0.67\n","Test Accuracy: 0.67\n"]}]},{"cell_type":"markdown","metadata":{"id":"EEGUdBFvBZy5"},"source":["Limiting the tree depth to 1 implies that we can only have a single if-then-else structure as the model.  That means the model can only discriminate between two alternatives.  However, our data has three labels. Consequently, the points of one of the labels will be misclassified.  Interestingly enough, this shows up in the accuracy scores: Only roughly two thirds of the data points are classified correctly and one third is misclassified.  Looking at the model itself we see that it discriminates between `setosa` and `virginica` implying that it misclassifies points with the label `versicolor`.  An interesting exercise would be to visualize this model as we did towards the end of the [visualization notebook](https://colab.research.google.com/github/lutzhamel/ds-notes/blob/master/notes/09-visualization.ipynb) and observe where the model makes the errors.\n"]},{"cell_type":"markdown","metadata":{"id":"_n_3glDSYqNB"},"source":["#### Medium Complexity Tree\n","\n","Here we limit the depth of the decision tree to 3."]},{"cell_type":"code","metadata":{"id":"TkwcT_FLYqNB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101466803,"user_tz":300,"elapsed":234,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"33963d8b-4a08-4220-e31c-6d0d7d1e69e1"},"source":["# set up the tree model object\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["if Petal.Length =< 2.350000023841858: \n","  |then setosa\n","  |else if Petal.Width =< 1.6500000357627869: \n","  |  |then if Petal.Length =< 4.950000047683716: \n","  |  |  |then versicolor\n","  |  |  |else virginica\n","  |  |else if Petal.Length =< 4.8500001430511475: \n","  |  |  |then virginica\n","  |  |  |else virginica\n","<------->\n","Tree Depth:  3\n","Train Accuracy: 0.98\n","Test Accuracy: 0.98\n"]}]},{"cell_type":"markdown","metadata":{"id":"y5k7y6tNYqNB"},"source":["#### High Complexity Tree\n","\n","Here we do not place any limits on the structure of the decision tree."]},{"cell_type":"code","metadata":{"id":"PeUOtP8qYqNB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101466804,"user_tz":300,"elapsed":8,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"22c9e954-77b2-4123-a5db-686c55b48949"},"source":["# set up the tree model object\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["if Petal.Width =< 0.800000011920929: \n","  |then setosa\n","  |else if Petal.Width =< 1.6500000357627869: \n","  |  |then if Petal.Length =< 4.950000047683716: \n","  |  |  |then versicolor\n","  |  |  |else if Petal.Width =< 1.550000011920929: \n","  |  |  |  |then virginica\n","  |  |  |  |else versicolor\n","  |  |else if Petal.Length =< 4.8500001430511475: \n","  |  |  |then if Sepal.Width =< 3.100000023841858: \n","  |  |  |  |then virginica\n","  |  |  |  |else versicolor\n","  |  |  |else virginica\n","<---------->\n","Tree Depth:  4\n","Train Accuracy: 1.00\n","Test Accuracy: 0.96\n"]}]},{"cell_type":"markdown","metadata":{"id":"9ynVxu2DuiF0"},"source":["Sure enough, if we look at the **test accuracy** of the three different models,\n","\n","* low-complexity model: 0.67\n","* medium-complexity model: 0.98\n","* high-complexity model: 0.96\n","\n","We see that the model performance behaves exactly as predicted by the learning curves above: The best model performance is by the medium-complexity model!\n","The **training accuracy** also behaves as predicted: the most complex model simply memorizes the dataset,\n","\n","* low-complexity model: 0.67\n","* medium-complexity model: 0.98\n","* high-complexity model: 1.00\n"]},{"cell_type":"markdown","metadata":{"id":"2Is2IM9NYqNC"},"source":["### Wisconsin Breast Cancer Dataset\n","\n","Let's try this again with a slightly larger datasest. This data set is available at <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">UCI</a>.\n","The data set describes benign and malignent tumors based on image measurements."]},{"cell_type":"code","metadata":{"id":"Y9Z8F6qrYqNC","colab":{"base_uri":"https://localhost:8080/","height":273},"executionInfo":{"status":"ok","timestamp":1707101466804,"user_tz":300,"elapsed":7,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"d0c8d7ac-6541-4d88-aea3-96cdae663b0a"},"source":["# set up our sklearn data shape for the iris data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","print(df.shape)\n","df.head()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(569, 32)\n"]},{"output_type":"execute_result","data":{"text/plain":["   ID  radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n","0   1    17.99     10.38      122.80  1001.0      0.11840       0.27760   \n","1   2    20.57     17.77      132.90  1326.0      0.08474       0.07864   \n","2   3    19.69     21.25      130.00  1203.0      0.10960       0.15990   \n","3   4    11.42     20.38       77.58   386.1      0.14250       0.28390   \n","4   5    20.29     14.34      135.10  1297.0      0.10030       0.13280   \n","\n","   concavity1  concave_points1  symmetry1  ...  texture3  perimeter3   area3  \\\n","0      0.3001          0.14710     0.2419  ...     17.33      184.60  2019.0   \n","1      0.0869          0.07017     0.1812  ...     23.41      158.80  1956.0   \n","2      0.1974          0.12790     0.2069  ...     25.53      152.50  1709.0   \n","3      0.2414          0.10520     0.2597  ...     26.50       98.87   567.7   \n","4      0.1980          0.10430     0.1809  ...     16.67      152.20  1575.0   \n","\n","   smoothness3  compactness3  concavity3  concave_points3  symmetry3  \\\n","0       0.1622        0.6656      0.7119           0.2654     0.4601   \n","1       0.1238        0.1866      0.2416           0.1860     0.2750   \n","2       0.1444        0.4245      0.4504           0.2430     0.3613   \n","3       0.2098        0.8663      0.6869           0.2575     0.6638   \n","4       0.1374        0.2050      0.4000           0.1625     0.2364   \n","\n","   fractal_dimension3  Diagnosis  \n","0             0.11890          M  \n","1             0.08902          M  \n","2             0.08758          M  \n","3             0.17300          M  \n","4             0.07678          M  \n","\n","[5 rows x 32 columns]"],"text/html":["\n","  <div id=\"df-bfc5b8b6-6006-45f0-afff-85d8ef2d6c67\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>radius1</th>\n","      <th>texture1</th>\n","      <th>perimeter1</th>\n","      <th>area1</th>\n","      <th>smoothness1</th>\n","      <th>compactness1</th>\n","      <th>concavity1</th>\n","      <th>concave_points1</th>\n","      <th>symmetry1</th>\n","      <th>...</th>\n","      <th>texture3</th>\n","      <th>perimeter3</th>\n","      <th>area3</th>\n","      <th>smoothness3</th>\n","      <th>compactness3</th>\n","      <th>concavity3</th>\n","      <th>concave_points3</th>\n","      <th>symmetry3</th>\n","      <th>fractal_dimension3</th>\n","      <th>Diagnosis</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.3001</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>...</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.1622</td>\n","      <td>0.6656</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.0869</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>...</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.1238</td>\n","      <td>0.1866</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.1974</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>...</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.1444</td>\n","      <td>0.4245</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.2414</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>...</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.2098</td>\n","      <td>0.8663</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.1980</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>...</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.1374</td>\n","      <td>0.2050</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","      <td>M</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 32 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfc5b8b6-6006-45f0-afff-85d8ef2d6c67')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bfc5b8b6-6006-45f0-afff-85d8ef2d6c67 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bfc5b8b6-6006-45f0-afff-85d8ef2d6c67');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-99ba9026-ac6f-4c8f-82f3-a801c59698b6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99ba9026-ac6f-4c8f-82f3-a801c59698b6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-99ba9026-ac6f-4c8f-82f3-a801c59698b6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"BBAt8H9RYqNE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101466982,"user_tz":300,"elapsed":182,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"43eb08e2-9dbe-4f27-feac-a08d073315a9"},"source":["# see if our data set is balanced\n","df[['Diagnosis']].value_counts()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Diagnosis\n","B            357\n","M            212\n","dtype: int64"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"4YNNuJijYqNF","executionInfo":{"status":"ok","timestamp":1707101466982,"user_tz":300,"elapsed":6,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["# set up our various datasets\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df[['Diagnosis']]\n","\n","# split the data\n","(X_train, X_test, y_train, y_test) = \\\n","    train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=1)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DO0NPMwrYqNF"},"source":["#### Low Complexity Tree\n","\n","Limit the model to a depth of 1."]},{"cell_type":"code","metadata":{"id":"UpY6qRv2YqNF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101466982,"user_tz":300,"elapsed":5,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"45f7e85b-caaf-49fd-e98c-d25eb89b4401"},"source":["# set up the tree model object\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=1)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["if perimeter3 =< 104.10000228881836: \n","  |then B\n","  |else M\n","<->\n","Tree Depth:  1\n","Train Accuracy: 0.94\n","Test Accuracy: 0.85\n"]}]},{"cell_type":"markdown","metadata":{"id":"uNqiiOLBYqNG"},"source":["#### Medium Complexity Tree\n","\n","Limit the model to a depth of 4."]},{"cell_type":"code","metadata":{"id":"MV6Sv3yEYqNG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101467129,"user_tz":300,"elapsed":150,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"5884a32d-a6fa-4dcf-b908-8ca6417f67a8"},"source":["# set up the tree model object\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=2)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["if perimeter3 =< 104.10000228881836: \n","  |then if concave_points3 =< 0.13505000621080399: \n","  |  |then if area2 =< 48.97500038146973: \n","  |  |  |then B\n","  |  |  |else if concavity2 =< 0.01723999995738268: \n","  |  |  |  |then M\n","  |  |  |  |else B\n","  |  |else if texture3 =< 29.454999923706055: \n","  |  |  |then B\n","  |  |  |else M\n","  |else if concave_points3 =< 0.14159999787807465: \n","  |  |then if texture3 =< 19.90999984741211: \n","  |  |  |then B\n","  |  |  |else if area2 =< 35.290000915527344: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |else if radius3 =< 15.87000036239624: \n","  |  |  |then if smoothness3 =< 0.1388000026345253: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","<---------->\n","Tree Depth:  4\n","Train Accuracy: 0.98\n","Test Accuracy: 0.92\n"]}]},{"cell_type":"markdown","metadata":{"id":"2Ilhu1CMYqNG"},"source":["#### High Complexity Tree\n","\n","Unlimited model complexity."]},{"cell_type":"code","metadata":{"id":"kolozCbJYqNG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101467130,"user_tz":300,"elapsed":5,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"72a36559-6305-449f-d1f3-38cfba101c7b"},"source":["# set up the tree model object\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=1)\n","\n","# fit the model on the training set of data\n","model.fit(X_train, y_train)\n","tree_print(model,X)\n","\n","# Train results: evaluate the model on the testing set of data\n","y_train_model = model.predict(X_train)\n","print(\"Train Accuracy: {:3.2f}\".format(accuracy_score(y_train, y_train_model)))\n","\n","# Test results: evaluate the model on the testing set of data\n","y_test_model = model.predict(X_test)\n","print(\"Test Accuracy: {:3.2f}\".format(accuracy_score(y_test, y_test_model)))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["if perimeter3 =< 104.10000228881836: \n","  |then if concave_points3 =< 0.13505000621080399: \n","  |  |then if area2 =< 48.97500038146973: \n","  |  |  |then B\n","  |  |  |else if smoothness3 =< 0.10676500201225281: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |else if texture3 =< 29.454999923706055: \n","  |  |  |then B\n","  |  |  |else M\n","  |else if concave_points3 =< 0.14159999787807465: \n","  |  |then if texture3 =< 19.90999984741211: \n","  |  |  |then B\n","  |  |  |else if radius2 =< 0.37575000524520874: \n","  |  |  |  |then if perimeter2 =< 2.0149999856948853: \n","  |  |  |  |  |then if concave_points3 =< 0.08526499941945076: \n","  |  |  |  |  |  |then B\n","  |  |  |  |  |  |else M\n","  |  |  |  |  |else B\n","  |  |  |  |else M\n","  |  |else if radius3 =< 15.87000036239624: \n","  |  |  |then if smoothness3 =< 0.1388000026345253: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","<---------------->\n","Tree Depth:  6\n","Train Accuracy: 1.00\n","Test Accuracy: 0.91\n"]}]},{"cell_type":"markdown","metadata":{"id":"jvzYVFHLGJ3Q"},"source":["Again, we find that the **test accuracy** of the three different models behaves just as predicted by our learning curves above,\n","\n","* low-complexity model: 0.85\n","* medium-complexity model: 0.92\n","* high-complexity model: 0.91\n","\n","The best model performance is by the medium-complexity model!\n","The **training accuracy** also behaves as predicted: the most complex model simply memorizes the dataset,\n","\n","* low-complexity model: 0.94\n","* medium-complexity model: 0.98\n","* high-complexity model: 1.00\n"]},{"cell_type":"markdown","metadata":{"id":"X6aOtYkFDsJ0"},"source":["# Model Search\n","\n","From our discussion above it is clear that in order to find the best model we have to perform a **search** over the model space using parameters that dictate the complexity of the model.  In the case of the decision tree model the tree depth controls the complexity of the model.  In order to find our best model we had to try different values for that parameter, or\n","\n","> We had to **search** over the model space of the decision tree by trying different values for tree depth in order to find a model with just the right complexity.\n"]},{"cell_type":"markdown","metadata":{"id":"Tb6ecfT_YqNH"},"source":["## Train and Test\n","\n","We have already seen that just using a training set for model evaluation does not work! Our solution was to split the training data into a training and a test/validation set.\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/train-test-data.png\" height=\"200\" width=\"450\">"]},{"cell_type":"markdown","metadata":{"id":"C027zEXTYqNH"},"source":["### Problem!\n","\n","* Train-testing relies on randomly splitting the training data into two parts.\n","\n","* If this split just happens to be a 'bad' split our results might be biased.\n","\n","An example of a 'bad' split is that by chance that most of the instances in our dataset with a particular label wind up in the test/validation set and none in the training set.  In this case, the model has a very limited chance to learn about patterns with this particular label and will probably misclassify most the instances with that label in the test/validation set.\n","\n","**Solution:** Cross-validation\n"]},{"cell_type":"markdown","metadata":{"id":"J8QuQ5OoYqNH"},"source":["## Cross-Validation\n","\n","In cross-validation we perform two trials (model constructions) where, in each trial, we switch the roles of our two sets (see the figure below).  In order to evaluate the model performance in cross-validation, we build and evaluate a model in each trial and then take the average performance between the two models as the performance of the cross-validation.  Notice that this will mitigate the 'bad' split issue mentioned above.\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/2fold-xval.png\" height=\"400\" width=\"450\">\n"]},{"cell_type":"markdown","metadata":{"id":"TT_q2zlrYqNI"},"source":["BUT, what if is the split was really bad: e.g. one of the sets does not contain any examples of one of the classes.\n","\n","**Solution:** Create more trials - *n-fold cross-validation*"]},{"cell_type":"markdown","metadata":{"id":"BGoJM3nCYqNI"},"source":["As a solution to a single bad split:\n","* perform the split multiple times,\n","* then train and test on each fold,\n","* take the average of the model performance of each fold in order to determine the **cross-validated model performance**\n","\n","Example:\n","* 5-fold cross-validation - split the training data into 5 partitions (folds)\n","* Use each fold as a test/validation set and the other folds as training set\n","* Multiple splits - even if one is bad it will be balanced out by the others.\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/5fold-xval.png\" height=\"400\" width=\"450\">\n","\n","**Note**: 5-fold cross-validation is interesting because each trial essentially has an 80-20 split: 80% of the data for training and 20% for testing.  This is one of the more common ways to split a dataset into training and testing sets.\n","\n","**Note**: We have to train and test models five times in 5-fold cross-validation."]},{"cell_type":"code","metadata":{"id":"LJJCbpFdYqNI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101467443,"user_tz":300,"elapsed":315,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"e61106ba-2324-46a8-be00-2a37120b6d3b"},"source":["# cross-validation Iris\n","import pandas as pd\n","from sklearn import tree\n","# grab cross validation code\n","from sklearn.model_selection import cross_val_score\n","\n","# get data\n","df = pd.read_csv(home+\"iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# set up the model\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2)\n","\n","# do the 5-fold cross validation\n","scores = cross_val_score(model, X, y, cv=5)\n","print(\"Fold Accuracies: {}\".format(scores))\n","print(\"Accuracy: {:3.2f}\".format(scores.mean()))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold Accuracies: [0.93 0.97 0.90 0.87 1.00]\n","Accuracy: 0.93\n"]}]},{"cell_type":"code","metadata":{"id":"Jp9f_GnKYqNJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101467721,"user_tz":300,"elapsed":281,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"6d01147e-c141-47b8-e15d-a94937c47c5e"},"source":["# cross-validation WDBC\n","import pandas as pd\n","from sklearn import tree\n","# grab cross validation code\n","from sklearn.model_selection import cross_val_score\n","\n","# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# set up the model\n","model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n","\n","# do the 5-fold cross validation\n","scores = cross_val_score(model, X, y, cv=5)\n","print(\"Fold Accuracies: {}\".format(scores))\n","print(\"Accuracy: {:3.2f}\".format(scores.mean()))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold Accuracies: [0.90 0.91 0.96 0.94 0.96]\n","Accuracy: 0.93\n"]}]},{"cell_type":"markdown","metadata":{"id":"oPyEgFNmEo0R"},"source":["## Model Search with Cross-Validation\n","\n","Once we switch to a cross-validation approach our model search becomes something different.  Instead of searching over the model space we search over\n","the **parameter space** for the best set of parameters.\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/cross-validated-curve.png\"  height=\"300\" width=\"450\">\n","\n","This is due to the fact that cross-validation builds **multiple** models and the cross-validated performance is the **mean performance** of the models built on the various folds. Searching over the parameter space is called **grid search**.\n"]},{"cell_type":"markdown","metadata":{"id":"CIJS_MTJYqNJ"},"source":["## Model Search as a Grid Search\n","\n","You probably figured out by now that the only way to find the best model for a particular dataset is to search for it by trying different (hyper-)parameters that control the complexity of the models.  Therefore:\n","\n","* Finding the best model involves searching for (hyper-)parameter values that give you the best testing/cross-validation accuracy.\n","* This is usually referred to as the *grid search*.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gd2jmal2YqNK"},"source":["Sklearn helps us do that efficiently:\n","Sklearn has a built-in grid search that can optimize the model parameters.  In our case the decision tree classifiere has two parameters: criterion and depth. The grid search will find the optimal value for both of these parameters. The grid search function will return two things:\n","\n","1. the optimal parameter set\n","2. the optimal classifier\n","\n","Consider,"]},{"cell_type":"code","metadata":{"id":"hJ3L6Tf2YqNK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101468848,"user_tz":300,"elapsed":1129,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"256c519b-a74f-4794-fafe-5803f3ba9914"},"source":["# Grid search with cross-validation for iris dataset\n","import pandas as pd\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","\n","# get data\n","df = pd.read_csv(home+\"iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# setting up grid search\n","model = model = tree.DecisionTreeClassifier(random_state=1)\n","param_grid = {\n","    'max_depth': list(range(1,11)), # search 1..10\n","    'criterion': ['entropy', 'gini']\n","    }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","\n","# performing grid search\n","grid.fit(X,y)\n","\n","# print out best parameters\n","print(\"Best parameters: {}\".format(grid.best_params_))\n","\n","# print out the best model\n","print(\"Best tree:\")\n","tree_print(grid.best_estimator_,X)\n","\n","# compute the accuracy of optimal classifier\n","predict_y = grid.best_estimator_.predict(X)\n","acc = accuracy_score(y, predict_y)\n","\n","# print accuracy\n","print(\"Accuracy of optimal classifier: {}\".format(acc))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters: {'criterion': 'gini', 'max_depth': 4}\n","Best tree:\n","if Petal.Width =< 0.800000011920929: \n","  |then setosa\n","  |else if Petal.Width =< 1.75: \n","  |  |then if Petal.Length =< 4.950000047683716: \n","  |  |  |then if Petal.Width =< 1.6500000357627869: \n","  |  |  |  |then versicolor\n","  |  |  |  |else virginica\n","  |  |  |else if Petal.Width =< 1.550000011920929: \n","  |  |  |  |then virginica\n","  |  |  |  |else versicolor\n","  |  |else if Petal.Length =< 4.8500001430511475: \n","  |  |  |then if Sepal.Length =< 5.950000047683716: \n","  |  |  |  |then versicolor\n","  |  |  |  |else virginica\n","  |  |  |else virginica\n","<---------->\n","Tree Depth:  4\n","Accuracy of optimal classifier: 0.9933333333333333\n"]}]},{"cell_type":"code","metadata":{"id":"ZPWg3VJqYqNK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707101470988,"user_tz":300,"elapsed":2143,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"98fc3d12-87b7-4bff-f748-7e0bac1ecf65"},"source":["# Grid search with cross-validation for wisconsin breast cancer dataset\n","import pandas as pd\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","\n","# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# setting up grid search\n","model = model = tree.DecisionTreeClassifier()\n","param_grid = {\n","    'max_depth': list(range(1,11)), # search 1..10\n","    'criterion': ['entropy', 'gini']\n","    }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","\n","# performing grid search\n","grid.fit(X,y)\n","\n","# print out what we found\n","print(\"Best parameters: {}\".format(grid.best_params_))\n","\n","# print out the best model\n","print(\"Best tree:\")\n","tree_print(grid.best_estimator_,X)\n","\n","# Get the accuracy\n","# Evaluate the optimal tree\n","# predicting\n","predict_y = grid.best_estimator_.predict(X)\n","\n","# accuracy of optimal classifier\n","print(\"Accuracy of optimal model: {:3.2f}\".format(accuracy_score(y, predict_y)))"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters: {'criterion': 'entropy', 'max_depth': 4}\n","Best tree:\n","if perimeter3 =< 105.95000076293945: \n","  |then if concave_points3 =< 0.13505000621080399: \n","  |  |then if area2 =< 48.97500038146973: \n","  |  |  |then if texture3 =< 30.145000457763672: \n","  |  |  |  |then B\n","  |  |  |  |else B\n","  |  |  |else if symmetry3 =< 0.20784999430179596: \n","  |  |  |  |then M\n","  |  |  |  |else B\n","  |  |else if texture3 =< 27.575000762939453: \n","  |  |  |then if symmetry3 =< 0.35785000026226044: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","  |else if perimeter3 =< 117.44999694824219: \n","  |  |then if smoothness3 =< 0.13610000163316727: \n","  |  |  |then if texture3 =< 25.670000076293945: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else if symmetry3 =< 0.2572999894618988: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |else if smoothness3 =< 0.09975999966263771: \n","  |  |  |then if radius2 =< 0.559799998998642: \n","  |  |  |  |then B\n","  |  |  |  |else M\n","  |  |  |else M\n","<---------->\n","Tree Depth:  4\n","Accuracy of optimal model: 0.98\n"]}]},{"cell_type":"markdown","metadata":{"id":"nC4tZjaJYqNL"},"source":["**Note**: Grid search is computationally very expensive! In the two cases above we had two types of splitting criteria (gini and entropy) and we had 10 levels of complexity.  In addition we performed 5-fold cross-validation for each parameter combination.  Doing the math , we built $2{\\times}10{\\times}5 = 100$ models for each grid search."]},{"cell_type":"markdown","metadata":{"id":"-dAakZWcYqNL"},"source":["# Model Accuracy Reexamined\n","\n","Consider a classification problem with two classes, then we observe the following outcomes of a prediction of a suitable classification model:\n","\n",">true positive (TP) -- predicted positive coincides with actual positive\n","\n",">true negative (TN) -- predicted negative coincides with actual negative\n","\n",">false positive (FP), Type I error -- predicted positive but actual negative\n","\n",">false negative (FN), Type II error -- predicted negative but actual positive\n","\n","Two types of errors possible!\n"]},{"cell_type":"markdown","metadata":{"id":"W3LEtcjRYqNL"},"source":["### The Confusion Matrix\n","\n","* We can arrange the predictions in a matrix form\n","* Errors will show up as values outside the major diagonal"]},{"cell_type":"markdown","metadata":{"id":"Ws6mbE8TYqNL"},"source":["<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/confusion2.png\" height=\"200\" width=\"250\">"]},{"cell_type":"markdown","metadata":{"id":"kb1HpOt_P15o"},"source":["# Putting it All Together"]},{"cell_type":"markdown","metadata":{"id":"b0g6hVA9YqNM"},"source":["## The Wisconsin Breast Cancer Data Set\n","\n","Let's apply everything we have learned so far: build the best model, and then evaluate it."]},{"cell_type":"code","metadata":{"id":"f3a_kG21YqNM"},"source":["# set up\n","import pandas as pd\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix\n","\n","# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","\n","# create our sklearn data\n","X  = df.drop(['ID','Diagnosis'],axis=1)\n","y = df[['Diagnosis']]\n","\n","# setting up grid search using 5-fold CV\n","model = tree.DecisionTreeClassifier(random_state=1)\n","param_grid = {\n","    'max_depth': list(range(1,11)),\n","    'criterion': ['entropy', 'gini']\n","    }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","\n","# performing grid search\n","null = grid.fit(X,y)\n","\n","# print out what we found\n","print(\"Best parameters: {}\".format(grid.best_params_))\n","\n","# print out the best model\n","print(\"Best tree:\")\n","tree_print(grid.best_estimator_,X)\n","\n","# compute and print the accuracy of best model\n","predict_y = grid.best_estimator_.predict(X)\n","acc = accuracy_score(y, predict_y)\n","print(\"Accuracy of best model: {:3.2f}\".format(acc))\n","\n","# build and print the confusion matrix\n","labels = ['M','B']\n","cm = confusion_matrix(y, predict_y, labels=labels)\n","cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wRyxcXR-u3BW"},"source":["y.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fvy3Rl9iwals"},"source":["Taking our top three features from our decision tree and creating a scatter plot."]},{"cell_type":"code","metadata":{"id":"Y-h6mknWvDpW"},"source":["import seaborn as sns\n","sns.set()\n","sns.pairplot(df, hue='Diagnosis',vars=[\"perimeter3\", \"concave_points3\", \"smoothness3\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObcLj7XGvWGQ"},"source":["## The Iris Data Set\n","\n","Here we are building a three way confusion matrix because we have three classification labels.  We apply our grid search to find the best model"]},{"cell_type":"code","metadata":{"id":"xsrwjAt-vuYh"},"source":["# Grid search with cross-validation for iris dataset\n","import pandas as pd\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","\n","# get data\n","df = pd.read_csv(home+\"iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# setting up grid search\n","model = model = tree.DecisionTreeClassifier(random_state=1)\n","param_grid = {\n","    'max_depth': list(range(1,11)), # search 1..10\n","    'criterion': ['entropy', 'gini']\n","    }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","\n","# performing grid search\n","grid.fit(X,y)\n","\n","# print out best parameters\n","print(\"Best parameters: {}\".format(grid.best_params_))\n","\n","# print out the best model\n","print(\"Best tree:\")\n","tree_print(grid.best_estimator_,X)\n","\n","# compute the accuracy of the best model\n","predict_y = grid.best_estimator_.predict(X)\n","acc = accuracy_score(y, predict_y)\n","\n","# print accuracy of best model\n","print(\"Accuracy of best model: {:3.2f}\".format(acc))\n","\n","# build and print the confusion matrix\n","labels = ['setosa','versicolor','virginica']\n","cm = confusion_matrix(y, predict_y, labels=labels)\n","cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDd4DaQ9wP6W"},"source":["y.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AC7sVyrbwq4T"},"source":["Scatter plot with the two top features from the decision tree."]},{"cell_type":"code","metadata":{"id":"pGdme1b1ww1r"},"source":["import seaborn as sns\n","sns.set()\n","sns.pairplot(df, hue='Species', vars=[\"Petal.Width\", \"Petal.Length\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84F_LVAOYqNM"},"source":["# Reading\n","\n","[Hyperparameters and Model Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html)"]},{"cell_type":"markdown","metadata":{"id":"ZloW_ew9YqNN"},"source":["# Midterm\n","\n","Please refer to the midterm BrightSpace page."]},{"cell_type":"code","metadata":{"id":"DyEiuRR4YqNN"},"source":[],"execution_count":null,"outputs":[]}]}