{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble to be able to run notebooks in Jupyter and Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    notes_home = \"/content/drive/Shared drives/CSC310/notes/\"\n",
    "    user_home = \"/content/drive/My Drive/\"\n",
    "    \n",
    "    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n",
    "    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & ML\n",
    "\n",
    "We saw that we convert text document into a ‘vector model’ (bag-of-words).\n",
    "\n",
    "The vector model allows us to perform mathematical analysis on documents - “which documents are similar to each other?”\n",
    "\n",
    "> Next question: can we construct machine learning models on document collections using the vector model?\n",
    "\n",
    "**Yes!** We can construct classifiers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again our news article data set.\n",
    "\n",
    "We would like to construct a classifier that can correctly classifier political and science documents.\n",
    "\n",
    "We will begin with our KNN algorithm (k nearest neighbors). Since documents are considered point in an n-dimensional space KNN seems well suited for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from assets.confint import classification_confint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** data **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lutz/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn billions of dollars (%GNP):\\nyear  GNP   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ajteel@dendrite.cs.Colorado.EDU (A.J. Teel) w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nMy opinion is this:  In a society whose econ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahhh, remember the days of Yesterday?  When we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\"...a la Chrysler\"??  Okay kids, to the near...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  \\nIn billions of dollars (%GNP):\\nyear  GNP   ...\n",
       "1   ajteel@dendrite.cs.Colorado.EDU (A.J. Teel) w...\n",
       "2  \\nMy opinion is this:  In a society whose econ...\n",
       "3  Ahhh, remember the days of Yesterday?  When we...\n",
       "4  \\n\"...a la Chrysler\"??  Okay kids, to the near..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"******** data **********\")\n",
    "\n",
    "# get the newsgroup database\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=cats)\n",
    "\n",
    "# extract into dataframes\n",
    "texts = pd.DataFrame(newsgroups_train.data, columns=['text'])\n",
    "labels = pd.DataFrame(newsgroups_train.target, columns=['label'])['label'].apply(lambda x: cats[x])\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** docarray **********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1058, 6267)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"******** docarray **********\")\n",
    "\n",
    "# build the stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "# get the default text analyzer from CountVectorizer\n",
    "analyzer = vectorizer = CountVectorizer(analyzer = \"word\", token_pattern = \"[a-zA-Z]+\").build_analyzer()\n",
    "\n",
    "# build a new analyzer that stems using the default analyzer to create the words to be stemmed\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "# build docarrayu\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
    "                                 binary=True,\n",
    "                                 min_df=2)\n",
    "docarray = vectorizer.fit_transform(texts.loc[:,'text']).toarray()\n",
    "docarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** model **********\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   15.5s remaining:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   15.5s remaining:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   19.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: best parameters: {'n_neighbors': 7}\n"
     ]
    }
   ],
   "source": [
    "print(\"******** model **********\")\n",
    "\n",
    "\n",
    "# KNN\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# grid search\n",
    "param_grid = {'n_neighbors': list(range(1,15,3))}\n",
    "grid = GridSearchCV(model, param_grid, cv=2, verbose=10, n_jobs=-1)\n",
    "grid.fit(docarray, labels)\n",
    "print(\"Grid Search: best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Accuracy **********\n",
      "Accuracy: 0.86 (0.84,0.88)\n"
     ]
    }
   ],
   "source": [
    "print(\"******** Accuracy **********\")\n",
    "\n",
    "# accuracy of best model with confidence interval\n",
    "best_model = grid.best_estimator_\n",
    "predict_y = best_model.predict(docarray)\n",
    "acc = accuracy_score(labels, predict_y)\n",
    "lb,ub = classification_confint(acc,docarray.shape[0])\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** confusion matrix **********\n",
      "Confusion Matrix:\n",
      "                    talk.politics.misc  sci.space\n",
      "talk.politics.misc                 514         79\n",
      "sci.space                           73        392\n"
     ]
    }
   ],
   "source": [
    "print(\"******** confusion matrix **********\")\n",
    "\n",
    "# build the confusion matrix\n",
    "cm = confusion_matrix(labels, predict_y, labels=cats)\n",
    "cm_df = pd.DataFrame(cm, index=cats, columns=cats)\n",
    "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (NB)\n",
    "\n",
    "* “Standard” model for text processing\n",
    "* Fast to train, has no problems with very high dimensional data\n",
    "* NB is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. \n",
    "* In simple terms, a NB classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. \n",
    "* For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mathematics\n",
    "\n",
    "[Source](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained)\n",
    "\n",
    "* Bayes theorem provides a way of calculating posterior probability $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$. Look at the equation below, where\n",
    "  * $P(c|x)$ is the posterior probability of class (c, target) given predictor (x, attributes).\n",
    "  * $P(c)$ is the prior probability of class.\n",
    "  * $P(x|c)$ is the likelihood which is the probability of predictor given class.\n",
    "  * $P(x)$ is the prior probability of predictor.\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's assume we have a predictor `Weather` and a target `Play` that contains classes (left table below).  \n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41.png\">\n",
    "\n",
    "We want to compute if we play tennis when sunny.  That is we compute the two probabilities,\n",
    "1. $P(Yes|Sunny)$\n",
    "1. $P(No|Sunny)$\n",
    "and then pick the statement with the higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, NB just counts, let's look at $P(Yes|Sunny)$,\n",
    "\n",
    "$P(Yes|Sunny) = \\frac{P(Sunny|Yes)P(Yes)}{P(Sunny)} = \\frac{3/9\\times 9/14}{5/14} = \\frac{.33 \\times .64}{.36}=.60$\n",
    "\n",
    "Now, let's look at $P(No|Sunny)$,\n",
    "\n",
    "$P(No|Sunny) = \\frac{P(Sunny|No)P(No)}{P(Sunny)} = \\frac{2/5\\times 5/14}{5/14} = \\frac{.40 \\times .36}{.36}=.40$\n",
    "\n",
    "We are playing tennis when sunny because the posterior probability $P(Yes|Sunny)$ is higher."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take our text classification problem and use a Naive Bayes classifier on it.\n",
    "\n",
    "The setup and data prep is the same as in the case of the KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** data **********\n",
      "******** docarray **********\n",
      "******** model **********\n",
      "******** Accuracy **********\n",
      "Accuracy: 0.95 (0.94,0.97)\n",
      "******** confusion matrix **********\n",
      "Confusion Matrix:\n",
      "                    talk.politics.misc  sci.space\n",
      "talk.politics.misc                 562         31\n",
      "sci.space                           19        446\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes\n",
    "\n",
    "# setup\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from assets.confint import classification_confint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "print(\"******** data **********\")\n",
    "\n",
    "# get the newsgroup database\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=cats)\n",
    "\n",
    "# extract into dataframes\n",
    "texts = pd.DataFrame(newsgroups_train.data, columns=['text'])\n",
    "labels = pd.DataFrame(newsgroups_train.target, columns=['label'])['label'].apply(lambda x: cats[x])\n",
    "\n",
    "print(\"******** docarray **********\")\n",
    "\n",
    "# build the stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "# get the default text analyzer from CountVectorizer\n",
    "analyzer = vectorizer = CountVectorizer(analyzer = \"word\", token_pattern = \"[a-zA-Z]+\").build_analyzer()\n",
    "\n",
    "# build a new analyzer that stems using the default analyzer to create the words to be stemmed\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "# build docarrayu\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
    "                                 binary=True,\n",
    "                                 min_df=2)\n",
    "docarray = vectorizer.fit_transform(texts.loc[:,'text']).toarray()\n",
    "docarray.shape\n",
    "\n",
    "print(\"******** model **********\")\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "model = MultinomialNB()\n",
    "# NOTE: NB does not have any hyper-parameters - no overfitting - no searching over parameter space!\n",
    "model.fit(docarray, labels)\n",
    "\n",
    "\n",
    "print(\"******** Accuracy **********\")\n",
    "\n",
    "# accuracy of best model with confidence interval\n",
    "best_model = model\n",
    "predict_y = best_model.predict(docarray)\n",
    "acc = accuracy_score(labels, predict_y)\n",
    "lb,ub = classification_confint(acc,docarray.shape[0])\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n",
    "\n",
    "print(\"******** confusion matrix **********\")\n",
    "\n",
    "# build the confusion matrix\n",
    "cm = confusion_matrix(labels, predict_y, labels=cats)\n",
    "cm_df = pd.DataFrame(cm, index=cats, columns=cats)\n",
    "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains very fast and has a higher accuracy than KNN and the difference in accuracy is statistically significant!\n",
    "\n",
    "> NB does not have any hyper-parameters - no overfitting - no searching over parameter space!\n",
    "\n",
    "Hint: Try cross-validating the NB model - you will find that the fold accuracies and the mean accuracy will fall into the CI computed above.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Exercise\n",
    "\n",
    "For this exercise you will build a classifier that can distinguish real news from fake news. A training set for this is available here:\n",
    "https://raw.githubusercontent.com/lutzhamel/fake-news/master/data/fake_or_real_news.csv\n",
    "\n",
    "The fields you are interested in are ‘text’ and ‘label’ with the obvious interpretations. \n",
    "\n",
    "Here are the action items for this exercise:\n",
    "* Use the vector model and text preprocessing techniques from class to construct a training data set.\n",
    "* Determine the dimensions of your vector model and print out the first 10 dimensions\n",
    "* Use that training data set to construct a Naive Bayes classifier.  \n",
    "* Compute the accuracy and 95% CI for the classifier.\n",
    "* Try your analysis with and without data preprocessing, is there a difference in accuracy of the models.\n",
    "\n",
    "The data set contains a large number of articles (takes a long time to train), you can downsample this to something like a 1,000 articles or so in order to speed up training and evaluation (hint: use shuffle).\n",
    "\n",
    "You are free to pick your own team (max three members)\n",
    "\n",
    "Extra Credit:  Try the same thing but instead of ‘text’ use ‘title’ for your training text.  How does a classifier built on this data set compare to the original classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
