{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"19a-NLP-2.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"NcBu5VHyfb_U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618171295328,"user_tz":240,"elapsed":328,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"8af59614-5f46-4db1-b78f-f54b3e1c40c8"},"source":["# preamble to be able to run notebooks in Jupyter and Colab\n","try:\n","    from google.colab import drive\n","    import sys\n","    \n","    drive.mount('/content/drive')\n","    notes_home = \"/content/drive/Shared drives/CSC310/ds/notes/\"\n","    user_home = \"/content/drive/My Drive/\"\n","    \n","    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n","\n","except ModuleNotFoundError:\n","    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n","    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"],"execution_count":94,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V5Aqfl2Lfb_X"},"source":["# NLP & ML\n","\n","We saw that we convert text document into a ‘vector model’ (bag-of-words).\n","\n","The vector model allows us to perform mathematical analysis on documents - “which documents are similar to each other?”\n","\n","> Next question: can we construct machine learning models on document collections using the vector model?\n","\n","**Yes!** We can construct classifiers.\n"]},{"cell_type":"markdown","metadata":{"id":"goe8Cr5Rfb_X"},"source":["Consider again our news article data set.\n","\n","We would like to construct a classifier that can correctly classifier political and science documents.\n","\n","We will begin with our KNN algorithm (k nearest neighbors). Since documents are considered point in an n-dimensional space KNN seems well suited for this problem."]},{"cell_type":"markdown","metadata":{"id":"ZVDJGfo9fb_Y"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"z2Ph5Nlcfb_Y","executionInfo":{"status":"ok","timestamp":1618171295539,"user_tz":240,"elapsed":530,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}}},"source":["# setup\n","import pandas as pd\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from assets.confint import classification_confint\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.stem import PorterStemmer\n","from assets.treeviz import tree_print"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Lx4nJN-fb_Z","colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"status":"ok","timestamp":1618171295544,"user_tz":240,"elapsed":527,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"a94479ab-798a-4d7a-af85-57ae7746ba7d"},"source":["print(\"******** data **********\")\n","\n","# get the newsgroup database\n","newsgroups = pd.read_csv(\"/content/drive/Shared drives/CSC310/ds/notes/assets/newsgroups-noheaders.csv\")\n","newsgroups.head(n=10)"],"execution_count":96,"outputs":[{"output_type":"stream","text":["******** data **********\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\nIn billions of dollars (%GNP):\\nyear  GNP   ...</td>\n","      <td>space</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ajteel@dendrite.cs.Colorado.EDU (A.J. Teel) w...</td>\n","      <td>space</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\\nMy opinion is this:  In a society whose econ...</td>\n","      <td>space</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ahhh, remember the days of Yesterday?  When we...</td>\n","      <td>space</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\\n\"...a la Chrysler\"??  Okay kids, to the near...</td>\n","      <td>space</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>\\n   As for advertising -- sure, why not?  A N...</td>\n","      <td>politics</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>\\n  What, pray tell, does this mean? Just who ...</td>\n","      <td>space</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\\nWhere does the shadow come from?  There's no...</td>\n","      <td>politics</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>^^^^^^^^^...</td>\n","      <td>politics</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Yet, when a law was proposed for Virginia tha...</td>\n","      <td>space</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text     label\n","0  \\nIn billions of dollars (%GNP):\\nyear  GNP   ...     space\n","1   ajteel@dendrite.cs.Colorado.EDU (A.J. Teel) w...     space\n","2  \\nMy opinion is this:  In a society whose econ...     space\n","3  Ahhh, remember the days of Yesterday?  When we...     space\n","4  \\n\"...a la Chrysler\"??  Okay kids, to the near...     space\n","5  \\n   As for advertising -- sure, why not?  A N...  politics\n","6  \\n  What, pray tell, does this mean? Just who ...     space\n","7  \\nWhere does the shadow come from?  There's no...  politics\n","8                                       ^^^^^^^^^...  politics\n","9  #Yet, when a law was proposed for Virginia tha...     space"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"id":"6dTMmTB8fb_b","colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"status":"ok","timestamp":1618171298883,"user_tz":240,"elapsed":3858,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"cec2c3ad-42f5-4edc-e48d-24570158ec52"},"source":["print(\"******** docarray **********\")\n","\n","# build the stemmer object\n","stemmer = PorterStemmer()\n","\n","# build a new default analyzer using CountVectorizer that only uses words: [a-zA-Z]+\n","# also eliminate stop words\n","analyzer= CountVectorizer(analyzer = \"word\", \n","                          stop_words = 'english',\n","                          token_pattern = \"[a-zA-Z]+\").build_analyzer()\n","\n","# build a new analyzer that stems using the default analyzer to create the words to be stemmed\n","def stemmed_words(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))\n","\n","# build docarray\n","vectorizer = CountVectorizer(analyzer=stemmed_words,\n","                             #analyzer=analyzer,\n","                             binary=True,\n","                             min_df=2) # each word has to appear at least twice\n","docarray = vectorizer.fit_transform(newsgroups['text']).toarray()\n","docarray.shape\n","doc_df = pd.DataFrame(docarray, columns=list(vectorizer.get_feature_names()))\n","doc_df.head()"],"execution_count":97,"outputs":[{"output_type":"stream","text":["******** docarray **********\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>aa</th>\n","      <th>abandon</th>\n","      <th>abbey</th>\n","      <th>abc</th>\n","      <th>abil</th>\n","      <th>abl</th>\n","      <th>aboard</th>\n","      <th>abolish</th>\n","      <th>abort</th>\n","      <th>abroad</th>\n","      <th>absenc</th>\n","      <th>absolut</th>\n","      <th>absorb</th>\n","      <th>absorpt</th>\n","      <th>abstract</th>\n","      <th>absurd</th>\n","      <th>abund</th>\n","      <th>abus</th>\n","      <th>abyss</th>\n","      <th>ac</th>\n","      <th>acad</th>\n","      <th>acadamia</th>\n","      <th>academ</th>\n","      <th>academi</th>\n","      <th>academia</th>\n","      <th>acceler</th>\n","      <th>accept</th>\n","      <th>access</th>\n","      <th>accid</th>\n","      <th>accident</th>\n","      <th>accommod</th>\n","      <th>accomod</th>\n","      <th>accompani</th>\n","      <th>accomplish</th>\n","      <th>accord</th>\n","      <th>account</th>\n","      <th>accredit</th>\n","      <th>accur</th>\n","      <th>accuraci</th>\n","      <th>accus</th>\n","      <th>...</th>\n","      <th>wwii</th>\n","      <th>x</th>\n","      <th>xavier</th>\n","      <th>y</th>\n","      <th>ya</th>\n","      <th>yah</th>\n","      <th>yale</th>\n","      <th>yamada</th>\n","      <th>yard</th>\n","      <th>ye</th>\n","      <th>yea</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>yearli</th>\n","      <th>yee</th>\n","      <th>yell</th>\n","      <th>yellow</th>\n","      <th>yeltsin</th>\n","      <th>yer</th>\n","      <th>yesterday</th>\n","      <th>yield</th>\n","      <th>yo</th>\n","      <th>york</th>\n","      <th>yoshiro</th>\n","      <th>young</th>\n","      <th>youngster</th>\n","      <th>youth</th>\n","      <th>ysc</th>\n","      <th>yscvax</th>\n","      <th>ytou</th>\n","      <th>yugoslavia</th>\n","      <th>yup</th>\n","      <th>z</th>\n","      <th>zealand</th>\n","      <th>zenit</th>\n","      <th>zero</th>\n","      <th>zeta</th>\n","      <th>zip</th>\n","      <th>zone</th>\n","      <th>zoo</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 6045 columns</p>\n","</div>"],"text/plain":["   aa  abandon  abbey  abc  abil  abl  ...  zenit  zero  zeta  zip  zone  zoo\n","0   0        0      0    0     0    0  ...      0     0     0    0     0    0\n","1   0        0      0    0     0    0  ...      0     0     0    0     0    0\n","2   0        0      0    0     0    0  ...      0     0     0    0     0    0\n","3   0        0      0    0     0    0  ...      0     0     0    0     0    0\n","4   0        0      0    0     0    0  ...      0     1     0    0     0    0\n","\n","[5 rows x 6045 columns]"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"qut5LJe8drZ1"},"source":["## Decision Tree"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p3vQAlJEYsAm","executionInfo":{"status":"ok","timestamp":1618171317203,"user_tz":240,"elapsed":22169,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"7aaceefa-6e68-4a47-a9fd-58ac0dcdf20b"},"source":["print(\"******** model **********\")\n","\n","\n","# Decision Tree\n","model = DecisionTreeClassifier()\n","\n","# grid search\n","param_grid = {'max_depth': list(range(1,30)), 'criterion':['gini','entropy']}\n","grid = GridSearchCV(model, param_grid, cv=2, verbose=10, n_jobs=-1)\n","grid.fit(docarray, newsgroups['label'])\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n","tree_print(grid.best_estimator_,doc_df)"],"execution_count":98,"outputs":[{"output_type":"stream","text":["******** model **********\n","Fitting 2 folds for each of 58 candidates, totalling 116 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n","[Parallel(n_jobs=-1)]: Batch computation too fast (0.1519s.) Setting batch_size=2.\n","[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.4s\n","[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.7s\n","[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    3.2s\n","[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    5.5s\n","[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    8.1s\n","[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   10.3s\n","[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   12.8s\n","[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:   16.6s\n","[Parallel(n_jobs=-1)]: Done 116 out of 116 | elapsed:   17.4s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Grid Search: best parameters: {'criterion': 'gini', 'max_depth': 23}\n","if space =< 0.5: \n","  |then if orbit =< 0.5: \n","  |  |then if peopl =< 0.5: \n","  |  |  |then if clinton =< 0.5: \n","  |  |  |  |then if homosexu =< 0.5: \n","  |  |  |  |  |then if tax =< 0.5: \n","  |  |  |  |  |  |then if parti =< 0.5: \n","  |  |  |  |  |  |  |then if crime =< 0.5: \n","  |  |  |  |  |  |  |  |then if trial =< 0.5: \n","  |  |  |  |  |  |  |  |  |then if libertarian =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |then if statement =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |then if liber =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |then if argument =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |then if u =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if presid =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if drug =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if yeah =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if hous =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if frank =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if admit =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if verdict =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if like =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if nation =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else if post =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else if mission =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else if hous =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |else if lehrer =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if gummint =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if leas =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |else if certainli =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |then space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |else if ok =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |then space\n","  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |else if mine =< 0.5: \n","  |  |  |  |  |  |  |then space\n","  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |else space\n","  |  |  |  |else if separ =< 0.5: \n","  |  |  |  |  |then space\n","  |  |  |  |  |else politics\n","  |  |  |else if nasa =< 0.5: \n","  |  |  |  |then if rocket =< 0.5: \n","  |  |  |  |  |then if sky =< 0.5: \n","  |  |  |  |  |  |then if optimist =< 0.5: \n","  |  |  |  |  |  |  |then if robot =< 0.5: \n","  |  |  |  |  |  |  |  |then if miner =< 0.5: \n","  |  |  |  |  |  |  |  |  |then if aloud =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |then if dug =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |then if durham =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |then if margin =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |then if treason =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if dusk =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if corner =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if planetari =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then if mail =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else if say =< 0.5: \n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |then politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else space\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |  |else politics\n","  |  |  |  |  |  |else politics\n","  |  |  |  |  |else politics\n","  |  |  |  |else if air =< 0.5: \n","  |  |  |  |  |then politics\n","  |  |  |  |  |else space\n","  |  |else politics\n","  |else if democrat =< 0.5: \n","  |  |then if radioact =< 0.5: \n","  |  |  |then if humanitarian =< 0.5: \n","  |  |  |  |then if sex =< 0.5: \n","  |  |  |  |  |then politics\n","  |  |  |  |  |else space\n","  |  |  |  |else space\n","  |  |  |else space\n","  |  |else if sustain =< 0.5: \n","  |  |  |then space\n","  |  |  |else politics\n","<------------------------------------------------------------------->\n","Tree Depth:  23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8E8XLIGjZIH5","executionInfo":{"status":"ok","timestamp":1618171317348,"user_tz":240,"elapsed":22307,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"1abe8e56-ec2d-4fd5-856e-987c8c756310"},"source":["print(\"******** Accuracy **********\")\n","\n","# accuracy of best model with confidence interval\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(docarray)\n","acc = accuracy_score(newsgroups['label'], predict_y)\n","lb,ub = classification_confint(acc,docarray.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"],"execution_count":99,"outputs":[{"output_type":"stream","text":["******** Accuracy **********\n","Accuracy: 0.92 (0.90,0.94)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZfWC5Ilocqr2","executionInfo":{"status":"ok","timestamp":1618171317349,"user_tz":240,"elapsed":22301,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"aea6154c-215c-40d8-d4fc-b04ee7d9fd6e"},"source":["print(\"******** confusion matrix **********\")\n","\n","# build the confusion matrix\n","cats = ['politics','space']\n","cm = confusion_matrix(newsgroups['label'], predict_y, labels=cats)\n","cm_df = pd.DataFrame(cm, index=cats, columns=cats)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":100,"outputs":[{"output_type":"stream","text":["******** confusion matrix **********\n","Confusion Matrix:\n","          politics  space\n","politics       578      2\n","space           82    376\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N0BitS8NcsNW"},"source":["## KNN"]},{"cell_type":"code","metadata":{"id":"ZIAw501Qfb_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618171347382,"user_tz":240,"elapsed":52327,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"17b7a4d2-811c-4dc7-8eb7-019cf41c2bde"},"source":["print(\"******** model **********\")\n","\n","\n","# KNN\n","model = KNeighborsClassifier()\n","\n","# grid search\n","param_grid = {'n_neighbors': list(range(1,15,3))}\n","grid = GridSearchCV(model, param_grid, cv=2, verbose=10, n_jobs=-1)\n","grid.fit(docarray, newsgroups['label'])\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))"],"execution_count":101,"outputs":[{"output_type":"stream","text":["******** model **********\n","Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    5.8s\n","[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   11.8s\n","[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   29.0s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Grid Search: best parameters: {'n_neighbors': 4}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FoSLjZfW7dQ","executionInfo":{"status":"ok","timestamp":1618171377304,"user_tz":240,"elapsed":82241,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"3d5cdf2b-508a-4387-d089-7dabae0eff75"},"source":["print(\"******** model fine tuning **********\")\n","\n","\n","# KNN\n","model = KNeighborsClassifier()\n","\n","# grid search\n","cpoint = grid.best_params_['n_neighbors']\n","lpoint = cpoint -2\n","hpoint = cpoint +3\n","param_grid = {'n_neighbors': list(range(lpoint,hpoint))}\n","grid = GridSearchCV(model, param_grid, cv=2, verbose=10, n_jobs=-1)\n","grid.fit(docarray, newsgroups['label'])\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))"],"execution_count":102,"outputs":[{"output_type":"stream","text":["******** model fine tuning **********\n","Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    5.8s\n","[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   11.7s\n","[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   29.0s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Grid Search: best parameters: {'n_neighbors': 2}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rXytX14mfb_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618171392562,"user_tz":240,"elapsed":97491,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"fa0fe4f2-57ef-4fea-edd9-eb513b2b28f4"},"source":["print(\"******** Accuracy **********\")\n","\n","# accuracy of best model with confidence interval\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(docarray)\n","acc = accuracy_score(newsgroups['label'], predict_y)\n","lb,ub = classification_confint(acc,docarray.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"],"execution_count":103,"outputs":[{"output_type":"stream","text":["******** Accuracy **********\n","Accuracy: 0.94 (0.92,0.95)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vrNMB9tyfb_d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618171392563,"user_tz":240,"elapsed":97483,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"bffe4b0b-ead1-49e0-dc84-a55ba33b151a"},"source":["print(\"******** confusion matrix **********\")\n","\n","# build the confusion matrix\n","cats = ['politics','space']\n","cm = confusion_matrix(newsgroups['label'], predict_y, labels=cats)\n","cm_df = pd.DataFrame(cm, index=cats, columns=cats)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":104,"outputs":[{"output_type":"stream","text":["******** confusion matrix **********\n","Confusion Matrix:\n","          politics  space\n","politics       577      3\n","space           60    398\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1H94tYstfb_d"},"source":["## Naive Bayes (NB)\n","\n","* “Standard” model for text processing\n","* Fast to train, has no problems with very high dimensional data\n","* NB is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. \n","* In simple terms, a NB classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. \n","* For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n"]},{"cell_type":"markdown","metadata":{"id":"Gb_q1xG5fb_e"},"source":["### The Mathematics\n","\n","[Source](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained)\n","\n","* Bayes theorem provides a way of calculating posterior probability $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$. Look at the equation below, where\n","  * $P(c|x)$ is the posterior probability of class (c, target) given predictor (x, attributes).\n","  * $P(c)$ is the prior probability of class.\n","  * $P(x|c)$ is the likelihood which is the probability of predictor given class.\n","  * $P(x)$ is the prior probability of predictor.\n","\n","<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172.png\" width=\"400\" height=\"400\">"]},{"cell_type":"markdown","metadata":{"id":"N-eW3am6fb_e"},"source":["### Example\n","\n","Let's assume we have a predictor `Weather` and a target `Play` that contains classes (left table below).  \n","\n","<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41.png\">\n","\n","We want to compute if we play tennis when sunny.  That is we compute the two probabilities,\n","1. $P(Yes|Sunny)$\n","1. $P(No|Sunny)$\n","and then pick the statement with the higher probability."]},{"cell_type":"markdown","metadata":{"id":"qg4Y_kGDfb_f"},"source":["Basically, NB just counts, let's look at $P(Yes|Sunny)$,\n","\n","$P(Yes|Sunny) = \\frac{P(Sunny|Yes)P(Yes)}{P(Sunny)} = \\frac{3/9\\times 9/14}{5/14} = \\frac{.33 \\times .64}{.36}=.60$\n","\n","Now, let's look at $P(No|Sunny)$,\n","\n","$P(No|Sunny) = \\frac{P(Sunny|No)P(No)}{P(Sunny)} = \\frac{2/5\\times 5/14}{5/14} = \\frac{.40 \\times .36}{.36}=.40$\n","\n","We are playing tennis when sunny because the posterior probability $P(Yes|Sunny)$ is higher."]},{"cell_type":"markdown","metadata":{"id":"kB8TjXIkfb_f"},"source":["Let’s take our text classification problem and use a Naive Bayes classifier on it.\n","\n","The setup and data prep is the same as in the case of the KNN classifier."]},{"cell_type":"code","metadata":{"id":"MFeBwTMPfb_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618171392726,"user_tz":240,"elapsed":97639,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"97ae1a62-d1b4-4a2f-f7f4-73f7b45d9793"},"source":["## Naive Bayes\n","\n","print(\"******** model **********\")\n","\n","\n","# Naive Bayes\n","model = MultinomialNB()\n","# NOTE: NB does not have any hyper-parameters - no overfitting - no searching over parameter space!\n","model.fit(docarray, newsgroups['label'])\n","\n","\n","print(\"******** Accuracy **********\")\n","\n","# accuracy of best model with confidence interval\n","best_model = model\n","predict_y = best_model.predict(docarray)\n","acc = accuracy_score(newsgroups['label'], predict_y)\n","lb,ub = classification_confint(acc,docarray.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n","\n","print(\"******** confusion matrix **********\")\n","\n","# build the confusion matrix\n","cats = ['politics','space']\n","cm = confusion_matrix(newsgroups['label'], predict_y, labels=cats)\n","cm_df = pd.DataFrame(cm, index=cats, columns=cats)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":105,"outputs":[{"output_type":"stream","text":["******** model **********\n","******** Accuracy **********\n","Accuracy: 0.96 (0.95,0.98)\n","******** confusion matrix **********\n","Confusion Matrix:\n","          politics  space\n","politics       556     24\n","space           13    445\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2w-o8bV1fb_g"},"source":["Trains very fast and has a higher accuracy than KNN and the difference in accuracy is statistically significant!\n","\n","> NB does not have any hyper-parameters - no overfitting - no searching over parameter space!\n","\n","Hint: Try cross-validating the NB model - you will find that the fold accuracies and the mean accuracy will fall into the CI computed above.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XP4MuOaafb_h"},"source":["Assignment -- See BrightSpace"]},{"cell_type":"code","metadata":{"id":"Zr8jF1n_fb_h","executionInfo":{"status":"ok","timestamp":1618171392729,"user_tz":240,"elapsed":97635,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}}},"source":[""],"execution_count":105,"outputs":[]}]}