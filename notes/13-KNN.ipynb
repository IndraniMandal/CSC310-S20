{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"GOTn9R3We-Y1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709256829891,"user_tz":300,"elapsed":343,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"e48dd24a-8da8-4388-888b-6e7e5bfee9ae"},"source":["###### Set Up #####\n","# verify our folder with the data and module assets is installed\n","# if it is installed make sure it is the latest\n","!test -e ds-assets && cd ds-assets && git pull && cd ..\n","# if it is not installed clone it\n","!test ! -e ds-assets && git clone https://github.com/lutzhamel/ds-assets.git\n","# point to the folder with the assets\n","home = \"ds-assets/assets/\"\n","import sys\n","sys.path.append(home)      # add home folder to module search path"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"markdown","metadata":{"id":"mYURltXbe-Y-"},"source":["# k-NN Classification\n","\n","k-NN: **k** **N**earest **N**eighbors"]},{"cell_type":"markdown","metadata":{"id":"MY9iVDxne-Y_"},"source":["Neighbors-based classification is a type of *instance-based learning*: it does not attempt to construct a general internal model, but simply stores instances of the training data.\n","Classification is computed from a simple majority vote of the *nearest neighbors of each point*: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n"]},{"cell_type":"markdown","metadata":{"id":"ibqp6g_5e-ZA"},"source":["## Illustration\n","\n","Consider the following figure,\n","\n","<!-- ![knn](assets/knn.png) -->\n","<center>\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/knn.png\" height=\"256\" width=\"280\">\n","</center>\n","\n","We want to assign the sample (green bullet) either to the class of blue squares or to the class of red triangles,\n","\n","* If k = 3 (solid line circle) it is assigned to the class of red triangles because there are 2 triangles and only 1 square inside the inner circle.\n","\n","* If k = 5 (dashed line circle) it is assigned to the class of blue squares (3 squares vs. 2 triangles inside the dashed circle).\n"]},{"cell_type":"markdown","metadata":{"id":"QOFR_NDue-ZA"},"source":["## k-NN Classification\n","\n","k-NN classification is a supervised learning algorithm, therefore the training examples are vectors in a multidimensional feature space, each with a class label.\n","The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n","In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n","A commonly used distance metric for continuous variables is the Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the Hamming distance.\n"]},{"cell_type":"markdown","metadata":{"id":"lW41YKJie-ZA"},"source":["## A Code Example\n","\n","Let's build an k-NN classifier for the iris dataset.  \n","\n","**NOTE**: we are not searching for the optimal model, we just want to build a classifier."]},{"cell_type":"code","metadata":{"id":"npqBRyHfe-ZB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709256832294,"user_tz":300,"elapsed":2405,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"4888ceee-6b4f-4703-cc61-1788fbf40f35"},"source":["import pandas as pd\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from confint import classification_confint\n","\n","# get data\n","df = pd.read_csv(home+\"iris.csv\")\n","X  = df.drop(columns=['id','Species'])\n","y = df[['Species']]\n","\n","# set up the model with k=3\n","model = KNeighborsClassifier(n_neighbors=3)\n","\n","# do train and test\n","train_X, test_X, train_y, test_y =\\\n","   train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=5)\n","model.fit(train_X, train_y['Species']) # KNN wants a series as its target\n","\n","# evaluate the model\n","predict_y = model.predict(test_X)\n","acc = accuracy_score(test_y, predict_y)\n","lb, ub = classification_confint(acc, test_X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.93 (0.84, 1.00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"VY_EDc4J45hZ"},"source":["The performance is not bad for a randomly chosen value for k.  "]},{"cell_type":"markdown","metadata":{"id":"x5zeXrzde-ZC"},"source":["# Model Comparison\n","\n","Here we are a little bit more careful with our model construction and do a cross-validated grid search for the optimal value of k.\n","Furthermore we want to see how our optimal k-NN classifier performance stacks up to the performance of a decision tree model in a statistical valid manner.\n","\n","\n","Letâ€™s work our way through this comparison using the `wdbc` dataset:\n","\n","* Build optimal k-NN and tree models using grid search\n","* Compute the accuracy for the classifiers\n","* Print out the confusion matrix for each classifier\n","* Print out the confidence interval for each classifier\n","* Decide if the difference between classifiers is statistically significant or not."]},{"cell_type":"markdown","metadata":{"id":"GrhiXUR9e-ZD"},"source":["## Set Up\n","\n","Get our training data and format in way that `sklearn` expects."]},{"cell_type":"code","metadata":{"id":"b2kNTS7ze-ZD","executionInfo":{"status":"ok","timestamp":1709256832294,"user_tz":300,"elapsed":3,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["# basic data routines\n","import pandas as pd\n","\n","# models\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# model evaluation routines\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from confint import classification_confint"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"II8S6swxW2su","executionInfo":{"status":"ok","timestamp":1709256832294,"user_tz":300,"elapsed":2,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":["# get data\n","df = pd.read_csv(home+\"wdbc.csv\").drop(columns=['ID'])\n","\n","# format training data for sklean\n","X  = df.drop(columns=['Diagnosis'])\n","actual_y = df[['Diagnosis']]"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NnqV3j2e-ZE"},"source":["## k-NN Classifier\n","\n","First up is the k-NN classifier.  In order to find the optimal model we set up a grid search over the number of neighbors.  In this case we search the values from 1 to 25."]},{"cell_type":"code","metadata":{"id":"wyT0W1J1e-ZE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709256840321,"user_tz":300,"elapsed":8029,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"bc896d1b-4077-49e4-84db-ebb2f91b0705"},"source":["# KNN\n","model = KNeighborsClassifier()\n","\n","# grid search\n","param_grid = {'n_neighbors': list(range(1,26))}\n","grid = GridSearchCV(model, param_grid, cv=5)\n","grid.fit(X, actual_y['Diagnosis']) # KNN wants a series as its target\n","\n","# accuracy of best model with confidence interval\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(X)\n","acc = accuracy_score(actual_y, predict_y)\n","lb,ub = classification_confint(acc,X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n","\n","# build the confusion matrix for more detailed error analysis\n","labels = ['M', 'B']\n","cm = confusion_matrix(actual_y, predict_y, labels=labels)\n","cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.94 (0.92,0.96)\n","Confusion Matrix:\n","     M    B\n","M  186   26\n","B    8  349\n"]}]},{"cell_type":"markdown","metadata":{"id":"BhlWnvWPJkbf"},"source":["Let's take a look at the performance data.  In terms of accuracy we see that the best k-NN model has an accuracy of 94% with a confidence interval of (92%, 96%).  From a medical application perspective the confusion matrix is worrisome.  We see that of the 212 malignant samples the model misclassifies 26 as benign.  This kind of error is called the 'false negative' error and in this case would mean that 12% of the malignant cases remain undetected. We also see that of the 357 benign samples it misclassifies 8 as malignant.  The is called the 'false positive' error. From a medical point of view this is not as worrisome because additional tests will identify these cases correctly as benign."]},{"cell_type":"markdown","metadata":{"id":"6g1xxMaPe-ZD"},"source":["## Decision Trees\n","\n","For decision trees we set up a grid search over the tree depth from 1 to 20 and the criterion which searches over `entropy` and `gini`."]},{"cell_type":"code","metadata":{"id":"USmn4hFMe-ZD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709256848344,"user_tz":300,"elapsed":8045,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}},"outputId":"03536950-c2dc-4864-90e5-6e2a96b3503f"},"source":["# decision trees\n","model = DecisionTreeClassifier(random_state=1)\n","\n","# grid search\n","param_grid = {'max_depth': list(range(1,21)), 'criterion': ['entropy','gini'] }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","grid.fit(X, actual_y)\n","\n","# accuracy of best model with confidence interval\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(X)\n","acc = accuracy_score(actual_y, predict_y)\n","lb,ub = classification_confint(acc,X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n","\n","# build the confusion matrix for a more detailed error analysis\n","labels = ['M', 'B']\n","cm = confusion_matrix(actual_y, predict_y, labels=labels)\n","cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.98 (0.97,0.99)\n","Confusion Matrix:\n","     M    B\n","M  210    2\n","B    7  350\n"]}]},{"cell_type":"markdown","metadata":{"id":"82db8d64YJgi"},"source":["The performance of the decision tree model is much better overall and from a medical point specifically.  Less than 1% of the malignant cases is classified as a 'false negative' giving much more confidence in its applicability in a medical setting. The accuracy of the model is 98% with a confidence interval of (97%, 99%)."]},{"cell_type":"markdown","metadata":{"id":"0BNwcln4e-ZF"},"source":["## Performance Comparison and Model Selection\n","\n","If we compare models we have to look beyond the raw performance numbers in this case 94% and 98% for k-NN and the decision tree model, respectively. We have to ask if the difference in performance between these two models is statistically significant.  Consider the performance of the k-NN model with an accuracy and confidence interval of,\n","```\n","94% (92%, 96%)\n","```\n","Also consider the performance of the decision tree model with an accuracy and confidence interval of,\n","```\n","98% (97%, 99%)\n","```\n","Here we see that\n","the confidence intervals for the decision tree and the K-NN classifier **do not overlap**.  That means here the decision tree is truly the better model and the performance difference between the two models is **statistically significant**.  \n","\n","**Observation**: Therefore we will select the **decision tree as a model** for our breast cancer data.\n"]},{"cell_type":"code","metadata":{"id":"8bGNDOW8e-ZF","executionInfo":{"status":"ok","timestamp":1709256848345,"user_tz":300,"elapsed":29,"user":{"displayName":"Lutz Hamel","userId":"10287662568849688016"}}},"source":[],"execution_count":6,"outputs":[]}]}