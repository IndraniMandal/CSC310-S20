{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"13-KNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"GOTn9R3We-Y1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638364836638,"user_tz":300,"elapsed":566,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"83df5a28-74f7-4753-e73c-584965ec451f"},"source":["###### Set Up #####\n","# verify our folder with the data and module assets is installed\n","# if it is installed make sure it is the latest\n","!test -e ds-assets && cd ds-assets && git pull && cd ..\n","# if it is not installed clone it \n","!test ! -e ds-assets && git clone https://github.com/lutzhamel/ds-assets.git\n","# point to the folder with the assets\n","home = \"ds-assets/assets/\" \n","import sys\n","sys.path.append(home)      # add home folder to module search path"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"markdown","metadata":{"id":"mYURltXbe-Y-"},"source":["# k-NN Classification\n","\n","k-NN: **k** **N**earest **N**eighbors"]},{"cell_type":"markdown","metadata":{"id":"MY9iVDxne-Y_"},"source":["Neighbors-based classification is a type of *instance-based learning*: it does not attempt to construct a general internal model, but simply stores instances of the training data. \n","Classification is computed from a simple majority vote of the *nearest neighbors of each point*: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n"]},{"cell_type":"markdown","metadata":{"id":"ibqp6g_5e-ZA"},"source":["## Example\n","\n","Consider,\n","\n","<!-- ![knn](assets/knn.png) -->\n","\n","<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/knn.png\" height=\"256\" width=\"280\">\n","\n","We want to assign the sample (green bullet) either to the class of blue squares or to the class of red triangles,\n","\n","* If k = 3 (solid line circle) it is assigned to the class of red triangles because there are 2 triangles and only 1 square inside the inner circle. \n","\n","* If k = 5 (dashed line circle) it is assigned to the class of blue squares (3 squares vs. 2 triangles inside the dashed circle).\n"]},{"cell_type":"markdown","metadata":{"id":"QOFR_NDue-ZA"},"source":["## k-NN Classification\n","\n","k-NN classification is a supervised learning algorithm, therefore the training examples are vectors in a multidimensional feature space, each with a class label. \n","The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n","In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n","A commonly used distance metric for continuous variables is the Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the Hamming distance.\n"]},{"cell_type":"markdown","metadata":{"id":"lW41YKJie-ZA"},"source":["## A Code Example\n","\n","Let's build an k-NN classifier for the iris dataset."]},{"cell_type":"code","metadata":{"id":"npqBRyHfe-ZB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638364836638,"user_tz":300,"elapsed":5,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"6dd540f9-0262-4eaf-c2e2-5dba3f1fc52b"},"source":["import pandas as pd\n","import numpy as np\n","np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from confint import classification_confint\n","\n","# get data\n","df = pd.read_csv(home+\"iris.csv\")\n","X  = df.drop(['id','Species'],axis=1)\n","y = df['Species']\n","\n","# set up the model with k=3\n","model = KNeighborsClassifier(n_neighbors=3)\n","\n","# do train and test\n","train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.8, test_size=0.2)\n","model.fit(train_X, train_y)\n","predict_y = model.predict(test_X)\n","acc = accuracy_score(test_y, predict_y)\n","lb, ub = classification_confint(acc, test_X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.90 (0.79, 1.00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"VY_EDc4J45hZ"},"source":["The performance is not bad for a randomly chosen value for k.  "]},{"cell_type":"markdown","metadata":{"id":"x5zeXrzde-ZC"},"source":["# Model Comparison\n","\n","Here we are a little bit more careful with our model construction and do a cross-validated grid search for the optimal value of k.\n","Furthermore we want to see how our optimal k-NN classifier performance stacks up to the performance of a decision tree model in a statistical valid manner.\n","\n","\n","Letâ€™s work our way through this comparison using the `wdbc` dataset:\n","\n","* Build optimal k-NN and tree models using grid search\n","* Compute the accuracy for the classifiers\n","* Print out the confusion matrix for each classifier\n","* Print out the confidence interval for each classifier\n","* Decide if the difference between classifiers is statistically significant or not."]},{"cell_type":"markdown","metadata":{"id":"GrhiXUR9e-ZD"},"source":["## Set Up\n","\n","Get our training data and format in way that `sklearn` expects."]},{"cell_type":"code","metadata":{"id":"b2kNTS7ze-ZD","executionInfo":{"status":"ok","timestamp":1638364836639,"user_tz":300,"elapsed":3,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}}},"source":["# basic data routines\n","import pandas as pd\n","\n","# models\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# model evaluation routines\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from confint import classification_confint\n","\n","# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","df = df.drop(['ID'],axis=1)\n","\n","# format training data for sklean\n","X  = df.drop(['Diagnosis'],axis=1)\n","actual_y = df['Diagnosis']"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NnqV3j2e-ZE"},"source":["## k-NN Classifier\n","\n","First up is the k-NN classifier.  In order to find the optimal model we set up a grid search over the number of neighbors.  In this case we search the values from 1 to 25."]},{"cell_type":"code","metadata":{"id":"wyT0W1J1e-ZE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638364838792,"user_tz":300,"elapsed":2156,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"0d378e91-74df-4eae-d432-babdf931458a"},"source":["# KNN\n","model = KNeighborsClassifier()\n","\n","# grid search\n","param_grid = {'n_neighbors': list(range(1,26))}\n","grid = GridSearchCV(model, param_grid, cv=5)\n","grid.fit(X, actual_y)\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n","\n","# accuracy of best model with confidence interval\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(X)\n","acc = accuracy_score(actual_y, predict_y)\n","lb,ub = classification_confint(acc,X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n","\n","# build the confusion matrix\n","labels = ['M', 'B']\n","cm = confusion_matrix(actual_y, predict_y, labels=labels)\n","cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Grid Search: best parameters: {'n_neighbors': 14}\n","Accuracy: 0.94 (0.92,0.96)\n","Confusion Matrix:\n","     M    B\n","M  186   26\n","B    8  349\n"]}]},{"cell_type":"markdown","metadata":{"id":"BhlWnvWPJkbf"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"6g1xxMaPe-ZD"},"source":["## Decision Trees\n","\n","For decision trees we set up a grid search over the tree depth from 1 to 20 and the criterion which searches over `entropy` and `gini`."]},{"cell_type":"code","metadata":{"id":"USmn4hFMe-ZD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638364841579,"user_tz":300,"elapsed":2797,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"2da5f3aa-9362-439b-beeb-6862e56f0b25"},"source":["# decision trees\n","model = DecisionTreeClassifier(random_state=1)\n","\n","# grid search\n","param_grid = {'max_depth': list(range(1,21)), 'criterion': ['entropy','gini'] }\n","grid = GridSearchCV(model, param_grid, cv=5)\n","grid.fit(X, actual_y)\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n","\n","# accuracy of best model with confidence interval\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(X)\n","acc = accuracy_score(actual_y, predict_y)\n","lb,ub = classification_confint(acc,X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n","\n","# build the confusion matrix\n","labels = ['M', 'B']\n","cm = confusion_matrix(actual_y, predict_y, labels=labels)\n","cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n","print(\"Confusion Matrix:\\n{}\".format(cm_df))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Grid Search: best parameters: {'criterion': 'entropy', 'max_depth': 4}\n","Accuracy: 0.98 (0.97,0.99)\n","Confusion Matrix:\n","     M    B\n","M  210    2\n","B    7  350\n"]}]},{"cell_type":"markdown","metadata":{"id":"0BNwcln4e-ZF"},"source":["## Performance Comparison and Model Selection\n","\n","The confidence intervals for the decision tree and the K-NN classifier do not overlap.  That means here the decision tree is truly the better model."]},{"cell_type":"code","metadata":{"id":"8bGNDOW8e-ZF","executionInfo":{"status":"ok","timestamp":1638364841580,"user_tz":300,"elapsed":7,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}}},"source":[""],"execution_count":15,"outputs":[]}]}