{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble to be able to run notebooks in Jupyter and Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    notes_home = \"/content/drive/Shared drives/CSC310/notes/\"\n",
    "    user_home = \"/content/drive/My Drive/\"\n",
    "    \n",
    "    sys.path.insert(1,notes_home) # let the notebook access the notes folder\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    notes_home = \"\" # running native Jupyter environment -- notes home is the same as the notebook\n",
    "    user_home = \"\"  # under Jupyter we assume the user directory is the same as the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN Classification\n",
    "\n",
    "k-NN: **k** **N**earest **N**eighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbors-based classification is a type of *instance-based learning*: it does not attempt to construct a general internal model, but simply stores instances of the training data. \n",
    "\n",
    "Classification is computed from a simple majority vote of the *nearest neighbors of each point*: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Consider,\n",
    "\n",
    "<!-- ![knn](assets/knn.png) -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lutzhamel/ds/master/notes/assets/knn.png\" height=\"256\" width=\"256\">\n",
    "\n",
    "* The test sample (green circle) should be classified either to the class of blue squares or to the class of red triangles. \n",
    "\n",
    "* If k = 3 (solid line circle) it is assigned to the class of red triangles because there are 2 triangles and only 1 square inside the inner circle. \n",
    "\n",
    "* If k = 5 (dashed line circle) it is assigned to the class of blue squares (3 squares vs. 2 triangles inside the dashed circle).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN Classification\n",
    "\n",
    "The training examples are vectors in a multidimensional feature space, each with a class label. \n",
    "\n",
    "The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n",
    "\n",
    "In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n",
    "\n",
    "A commonly used distance metric for continuous variables is the Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the Hamming distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy: 1.00\n",
      "Fold Accuracies: [0.97 0.97 0.93 0.97 1.00]\n",
      "XV Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# get data\n",
    "df = pd.read_csv(notes_home+\"assets/iris.csv\")\n",
    "X  = df.drop(['id','Species'],axis=1)\n",
    "y = df['Species']\n",
    "\n",
    "# set up the model with k=3\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# do train-test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "model.fit(train_X, train_y)\n",
    "predict_y = model.predict(test_X)\n",
    "print(\"Train-Test Accuracy: {:3.2f}\".format(accuracy_score(test_y, predict_y)))\n",
    "\n",
    "# do the 5-fold cross validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Fold Accuracies: {}\".format(scores))\n",
    "print(\"XV Accuracy: {:3.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "We now have two different kinds of models, decision trees and k-NN, we can use to do classification.\n",
    "\n",
    "Let’s work our way through an example using the dataset ‘wdbc’ and compare the model performance of each of the models on that data set:\n",
    "\n",
    "* Build optimal tree and KNN models using grid search\n",
    "* Compute the accuracy for the classifiers\n",
    "* Print out the confusion matrix for each classifier\n",
    "* Print out the confidence interval for each classifier\n",
    "* Decide if the difference between classifiers is statistically significant or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic data routines\n",
    "import pandas as pd\n",
    "\n",
    "# models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# model evaluation routines\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from assets.confint import classification_confint\n",
    "\n",
    "# get data\n",
    "df = pd.read_csv(notes_home+\"assets/wdbc.csv\")\n",
    "df = df.drop(['ID'],axis=1)\n",
    "\n",
    "# format training data for sklean\n",
    "X  = df.drop(['Diagnosis'],axis=1)\n",
    "actual_y = df['Diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: best parameters: {'criterion': 'entropy', 'max_depth': 4}\n",
      "Accuracy: 0.98 (0.97,0.99)\n",
      "Confusion Matrix:\n",
      "     M    B\n",
      "M  210    2\n",
      "B    7  350\n"
     ]
    }
   ],
   "source": [
    "# decision trees\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# grid search\n",
    "param_grid = {'max_depth': list(range(1,21)), 'criterion': ['entropy','gini'] }\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(X, actual_y)\n",
    "print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# accuracy of best model with confidence interval\n",
    "best_model = grid.best_estimator_\n",
    "predict_y = best_model.predict(X)\n",
    "acc = accuracy_score(actual_y, predict_y)\n",
    "lb,ub = classification_confint(acc,X.shape[0])\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n",
    "\n",
    "# build the confusion matrix\n",
    "labels = ['M', 'B']\n",
    "cm = confusion_matrix(actual_y, predict_y, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: best parameters: {'n_neighbors': 14}\n",
      "Accuracy: 0.94 (0.92,0.96)\n",
      "Confusion Matrix:\n",
      "     B    M\n",
      "B  349    8\n",
      "M   26  186\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# grid search\n",
    "param_grid = {'n_neighbors': list(range(1,51))}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(X, actual_y)\n",
    "print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# accuracy of best model with confidence interval\n",
    "best_model = grid.best_estimator_\n",
    "predict_y = best_model.predict(X)\n",
    "acc = accuracy_score(actual_y, predict_y)\n",
    "lb,ub = classification_confint(acc,X.shape[0])\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n",
    "\n",
    "# build the confusion matrix\n",
    "labels = ['B', 'M']\n",
    "cm = confusion_matrix(actual_y, predict_y, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison and Model Selection\n",
    "\n",
    "The confidence intervals for the decision tree and the K-NN classifier do not overlap.  That means here the decision tree is truly the better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
